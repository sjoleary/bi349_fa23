---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Exploratory Analysis

```{r}
#| label: setup
#| include: false

# custom functions ----

library(ggplot2)

theme_standard <- theme_classic() +
  theme(
    axis.text = element_text(size = 11, color = "black"),
    axis.title = element_text(size = 16, color = "black"),
    axis.title.y = element_text(vjust = 1.5, color = "black"),
    axis.line = element_line(color = "black"),
    
    legend.position = "bottom",
    
    panel.background = element_rect(fill = "white", color = NA), 
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), 
    strip.background = element_rect(fill = "grey95", color = "black"), 
    strip.text.x = element_text(size = 16, color = "black"),
    strip.text.y = element_text(size = 16, color = "black")) 

theme_facet <- theme_classic() +
  theme(
    axis.text = element_text(size = 11, color = "black"),
    axis.title = element_text(size = 16, color = "black"),
    axis.title.y = element_text(vjust = 1.5, color = "black"),
    axis.line = element_line(color = "black"),
    
    legend.position = "bottom",
    
    panel.background = element_rect(fill = "white", color = NA), 
    panel.border = element_rect(fill = NA, color = "black"), 
    panel.grid.major = element_line(color = "grey85"), 
    panel.grid.minor = element_blank(), 
    strip.background = element_rect(fill = "grey95", color = "black"), 
    strip.text.x = element_text(size = 16, color = "black"),
    strip.text.y = element_text(size = 16, color = "black")) 



# set options
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE)

options(htmltools.dir.version = FALSE)

```


**Learning Objectives**

After completing this tutorial you should be able to

* identify specific steps to explore a new data set
* formulate a question and determine if your data set is appropriate to answer it
* refine questions based on data contained in a data set
* outline a strategy for data exploration using graphs


[Download the directory for this project here](), make sure the directory is unzipped and move it to your `bi328` directory. You can open the `Rproj` for this module either by double clicking on it which will launch `Rstudio` or by opening `Rstudio` and then using `File > Open Project` or by clicking on the `Rproject` icon in the top right of your program window and selecting `Open Project`. 

There should be a file named `16_exploratory-analysis.qmd` in that project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") `quarto` file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `quarto` document to record your answers. Remember to use comments to annotate your code; at minimum you should have one comment per code set^[You should do this whether you are adding code yourself or using code from our manual, even if it isn't commented in the manual... especially when the code is already included for you, add comments to describe how the function works/what it does as we introduce it during the participatory coding session so you can refer back to it.] you may of course add as many comments as you need to be able to recall what you did. Similarly, take notes in the document as we discuss discussion/reflection questions but make sure that you go back and clean them up for "public consumption".

Let's make sure to read in the libraries we need for this analysis.

```{r}

# install janitor
install.packages("janitor")

# install skimr
install.packages("skimr")

```

Now we can load all of our libraries and get started

```{r}

# load libraries

# reporting
library(knitr)

# visualization
library(ggplot2)
library(ggthemes)
library(patchwork)

# data wrangling
library(dplyr)
library(tidyr)
library(readr)
library(skimr)
library(janitor)
library(magrittr)

# turn off scientific notation
options(scipen=999)

```

## Exploratory Analysis

An exploratory analysis is systematic way of exploring data set primarily through visualizations. Generally, it is an iterative process that starts with a **question** or **hypothesis** and then, most of your exploratory analysis consists of visualizing, transforming, and even modeling your data to answer that question. Finally, you need to summarize your results and conclusions and then use those answers to refine your original question or generate new questions. Frequently, an exploratory analysis is the first step before deciding on what your formal analysis will look like and can serve to generate hypothesis for further exploration.

Part of exploratory analysis is quality control of your data, determining if it meets your expectations and cleaning it up as needed. Frequently it includes using the existing raw data to transform your data into parameters that are more useful for your assessment.


## Formulate a question

Recall, from our visualization of Climate Change indicators that we left the question of whether or not hurricanes are going "to get worse" unresolved even after comparing both changes in the number of names storms, major hurricanes, and the intensity.

Let's say we wanted to explore whether or not natural disasters have gotten worse as climate change has progressed. To do this we can turn to the [International Disaster Database](https://www.emdat.be/).


::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Consider this

Go to their website and use information you can find about the data collected to answer the following questions (use bullet points where appropriate):

1. How is (emergency) disaster defined?
2. What are the major classifications of natural disasters included in the data set?
3. Which of these categories do you think are most likely impacted by climate change?
4. What measures can you use to determine if natural disasters are "getting worse"? Should different categories have different measures?
5. What other mechanisms do you think could result in an increase in natural disasters?

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

[Your answer here]

:::


::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Consider this

"Are natural disasters getting worse" is a pretty general question. Based on your overview of what is in the data set, try to formulate at least three more specific questions that you could explore using this data set.

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

1. 
2. 
3.

:::


## Read & Wrangle data set

You should have a copy of the tab-delimited file in your `data` folder. It contains an additional 6 lines at the top that have information on when and where it was downloaded.

```{r}

disaster <- read_delim("data/nat-disasters_emdat-query-2021-10-05.txt", delim = "\t", skip = 6)

```

Our first step always is to take a look at the data set and make sure that it has read in correctly (i.e. that the columns are correctly separated, numeric columns are numeric, character columns are characters etc.) and is in a usable format.


::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Consider this

Take a quick look at the data set using `View()` to make sure everything has read in correctly. Quickly skim to get an idea of how many columns/rows there are, how much missing data there is, that data types are correct etc.

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

[Your answer here]

:::

Even though data has read in correctly does not necessarily mean that it is in the most use-able format ... remember plotting and filtering data sets requires typing variable names in over and over again. Therefore, we want to make sure that everything is set up in a way where we will be able to avoid mistakes that are mostly due to typos.

For example, let's take a look at the column names:

```{r}

colnames(disaster)

```

Apparently, they have not read our guidelines for naming things. Column headers with special characters and spaces are super annoying, you might recall from our previous adventures in data wrangling that every time we want to call a column with spaces we are going to have to use back ticks to tell R that it isn't separate words but a single name.

Now is probably a good time to learn how to rename columns. The most straightforward way is using `rename()`.

Let's say we wanted to rename `Dis No` to `DisNo` the syntax is simple `NewName = OldName`.

```{r}

disaster %>%
  rename(DisNo = `Dis No`) %>%
  head()

```

This would be pretty annoying if we wanted to rename all of our columns by hand. 

Fortunately, we can leverage the function `janitor::clean_names()` function^[the notation format you see here specifies the `Rpackage` and then the function within it, as `package::function()`.] which is designed to parse letter cases and separators - the default is to convert it to `snake_case`, i.e. all names are lowercase and words are separated by an underscore, it also deals with duplicate names and special characters.

Let's see what this does for us:

```{r}

disaster <- read_delim("data/nat-disasters_emdat-query-2021-10-05.txt", delim = "\t", skip = 6) %>%
  clean_names()

colnames(disaster)

```

That looks much better. 


## Get an overview of the data set

Our next step always is to determine what information is contained in the data set and how it is organized. 

First, we will need to understand what information is contained in each column. Some of the column headers are more self-explanatory than others. One of the most helpful documents at this point is the metadata which will tell us what information is in each column. A good place to look for that is the [data portal](https://public.emdat.be) where you accessed the data set itself.


::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Consider this

Determine where you can find meta-data for our data set and describe what information is contained in the data set based on the included columns. Indicate which you think will be most useful to answer our general question based on your answers to Q1.

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

[Your answer here]

:::


Now, we can take a look at some of the specifics of how the information is organized in the data set and specifics on the information contained.

::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Consider this

Based on some of our previous exploits exploring data, come up with a checklist of at least five things to routinely check any time you are exploring a new data set, wherever possible include what function(s) you could use to look at that.

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

1.
2.
3.
4.
5.

:::


::: {.callout-tip icon=false collapse=true}

## {{< fa person-chalkboard >}}    Pointers

Factors to consider are among other things understanding how many rows/columns there are, what variables are included, whether it is in a tidy format or not, what data type each variable is, and what the range/distribution of values is.

You can get dimensions of the data set using `ncols()` and `nrows()`, `str()` will give you dimensions, the class, and data types (classes) of individual columns.

:::

Next to all of the things you just listed there, another helpful functions is `skimr::skim()`


::: {.callout-note icon=false}

## {{< fa clipboard-question >}}   Give it a whirl

Run the function on your data set and briefly describe what information you can get^[You may want to adjust the width of your console pane toget a better look at the output.].

```{r}

skim(disaster)

```

:::


::: {.callout-important icon=false}

## {{< fa pen >}}   Did it!

[Your answer here]

:::

For numerical columns mean values and percentiles can give you an overview of how the data is distributed. However, for columns that contain strings or characters we cannot produce those types of summary statistics. Rather, we are probably more interested on how many unique values those columns contain and what those unique entries are.

`skim()` already gave us the number of unique entries for each column, we have two options we can use to determine what those values are. The first is `unique()`. This function makes use of the fact that each column is a vector^[Remember, we can access each column of a dataframe as `df$columnname].

```{r}

unique(disaster$disaster_subgroup)

```

When you run this function it returns a vector; while this might be helpful in some contexts, it would be helpful if we could also view it in a tabular format. Do not fret - `tidyverse` got you.

```{r}

disaster %>%
  distinct(disaster_subgroup)

```

## Chose a question to explore

Frequently you will have a general question (which is why you pulled a data set in the first place); in this case "Are natural disasters getting worse with climate change?". 

There may be more than one way to go after that question ... and you might need some additional exploration of the data set to generate additional questions (hypothesis) and ultimately find the one you are mot interested in. 

In this case, we could focus on whether we are indeed observing an increase in frequency of events and/or we could compare if different groups of natural disasters that are more dependent on climate/weather conditions (storms, hurricanes) are increase more rapidly compared to categories that have other origins (e.g. earthquakes or volcanic eruptions). 

Or, we could have already done some analysis (or background reading) that indicates that natural disasters linked to climate patterns are indeed predicted to get worse based on modeling. In that case we might be more interested whether the impact of natural disasters on humans is getting increasingly worse. We could be at a point where at least some of the changes to the climate system are inevitable and in that case the question of mitigation of effects and preparedness like early warning systems or infrastructure become more important. We cannot stop the events from happening but to an extent we can control how well we can deal with them.

Exploratory Analysis is fundamentally a very creative process and thinking outside the box can be what sets your analysis apart from what everyone else is doing. However, before we can start thinking outside the box we need to first figure out what "the box is" and use that as our starting point. This does mean you should always start with the boring standard stuff, cover your bases and then from there work towards something more interesting - though sometimes the "boring" stuff is the most revealing!


## Plot the data!

While tables are sometimes more appropriate, most exploratory analysis involves generating plots that help you get an overview of the data in relation to your question.

`r msmbstyle::question_begin(label = "ques:eda-7")`

We should make a distinction between **exploratory figures** and **explanatory** or **final figures**. Briefly contrast the two in terms of their goal and audience and how this might be reflected in the presentation of the visualization.

`r msmbstyle::question_end()`

A good place to start is to get an idea of the **variation** or **variance** of the data you are interested in.

`r msmbstyle::question_begin(label = "ques:eda-8")`

List 3 - 5 ways that you can assess the variance of a measured value. What metrics can you calculate and what options do we have to visualize?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

You can calculate summary statistics

* mean, median
* mininimum, maximum values (range)
* quantiles

(`fivenum()` will give you min, 25th percentile, median, 75th percentile, max)

To visualize

* barplots for categorical values
* histograms for continuous values (your individual bins essentially become components of a categorical value)
* boxplots to compare distributions of continuous values

For all of these assessments your should always ask the following questions

* Which values are more common than others? Why?
* Which values are more rare than others? Why?
* Are there distinct clusters that emerge?
* Do overall patterns meet my expectations?
* Are there patterns that stand out? Are they unusual/unexpected?

In each case, as you pick up patterns you want to also ask "why" questions ... those are the ones that will lead you to the next steps of your exploration.

Typically you will want to explore outlier values (are they true outliers or do they need to be removed?) and patterns of missing data (too much missing data and you might need to find a different data set)

`r msmbstyle::solution_end()`

Let's start by figuring out how many observations fall into each sub-category of natural disasters^[We finally get to see the alternative to `stat = "identity"` in using barplots! To plot distribution of categorical variable you are now telling `ggplot` to count the number of observations for each category].

```{r}

ggplot(disaster, aes(x = disaster_subgroup)) +
  geom_bar(stat = "count")

```

and we could do the same for our disaster types.

```{r}

ggplot(disaster, aes(x = disaster_type)) +
  geom_bar(stat = "count")


```

We probably want to be able to read those labels

```{r}

ggplot(disaster, aes(x = disaster_type)) +
  geom_bar(stat = "count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

This is giving us global numbers, we might want to break it up by region.

```{r fig.height=7}

ggplot(disaster, aes(x = disaster_type)) +
  geom_bar(stat = "count") +
  facet_wrap(. ~ region) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

If we want to think specifically about whether impact is increasing we should filter our data set to include only events for which damages where assessed.

Let's create new object with only those observations and then get an overview of distribution of the damage incurred.

```{r}

damages <- disaster %>%
  filter(!is.na(total_damages_000_us))


ggplot(damages, aes(x = total_damages_000_us)) +
  geom_histogram()

```

That looks like we might have some outliers that are skewing our histogram. We can try looking at e.g. a square root scale, which skews the y-axis in a systematic way^[This is the type of thing where in order to not be unintentionally mislead you would always want to add "Note, the skewed y-scale"].

```{r}

ggplot(damages, aes(x = total_damages_000_us)) +
  geom_histogram() +
  scale_y_sqrt()

```

Another way would be to set the binwidths manually, let's also divide our dollar amounts by 1,000,000 to make the x-axis more legible.

```{r}

ggplot(damages, aes(x = total_damages_000_us/1000000)) +
  geom_histogram(binwidth = 5)

```

Let's look at variation within different disaster types.

```{r}

ggplot(damages, aes(x = total_damages_000_us/1000000)) +
  geom_histogram(binwidth = 5) +
  facet_wrap(. ~ disaster_type)

```

It looks like we might want to limit our assessment to droughts, earthquakes, wildfires, floods, and storms. To make these comparisons if might be more helpful to switch to a boxplot.

```{r}

damages <- damages %>%
  filter(disaster_type %in% c("Drought", "Earthquake", "Flood", "Storm", "Wildfire"))

ggplot(damages, aes(x = disaster_type, y = total_damages_000_us/1000000)) +
  geom_boxplot()
  

```

We can also quickly calculate a set of summary stats for comparison

```{r}

damages %>%
  group_by(disaster_type) %>%
  summarize(mean = mean(total_damages_000_us)/1000000,
            sd = sd(total_damages_000_us)/1000000,
            min = min(total_damages_000_us)/1000000,
            max = max(total_damages_000_us)/1000000) %>%
  kable()

```

After you've explored patterns within variables you are interested in you are also going to want to explore relationships (**Covariation**) among variables^[Remember, when we start looking at correlation, we always need to be careful about how/when we infer causation!].

`r msmbstyle::question_begin(label = "ques:eda-9")`

List 3 - 5 ways that you can assess the covariance of a measured value.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

You can calculate summary statistics and compare them, but for initial exploration of patterns visualizations are going to be much more handy.

You could create faceted barplot or have bars next to each other in a figure; faceted historgrams can be helpful too. But especially with increasing number of comparisons, one of the most straightforward ways to compare distributions are boxplots.

Comparing two categorical values can be challenging - heat plots can be used to visualize the number of observations that fulfill both categories, for two continuous values you can use scatter plots (though these become less useful with increasing number of observations in your data set).

`r msmbstyle::solution_end()`

We have already spent a little bit of time looking at more than one variable by looking at subgroups with in our data set, in this case the disaster types and regional differences. In our case the relationship we are really interested in is change over time. This means we are comparing two continuous values but it is a specific comparison we frequently describe as a time-series.

Let's take a look

```{r}

ggplot(damages, aes(x = year, y = total_damages_000_us/1000000)) +
  geom_point()

```

We could enhance this figure by e.g. color coding by disaster type.

```{r}

ggplot(damages, aes(x = year, y = total_damages_000_us/1000000, fill = disaster_type)) +
  geom_point(shape = 21)

```

Or we could color code by continent and make individual panels for different disaster types.

```{r}

ggplot(damages, aes(x = year, y = total_damages_000_us/1000000, color = continent)) +
  geom_point() +
  facet_wrap(. ~ disaster_type)

```

An alternative approach might be assess whether we are seeing changes in scale of each disaster and whether those show the same pattern as damages incurred, because the question we are really after is whether despite frequency and intensity increasing (or at least predicted to increase) that is also resulting in a scaling up of the damages.

Each of these has a different way of being measure so we will need to look at each disaster type in turn.

```{r}

damages %>%
  filter(disaster_type == "Storm") %>%
  ggplot(aes(x = dis_mag_value, y = total_damages_000_us/1000000, color = year)) +
  geom_point() +
  scale_color_viridis_c()

```

```{r}

damages %>%
  filter(disaster_type == "Flood") %>%
  ggplot(aes(x = dis_mag_value, y = total_damages_000_us/1000000, color = year)) +
  geom_point() +
  scale_color_viridis_b()

```

```{r}

damages %>%
  filter(disaster_type == "Earthquake") %>%
  ggplot(aes(x = dis_mag_value, y = total_damages_000_us/1000000)) +
  geom_point()

```

```{r}

damages %>%
  filter(disaster_type == "Drought") %>%
  ggplot(aes(x = dis_mag_value, y = total_damages_000_us/1000000)) +
  geom_point()

```

```{r}

damages %>%
  filter(disaster_type == "Wildfire") %>%
  ggplot(aes(x = dis_mag_value, y = total_damages_000_us/1000000)) +
  geom_point()

```

So far we haven't really found any interesting patterns that pop out - though keeping mind that "non-patterns" are also noteworthy.

We do know that we tend to have a pretty tight distribution with damages being pretty similar across events but then we have individual events that are outlier incurring much higher damages. Since are looking at very broad-scale patterns of it might make sense to **transform** our data in order to get a new metric that enables a more helpful comparison. For example, we could calculate total costs incurred per year for each disaster type and the compare how those total values are changing over time.

```{r}

total_yr <- disaster %>%
  filter(!is.na(total_damages_000_us)) %>%
  group_by(year,disaster_type) %>%
  summarize(total_damages_yr = sum(total_damages_000_us))

```
Let's plot that data set.

```{r}

ggplot(total_yr, aes(x = year, y = total_damages_yr/1000000, color = disaster_type)) +
  geom_point()

```
Let's split those into different panels.

```{r fig.width=6, fig.height=6}

ggplot(total_yr, aes(x = year, y = total_damages_yr/1000000)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(. ~ disaster_type) +
  theme(legend.position = "bottom")

```

Now we're getting somewhere ... I added all the disaster types back in. We also need to keep in mind that increase in total damages per year can be a combination of more individual events but all have small number of damages incurred or if we have more incidence of high cost events and of course, it could be a combination thereof.

Let's create a data set with limited disaster types so we can elucidate patterns more easily, and while we're at it also calculate frequency of each disaster type per year.

```{r}

total_yr <- disaster %>%
  filter(!is.na(total_damages_000_us) & disaster_type %in% c("Drought", "Earthquake", "Extreme Temperature",
                                                               "Flood", "Storm", "Wildfire")) %>%
  group_by(year,disaster_type) %>%
  summarize(total_damages_yr = sum(total_damages_000_us),
            events_yr = n())

```

Let's plot this revised data set.

```{r}

ggplot(total_yr, aes(x = year, y = total_damages_yr)) +
  geom_point() +
  facet_wrap(. ~ disaster_type)

```
And let's quickly check what the relationship is of frequency and total damages.

```{r}

ggplot(total_yr, aes(x = events_yr, y = total_damages_yr)) +
  geom_point() +
  geom_smooth()

```
So the number of events definitely appears to play a role in the total damages due to various disaster types per year, though we would need a more format analysis to disentangle those effects.

Are multi-panel time-series data seems to have the data transformed in a way that will give us an answer to our question - but it's not really a great visualization if we wanted to share results from our exploratory analysis. Generally, you'll hit a point where you know have the data transformed in a way that is useful and you could start playing around with optimizing the presentation of that data set.

So let's start shifting from our **exploratory** figures to an **explanatory** figure that we might include in a report or an update to our collaborators.

One of the problems with our multiplot visualization is that the scales for all of the y-axis are the same because we have some outlier that compress the scale. Maybe a traditional scatterplot is not ideal. Because we are interested in changes in magnitude we could try a less traditional plot, like a bubble chart. Instead of plotting the total damages incurred on the y-axis, we can plot our disaster types and then adjust the size of each bubble according to damages incurred.

```{r}

ggplot(total_yr, aes(x = year, y = disaster_type, size = total_damages_yr/1000000)) +
  geom_point()

```
Now we're cooking!

Let's think of some ways that we can refine this plot. We have overlapping points, we also might want additional bin sizes.

```{r}

ggplot(total_yr, aes(x = year, y = disaster_type, size = total_damages_yr/1000000, fill = disaster_type)) +
  geom_point(shape = 21, alpha = .5) +
  scale_size_continuous(breaks = c(10, 25, 50, 100, 200),
                        name = "damages [USD]") +
  theme(legend.position = "bottom")

```
Let's see if we can fix our legend to something more visible, we might want different colors, and we'll need to add axis labels, plot titles etc.

```{r fig.width=6, fig.height=5}

ggplot(total_yr, aes(x = year, y = disaster_type, size = total_damages_yr/1000000, fill = disaster_type)) +
  geom_point(shape = 21, alpha = .5) +
  scale_fill_viridis_d() +
  scale_size_continuous(breaks = c(5, 10, 25, 50, 100, 200),
                        name = "damages [USD]") +
  labs(x = "total damages incurred [USD]", y = "disaster type",
       title = "Economic costs incurred by natural disasters (1900 - 2020)",
       subtitle = "The size of the bubble represents the total damages incurred by all events in a given category.",
       caption = "data: EMDAT/International Disaster Database") +
  theme_standard +
  theme(legend.position = "bottom") + 
  guides(fill = FALSE,
         size = guide_legend(override.aes = list(fill = "black", alpha = 1)))


```

A final step can be starting to ask whether patterns are specific to the data set or whether there are true non-random relationships between variables; this is starts to overlap with a more formal analysis now that you have an overview of what patterns you want to explore and will not always be part of your exploratory analysis. Typical questions we might further explore at this point include e.g.

* Is this pattern due to random change or does the independent variable have significant explanatory power?
* How can the relationship implied by the pattern be described (e.g. linear regression)?
* How strong is the relationship (R2 value)?
* What other variables might affect or explain the relationship (e.g. are these variable correlated because they are both correlated to another variable that is responsible for the mechanism?)
* Does this relationship hold across subgroups of the relationship itself?

Fitting trend lines helps elucidate patterns (especially in time series). This is frequently where exploratory analysis transitions into a more formal analysis that involves statistical tests and fitting models. Keep in mind that we want to have a good grasp on whether our analysis is descriptive, inferential or causal/mechanistic in nature. Exploratory analysis is "quick and dirty", you are mainly focused on summarizing the data and highlighting broad-scale patterns that emerge. They are useful for assessing the quality of your data, eye-balling if patterns are what you expect (was your hypothesis correct(ish)), and getting and idea of what the next steps in a more formal analysis should be.

## Iterate your exploration

Now that we have an answer(ish) to our question - the first thing we should do is question our answer^[especially if your answer conforms to your *a priori* expectations!]. Always consider limitations of your data, whether there are alternative explanations, if your data has some inherent issues you need to be aware of etc. It can also be helpful to find at least one external data source to assess whether your answers are roughly in line with other (expected) measurements.

It is called an exploratory analysis for a reason, because it is just the beginning. Usually this is the beginning of a more sophisticated analysis. Important questions to ask always include

* Is your data appropriate for the question your are asking? 
* Do you need additional data to refine your answer?
* Do you have the right question or do you need to refine your question?

Exploratory Analysis frequently is more about hypothesis generating, rather than hypothesis testing. So you data exploration can be an important step to defining the precise (set of) question(s) you want to explore. Not infrequently, the exploration of an initial question leads you to generating additional (perhaps more interesting) questions.

So for example, one of the first questions we might ask is whether the pattern we see with higher damages being incurred holds if we compare this to loss of life.

```{r fig.width=6, fig.height=5}

deaths <- disaster %>%
  filter(!is.na(total_deaths)) %>%
  group_by(year, disaster_type) %>%
  summarize(total_deaths_yr = sum(total_deaths)) 

ggplot(deaths, aes(x = year, y = disaster_type, size = total_deaths_yr/100000, fill = disaster_type)) +
  geom_point(shape = 21, alpha = .5) +
  scale_fill_viridis_d() +
  scale_size_continuous(breaks = c(.5, 1, 5, 10, 15, 30),
                        name = "100k deaths") +
  labs(x = "total deaths incurred", y = "disaster type",
       title = "Total deaths due to natural disasters (1900 - 2020)",
       subtitle = "The size of the bubble represents the total loss of life incurred by all events per year.",
       caption = "data: EMDAT/International Disaster Database") +
  theme_standard +
  theme(legend.position = "bottom") + 
  guides(fill = FALSE,
         size = guide_legend(override.aes = list(fill = "black", alpha = 1)))

```

Additionally we could break down regional patterns, or even pull in additional data of GDP of individual countries to compare costs incurred scaled to the economies of different countries.

It's always helpful to take notes as you go to jot down questions and also ideas on what might explain certain patterns you see.


## Now you!

As you suspected, training wheels are coming off.

`r msmbstyle::question_begin(label = "ques:eda-10")`

Formulate three specific questions this data set can answer, consider that you have options like looking at specific geographic subsets, or different categories of natural disasters, you can also consider whether your would expect different types of natural disasters to show similar or different patterns.

Use headers, plain text, code chunks etc. as appropriate to write a mini report for each question - this should include stating your question as concisely as possible, exploratory figures, final explanatory figure, description & discussion of your results. Comment your code to generate your figures **line by line** with descriptive comments.

For each question, generate 3 - 5 exploratory figures that can help answer your question, these can include different visualizations of the same variables or different (combinations of) variables. For each question, chose the figure that summarizes your results best and refine your visualization so that it best summarizes your results, this includes a figure title, labeled axis, color choice, how data is encoded etc.

*It's possible that you have some additional ideas on how you would like to refine your figure but you're not sure how you can achieve that using your current ggplot skill set. If that is the case, do not get hung up on cosmetics, rather make a note and we will refine figures based on all of our feedback in class.*

Write a figure legend that would accompany your figure in a report or paper, write a 3 - 5 sentence description of each figure (i.e. this should communicate the results with sufficient detail that somebody who *has not* seen the figure still fully understand them) and briefly discuss your results in 3-5 sentences in the context of the question you were asking (i.e. what are your conclusions, are there limitation to your data, alternative explanations, ...).

For each initial question, jot down 2-3 ideas of where you would take your exploratory analysis in the next step.

Chose one question, your final figure and accompanying legend, description, and discussion and share it in our `#hwassignments` channel on slack.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:eda-11")`

Take a look your classmate's posts on slack, identify what you like about the visualization along with some **constructive** points for improvement. Getting feedback is really important. Our goal is clear communication; you have spent more time thinking about this question and the visualization so some things might be obvious to you that others might not catch right away, similar a set of colors might look good to you, but somebody else might struggle to tell things apart. The figure might be too busy etc. Sometimes it can also be helpful to have other people help you refine your question.

*Make some notes for yourself so we can share in class; you do not need to include this in your HW submission. You should also make some notes on constructive criticism of your own figures of things you may have wanted to improve by weren't sure how to achieve that.*

`r msmbstyle::question_end()`

`r msmbstyle::question_begin(label = "ques:eda-11")`

Our next step will be exploring where we can find data sets. 

Your final project can be summed up as "find a data set - do something with it". Chances of finding things are always a lot higher when you have at least a rough idea of what you are looking for. In preparation for formulating a mini-proposal for your final project, identify a broad area of interest for you and brainstorm 2-3 ideas of more specific questions you're interested in. 

Don't worry, this is not final, just a starting point. As we take a look at databases and data repositories you might something in a different area you are interested in exploring.

`r msmbstyle::question_end()`





