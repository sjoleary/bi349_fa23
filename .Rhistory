# load object into environment
load("results/taxotab.rds")
#| label: setup
#| include: false
# set options
knitr::opts_chunk$set(
tidy = FALSE,
message = FALSE,
warning = FALSE)
options(htmltools.dir.version = FALSE)
BiocManager::install("phyloseq")
library(phyloseq)
library(vegan)
# load libraries
library(knitr)
library(tidyverse)
# ASV table
load("data/seqtab.nochim.rdata")
# Taxonomy table
load("data/taxotab.01.05.rdata")
# load sample data
soil <- read_delim("data/soil.csv", delim = ";")
View(soil)
# create phyloseq object
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(soiltab),
tax_table(as.matrix(taxo$tax)))
# create phyloseq object
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(soil),
tax_table(as.matrix(taxo$tax)))
View(seqtab.nochim)
View(taxo)
taxo[["tax"]]
# create phyloseq object
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(soil),
tax_table(as.matrix(taxo$tax)))
as.matrix(taxo$tax)
load(file = "data/ps.rdata")
View(ps)
ntaxa(ps)
ps_fungi <- subset_taxa(ps, Kingdom == "k_Fungi")
ps_fungi <- subset_taxa(ps, Kingdom == "k__Fungi")
ps_fungi_nosd_log <- transform_sample_counts(ps.fungi.nosd, function(x) log(x+1))
ps_fungi_nosd_log <- transform_sample_counts(ps_fungi_nosd, function(x) log(x+1))
ps_fungi_nosd <- filter_taxa(ps.fungi, function(x) sum(x) > 2, TRUE)
ps_fungi_nosd <- filter_taxa(ps_fungi, function(x) sum(x) > 2, TRUE)
ps_fungi_nosd_log <- transform_sample_counts(ps_fungi_nosd, function(x) log(x+1))
ps_fungi_nosd_hel <- transform_sample_counts(ps.fungi.nosd, function(x) sqrt(x/sum(x)))
ps_fungi_nosd_hel <- transform_sample_counts(ps_fungi_nosd, function(x) sqrt(x/sum(x)))
ps_transf = tax_glom(ps_fungi_nosd_hel, "Species", NArm = FALSE)
View(soil)
View(soil)
citr:::insert_citation()
otu_table(ps_transf)[1:2, 1:2]
otu <- otu_table(ps_transf)
otu
otu <- otu_table(ps_transf) %>%
as.data.frame()
View(otu)
tax <- tax_table(ps_transf)
tax
tax <- tax_table(ps_transf) %>%
as.data.frame()
View(tax)
tax <- tax_table(ps_transf) %>%
as.data.frame() %>%
rows_as_columns("asv")
tax <- tax_table(ps_transf) %>%
as.data.frame() %>%
columns_as_rows("asv")
tax <- tax_table(ps_transf) %>%
as.data.frame() %>%
rownames_to_column("asv")
asv_counts <- otu_table(ps_transf) %>%
as.data.frame()
View(asv_counts)
asv_tax <- tax_table(ps_transf) %>%  #
as.data.frame() %>%            #
rownames_to_column("asv")      #
View(asv_counts)
View(asv_tax)
asv_counts <- otu_table(ps_transf, taxa_are_rows = TRUE) %>%
as.data.frame()
View(asv_counts)
asv_counts <- otu_table(ps_transf, taxa_are_rows = FALSE) %>%
as.data.frame()
head(asv_tax)
View(asv_counts)
asv_counts <- otu_table(ps_transf, taxa_are_rows = FALSE) %>%
as.data.frame() %>%
t()
asv_counts <- otu_table(ps_transf) %>%  #
t() %>%                               #
as.data.frame() %>%                   #
rownames_to_column("asv")             #
View(asv_counts)
View(asv_counts)
tax_count <- asv_counts %>%
left_join(asv_tax)
View(tax_count)
colnames(tax_count)
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(Kingdowm, Phylum, Class, Order, Family, Genus, Species, everything())
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(Kingdom, Phylum, Class, Order, Family, Genus, Species, everything())
head(tax_count)
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(Kingdom, Phylum, Class, Order, Family, Genus, Species, everything())
head(tax_count)
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(-asv, Kingdom, Phylum, Class, Order, Family, Genus, Species, everything())
head(tax_count)
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(-asv, Kingdom, Phylum, Class, Order, Family, Genus, Species, everything())
head(tax_count)
tax_count <- asv_counts %>%
left_join(asv_tax) %>%
select(Kingdom, Phylum, Class, Order, Family, Genus, Species, everything(), -asv)
head(tax_count)
tidy_counts <- tax_count %>%
pivot_longer(names_to = "sample", values_to = "count")
tidy_counts <- tax_count %>%
pivot_longer(names_to = "sample", values_to = "count", 8:81)
head(tax_count)
head(tidy_counts)
unique(tidy_counts$sample)
length(unique(tidy_counts$sample))
View(soil)
#| echo: false
tidy_counts <- tax_count %>%
pivot_longer(names_to = "ID", values_to = "count", 8:81)
head(tidy_counts)
tidy_counts <- tidy_counts %>%
left_join(soil)
head(tidy_counts)
tidy_counts %>%
distinct(Phylum) %>%
separate(Phylum, into = c("tmp", "Phylum"), sep = "__")
kable(
tidy_counts %>%
distinct(Phylum) %>%
separate(Phylum, into = c("tmp", "Phylum"), sep = "__")
)
kable(
tidy_counts %>%
distinct(Phylum) %>%
separate(Phylum, into = c("tmp", "Phylum"), sep = "__") %>%
arrange(Phylum)
)
View(soil)
kable(
tidy_counts %>%
group_by(forest, phylum) %>%
summarize(mean_rel_abund = mean(count))
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count))
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
pivot_wider(names_from = "forest", values_from = "mean_rel_abundance")
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
pivot_wider(names_from = "forest", values_from = "mean_rel_abund")
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
mutate(mean_abund = mean_rel_abund*100) %>%
pivot_wider(names_from = "forest", values_from = "mean_abund"),
digits =
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
mutate(mean_abund = mean_rel_abund*100) %>%
pivot_wider(names_from = "forest", values_from = "mean_abund"),
digits = 2
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
mutate(mean_abund = mean_rel_abund*100) %>%
select(-mean_rel_abund) %>%
pivot_wider(names_from = "forest", values_from = "mean_abund"),
digits = 2
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
pivot_wider(names_from = "forest", values_from = "mean_abund"),
digits = 4
)
kable(
tidy_counts %>%
group_by(forest, Phylum) %>%
summarize(mean_rel_abund = mean(count)) %>%
pivot_wider(names_from = "forest", values_from = "mean_rel_abund"),
digits = 4
)
tidy_counts
diversity <- tidy_counts %>%
group_by(ID) %>%
summarize(richness = specnumber(count),
shannon = diversity(count, index = "shannon"),
simpson = diversity(count, index = "simpson"),
invsimpson = diversity(count, index = "invsimpson"),
total_reads = sum(n)) %>%
pivot_longer(cols = c(richness, shannon, simpson, invsimpson),
names_to = "metric")
diversity <- tidy_counts %>%
group_by(ID) %>%
summarize(richness = specnumber(count),
shannon = diversity(count, index = "shannon"),
simpson = diversity(count, index = "simpson"),
invsimpson = diversity(count, index = "invsimpson")) %>%
pivot_longer(cols = c(richness, shannon, simpson, invsimpson),
names_to = "metric")
diversity
diversity <- tidy_counts %>%
group_by(ID, forest) %>%
summarize(richness = specnumber(count),
shannon = diversity(count, index = "shannon"),
simpson = diversity(count, index = "simpson"),
invsimpson = diversity(count, index = "invsimpson")) %>%
pivot_longer(cols = c(richness, shannon, simpson, invsimpson),
names_to = "metric")
diversity
ggplot(diversity, aes(x = ID, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 3) +
facet_grid(metric ~ ., scales = "free") +
labs(x = " ", y = " ")
ggplot(diversity, aes(x = ID, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 1) +
facet_grid(metric ~ ., scales = "free") +
labs(x = " ", y = " ")
ggplot(diversity, aes(x = forest, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 1) +
facet_grid(metric ~ ., scales = "free") +
labs(x = " ", y = " ")
ggplot(diversity, aes(x = forest, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 3) +
facet_grid(metric ~ ., scales = "free") +
labs(x = " ", y = " ")
View(soil)
ggplot(diversity, aes(x = forest, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 3) +
facet_grid(metric ~ horizon, scales = "free") +
labs(x = " ", y = " ")
diversity <- tidy_counts %>%
group_by(ID, forest, horizon) %>%
summarize(richness = specnumber(count),
shannon = diversity(count, index = "shannon"),
simpson = diversity(count, index = "simpson"),
invsimpson = diversity(count, index = "invsimpson")) %>%
pivot_longer(cols = c(richness, shannon, simpson, invsimpson),
names_to = "metric")
ggplot(diversity, aes(x = forest, y = value, color = forest)) +
geom_boxplot(aes(color = forest), outlier.shape = NA, fill = "transparent", size = 1) +
geom_point(aes(fill = forest),
position = position_jitterdodge(jitter.width = .3),
shape = 21, color = "black", size = 3) +
facet_grid(metric ~ horizon, scales = "free") +
labs(x = " ", y = " ")
library(ggthemes)
# Chunk 1: setup
#| label: setup
#| include: false
library(ggplot2)
library(ggthemes)
# custom functions ----
theme_standard <- theme_classic() +
theme(
axis.text = element_text(size = 11, color = "black"),
axis.title = element_text(size = 16, color = "black"),
axis.title.y = element_text(vjust = 1.5, color = "black"),
axis.line = element_line(color = "black"),
legend.position = "bottom",
panel.background = element_rect(fill = "white", color = NA),
panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
strip.background = element_rect(fill = "grey95", color = "black"),
strip.text.x = element_text(size = 16, color = "black"),
strip.text.y = element_text(size = 16, color = "black"))
theme_facet <- theme_classic() +
theme(
axis.text = element_text(size = 11, color = "black"),
axis.title = element_text(size = 16, color = "black"),
axis.title.y = element_text(vjust = 1.5, color = "black"),
axis.line = element_line(color = "black"),
legend.position = "bottom",
panel.background = element_rect(fill = "white", color = NA),
panel.border = element_rect(fill = NA, color = "black"),
panel.grid.major = element_line(color = "grey85"),
panel.grid.minor = element_blank(),
strip.background = element_rect(fill = "grey95", color = "black"),
strip.text.x = element_text(size = 16, color = "black"),
strip.text.y = element_text(size = 16, color = "black"))
# other options ----
knitr::opts_chunk$set(
tidy = FALSE,
message = FALSE,
warning = FALSE)
options(htmltools.dir.version = FALSE)
# Chunk 3
# load libraries ----
# documentation
library(knitr)
# eDNA analysis
library(dada2)
# wrangling
library(tidyr)
library(dplyr)
library(tibble)
library(readr)
library(glue)
# visualization
library(ggplot2)
library(ggthemes)
library(plotly)
library(patchwork)
# Chunk 4
# designate file paths ----
# create variable with initially trimmed reads
raw <- "data/seq/raw"
# path for filtered reads
filt <- "data/seq/filt"
# create lists of matched forward & reverse fastq files ----
# forward reads
fnFs <- sort(list.files(raw, pattern = "_R1.fastq", full.names = TRUE))
# reverse reads
fnRs <- sort(list.files(raw, pattern = "_R2.fastq", full.names = TRUE))
# create vector with sample names
sample.names <- substr(basename(fnFs), 1, (nchar(fnFs)-25))
# create file names & paths for filtered reads ----
# named vectors of forward reads
filtFs <- file.path(filt, glue("{sample.names}_R1.fastq.gz"))
names(filtFs) <- sample.names
# named vectors of reverse reads
filtRs <- file.path(filt, glue("{sample.names}_R2.fastq.gz"))
names(filtRs) <- sample.names
# Chunk 5: fig-err-forw
#| label: fig-err-forw
#| fig-cap: "Quality scores for the forward reads of the first sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is em quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position."
#| fig-height: 9
ggplotly(plotQualityProfile(fnFs[1]))
# Chunk 6: fig-err-rev
#| label: fig-err-rev
#| fig-cap: "Quality scores for the reverse reads in 12th sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position."
#| fig-height: 9
#| echo: false
ggplotly(plotQualityProfile(fnRs[12]))
# Chunk 7
# filter reads
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, #
truncLen = c(280, 280),     #
trimLeft = c(18, 2),        #
truncQ = 6,                 #
maxEE = c(2,2),             #
minLen = 35,                #
rm.phix = TRUE,             #
matchIDs = FALSE,           #
compress = TRUE)            #
# Chunk 9: tbl-filt-results
#| label: tbl-filt-results
#| tbl-cap: "Mean and standard deviation of the proportion of reads lost due to quality filtering and the mean and standard deviation of the number of remaining reads per sample post filtering."
#| echo: false
# summarize proportion of reads lost during trimming
check <- as.data.frame(out) %>%
mutate(perc_lost = 100-(reads.out/reads.in*100)) %>%
summarize(mean_loss = mean(perc_lost),
std_loss = sd(perc_lost),
mean_reads = mean(reads.out),
std_reads = sd(reads.out))
kable(
check
)
# Chunk 10: fig-err-for-fil
#| label: fig-err-for-fil
#| fig-cap: "Quality scores of forward reads for 16 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position."
#| fig-height: 9
#| echo: false
ggplotly(plotQualityProfile(filtFs[4]))
# Chunk 11: fig-err-rev-fil
#| label: fig-err-rev-fil
#| fig-cap: "Quality scores of reverse reads for 12 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position."
#| fig-height: 9
#| echo: false
ggplotly(plotQualityProfile(filtRs[4]))
# Chunk 12
#| cache: true
# forward reads
errF <- learnErrors(filtFs,               #
nbases = 1e8,         #
multithread = FALSE)  #
# reverse reads
errR <- learnErrors(filtRs,
nbases = 1e8,
multithread = FALSE)
# Chunk 13: fig-err-rates
#| label: fig-err-rates
#| fig-cap: "Error rates for each possible transition for forward (top) and reverse reads (bottom). Observed (grey points) and estimated (black line) error rates for each consensus quality score. Expected error rates for nominal definition of the quality score are in red."
#| fig-height: 15
# Visualize estimated error
p1 <- plotErrors(errF, nominalQ = TRUE)
p2 <- plotErrors(errR, nominalQ = TRUE)
p1 / p2
# Chunk 14
#| results: hide
derepFs <- lapply(filtFs,
derepFastq,
verbose = FALSE)
derepRs <- lapply(filtRs,
derepFastq,
verbose = FALSE)
# Chunk 15
#| results: hide
dadaFs <- dada(derepFs,
err = errF,
selfConsist = FALSE,
multithread = TRUE)
dadaRs <- dada(derepRs,
err = errR,
selfConsist = FALSE,
multithread = TRUE)
# Chunk 16
#| results: hide
mergeASVs <- mergePairs(dadaFs, filtFs,
dadaRs, filtRs,
minOverlap = 12,
maxMismatch = 0)
# Chunk 17
head(mergeASVs[1])
# Chunk 18
# Largest overlap
max(mergeASVs[[1]]$nmatch)
# Smallest overlap
min(mergeASVs[[1]]$nmatch)
# Chunk 19
seqtab <- makeSequenceTable(mergeASVs)
# Chunk 20
dim(seqtab)
seqtab[,1]
# Chunk 21
seqtab.nochim <- removeBimeraDenovo(seqtab,
method = "consensus",
multithread = FALSE,
verbose = TRUE)
# Chunk 22: fig-seq-length-ASV
#| label: fig-seq-length-ASV
#| fig-cap: "Distribution of sequence length for merged ASVs. The red dotted line indicates sequences that are 100-105bp long. We are targeting a 106 bp amplicon and are using 2x150 bp sequencing. Primer sites are approx 25bp each. "
#| echo: false
getSequences(seqtab) %>%
nchar() %>%
as.data.frame() %>%
ggplot(aes(x = .)) +
geom_histogram(binwidth = 5, color = "black", fill = "darkorange") +
labs(x = "sequence length", y = "number of unique sequences") +
theme_standard
# custom function to get read numbers
getN <- function(x) sum(getUniques(x))
# create data table with number of reads per sample at each step
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergeASVs, getN), rowSums(seqtab.nochim)) %>%
as.data.frame() %>%
rownames_to_column("sample") %>%
rename(trimmed = reads.out,
denoisedF = V3,
denoisedR = V4,
merged = V5,
rm_chimera = V6) %>%
pivot_longer(names_to = "filter", values_to = "reads", cols = 2:7) %>%
mutate(filter = ordered(filter, levels = c("reads.in", "trimmed", "denoisedF", "denoisedR", "merged", "rm_chimera")),
k_reads = reads/1000)
# plot distribution
ggplot(track, aes(x = filter, y = k_reads)) +
geom_boxplot(fill = "darkorange") +
labs(x = "filter/processing step", y = "thousand reads per sample") +
theme_standard
