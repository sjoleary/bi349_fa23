[
  {
    "objectID": "D_clim-change.html#investigating-climate-change",
    "href": "D_clim-change.html#investigating-climate-change",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Investigating Climate Change",
    "text": "Investigating Climate Change\nThe most recent IPCC Asessement has made two very clear statements regarding climate change:\n\nClimate change is “unequivocally” caused by humans.\nThe impact of anthropogenic climate change is “unprecedented”.\n\nBased on their assessment of studies having looked at global warming and climate change to date, the global mean warming is estimated at 1.1°C compared to the pre-industrial record. Additionally, under every emission scenario and average warming of 1.5 °C is likely to be reached in in next 20 years. Finally, some effects such as melting glaciers, ice sheets and the permafrost have been deemed irreversible.\nIn this module we are going to learn about data visualization using ggplot2 as we explore drivers and impacts of climate change. For our exploration of the drivers of climate change we will pull data sets describing temperature and greenhouse gas concentrations past and present. As we explore the impacts, we will consider whether the effects of climate change are something we will observe in the future if no action is taken or if we are already experiences widespread impacts changing the earth-climate system in ways that we can currently observe. And while we’re on the topic of visualizations and climate change we will then also explore how visualizations are used for misinformation."
  },
  {
    "objectID": "D_clim-change.html#the-earth-climate-system",
    "href": "D_clim-change.html#the-earth-climate-system",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The earth-climate system",
    "text": "The earth-climate system\nThe Earth-climate system, also known as the Earth’s climate system, refers to the complex and interconnected set of physical, chemical, biological, and atmospheric processes that govern the Earth’s climate. It describes all the components and interactions that determine the planet’s climate patterns and conditions over time.\nThe climate system is comprise of five major components: The hydrosphere, cryosphere, atmosphere, lithosphere, and biosphere.\n\nThe atmosphere consists of a mixture of gases, primarily nitrogen and oxygen, along with trace gases like carbon dioxide (CO2), methane (CH4), and water vapor (H2O). These gases play a critical role in regulating the planet’s temperature through the greenhouse effect.\nThe hydrosphere encompasses all of Earth’s water, including oceans, lakes, rivers, glaciers, and groundwater. The movement and distribution of water in the hydrosphere have a significant impact on climate patterns and weather events.\nThe cryosphere includes all of Earth’s frozen water, such as polar ice caps, glaciers, and permafrost. Changes in the cryosphere, such as melting ice, can have profound effects on sea levels and regional climate.\nThe lithosphere refers to Earth’s solid outer layer, including the continents, ocean floors, and the Earth’s crust. It plays a role in the redistribution of heat and the formation of land forms that can influence climate patterns.\nThe biosphere consists of all living organisms on Earth, including plants, animals, and microorganisms. Biological processes, such as photosynthesis and respiration, influence the composition of greenhouse gases in the atmosphere and affect climate.\n\nYou can think of the biosphere as the global ecosystem composed of all living organisms and the abiotics factors they derive energy and nutrients from. Another way to think of it is all the regions of the lithosphere, atmosphere, hydrosphere, and cryosphere occupied by living organisms. The fact that it is comprised of living organisms (biotic factors) sets it apart from the other components of the earth climate system.\nOverall, the climate system is an interactive system acted on by internal and external forcing mechanisms.\nThe components of the climate system are open systems with the freedom to exchange mass, heat, and momentum. For example the ocean and atmosphere exchange gases like carbon dioxide as the ocean acts a large carbon sink. Similarly, we observe an exchange of mass through the water cycle that links the atmosphere and hydrosphere through processes such as evaporation and condensation/precipitation. Even here in New England you can observe the exchange of heat as summer (air) temperatures cause the ocean to warm (minimally). Finally, surface waves are the result of the exchange of momentum as the wind causes the surface waters to move.\nNext to these internal mechanisms of the different components of the earth climate system impacting each other from within, the earth climate system is additionally impacted by external factors, primarily solar radiation. For example, the output of the sun heats the hydropshere and atmosphere. As the lithosphere encompasses the rigid outer part of the earth consisting of the crust and upper mantle, plate tectonics and volcanic eruptions are frequently also considered external mechanisms."
  },
  {
    "objectID": "D_clim-change.html#climate-regimes",
    "href": "D_clim-change.html#climate-regimes",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Climate regimes",
    "text": "Climate regimes\nWeather is the condition of the atmosphere for a specific time & place – climate is a long-term statistical portrait of a specific place, region, or the entire planet.\nWeather is a snapshot of atmopsheric conditions at a specific time for a specific place. It is directly observable and can be broken down into readily measureable, discrete characteristics including temperature and precipitation but also extending to include among others wind speed and direction, cloud cover and type, visibility, or air pressure.\nBy contrast, climate comprises the statistical averages of weather of long-term timescales & involves behavior of entire complex earth system. Generally, climate refers to the long-term patterns and average weather conditions along with extremes in a specific place, region or on Earth as a whole based on a single or multiple stations (locations). It represents the statistical summary of weather patterns over an extended period, typically 30 years or more.\n\n\n\nChanges in weather, climate variability, and climate change occur on very different time scales.\n\n\nBoth weather and climate do vary over time for natural reasons but on very different time scales. While weather can change at a moment’s notice2, climate variability describes (natural) shifts in climate conditions on decadal time scales. Finally, climate change describes long-term changes on scales of centuries to millenia.2 And as we all know the length of the “moment’s notice” is inversely proportional to the probability of you wearing an umbrella/having a jacket with you."
  },
  {
    "objectID": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "href": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The energy budget and global temperatures",
    "text": "The energy budget and global temperatures\nTemperature is a primary determinant of climate. Overall, earth maintains a stable average temperature (climate) by balancing energy received from the sun with energy emitted by earth back into space. Global temperature is a function of how much energy the earth receives and stores which in turn is influenced by three major factors:\n\nThe amount of energy received from the sun.\nReflection of energy by earth’s surface.\nAtmospheric composition (greenhouse gas effect).\n\nThe Earth’s energy budget is a concept that describes the balance between the incoming energy from the Sun and the outgoing energy radiated back into space from the Earth. It provides a framework for understanding how energy flows into and out of the Earth’s climate system. The Earth’s energy budget is essential for maintaining the planet’s temperature and climate.\n\n\n\nThe atmospheric energy budget source: weather.gov\n\n\nA material may transmit, reflect, emit or absorb radiation, and generally does more than one of these at a time. Earth’s energy budget consists of two different form of radiation\n\nIncoming shortwave radiation from the sun (Insolation): This is the energy received from the Sun. Sunlight, or solar radiation, is the primary source of energy for the Earth’s climate system. It includes both wavelengths in the visible and non-visible range, primarily UV.\nOutgoing longwave radiation: As the Earth’s surface and atmosphere absorb solar energy, they emit heat in the form of infrared radiation. This outgoing longwave radiation is a crucial part of the Earth’s energy budget.\n\nWhile some gases such as Ozone absorb shortwave (UV) radiation, Greenhouse gases such as water vapor, CO2, and Methane are defined by their property that they transmit short-wave radiation but absorb longwave radiation. This means that the greenhouse gases let through the incoming solar radiation but absorb large parts of the longwave radiation being emitted from earth’s surface. This so called greenhouse effect is a crucial component of the Earth’s climate system as it turns the atmosphere into a “warm blanket”. Ultimately, the energy absorption by the atmosphere stores more energy near the earth’s surface than if there was no atmosphere, making life on the planet possible in the first place.\nHowever, human activities have led to an enhance greenhouse effect. The reason why the IPCC describes current climate change as anthropogenic is that increasing levels of atmospheric CO2 and other greenhouse gases since the Industrial Revolution are driving the rapid increase in temperatures. Earth absorbs incoming solar radiation at its surface and emits long-wave radiation to maintain the energy balance at the surface. Only as small portion of that emitted radiation goes directly into space, most of it is absorbed by greenhouse gases (e.g. CO2) in the atmosphere. For the atmosphere to maintain its energy balance it emits radiation to space and back to earth. With increasing concentrations of GHG, the atmosphere absorbs and re-emits increasingly more energy. This creates an imbalance at earth’s surface and as a response earth continues to emit more energy to re-balance the budget and as a result global temperatures increase.\nIn this module we will look at several data sets that support the fact that the currently observed climate change is indeed unprecedented in the rate of change, that it is correlated to rapidly increasing greenhouse gases and therefore consistent with the description of being unequivocally human-caused and that the impacts of climate change are currently being observed across the earth climate system in a manner consistent with rapidly increasing global temperatures."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "href": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Genetic barcoding",
    "text": "Genetic barcoding\nGenetic barcoding, also known as DNA barcoding, is a molecular biology technique used to identify and distinguish between different species of organisms by analyzing a short and standardized DNA sequence. This method is based on the idea that each species has a unique DNA barcode or genetic signature within a particular region of its genome and is particularly useful for rapid species identification, especially when dealing with complex or difficult-to-identify specimens even from small, damaged, or industrial processed material.\nGenetic barcoding relies on the ability to amplify a standard gene (locus) across a wide range of taxonomic groups using universal primers. A good barcoding locus is typically a short, conserved section of DNA that contains enough genetic variation to differentiate between species (intraspecific variance) but remains relatively constant within a species (little to no intraspecific variance). The mitochondrial cytochrome c oxidase subunit 1 (COI) gene is a commonly used barcode region in animals, while other genes or regions may be used for plants, fungi, and microorganisms. The data set we will look at is fungi for which ITS2 is commonly used.\nRegardless of which genetic region is actually implemented the key steps remain the same:\n\nDNA Extraction: Genetic material (usually DNA) is extracted from the biological sample of interest, this can be tissue, cells, or even environmental DNA (eDNA) extracted from water, soil, or other sources.\nPCR Amplification: Polymerase chain reaction (PCR) is employed to selectively amplify the barcode region from the extracted DNA. Generally, a set of universal primers are used that will amplify in a wide range of taxonomic groups.\nSequencing: The PCR-amplified DNA fragments (Amplicons) are subjected to DNA sequencing, typically using Sanger sequencing. The resulting sequence data contain the barcode information.\nComparison to reference database: The obtained DNA barcode sequence is compared to a reference database containing sequences from known species. Bioinformatics tools and algorithms are used to search for matches or close matches in the database.\nSpecies Identification: Based on the comparison results, researchers can identify the species of the specimen. If the sequence closely matches a known barcode sequence in the database, the specimen can be confidently identified."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#metabarcoding",
    "href": "C_bioinformatics-eDNA.html#metabarcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Metabarcoding",
    "text": "Metabarcoding\nMetabarcoding of samples with mixed DNA templates allows us to characterize biological communities. Barcoding generally required there to be only one species present in the extracted DNA in order to get a clean sequence for comparison and taxonomic assignment. However, for many applications it would be useful to amplify and sequence DNA that potentially contains DNA from multiple species. Advances in sequencing technology (high throughput sequencing and next generation sequencing) has allowed us to perform metabarcoding studies for which the same steps apply except that during sequencing a large number of reads are produced. These sequences then need to be analyzed to identify the unique sequences present in the data set and then those are matched to a database."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#edna",
    "href": "C_bioinformatics-eDNA.html#edna",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "eDNA",
    "text": "eDNA\nEnvironmental DNA is DNA captured from an environmental sample without the need for pre-isolating specific targets. Macroorganisms shed DNA as cellular or extracellular material into the environment. Typical sources include mucous, the excretion of bodily fluids (feces, urine), and the sloughing off of skin cells, scales or other tissue. This means that we can capture DNA from an environmental sample without pre-isolating specific target organisms. For example, we take a water sample (1 – 5l) and then use a nitrocellulose filter to trap the DNA and then extract that DNA using very straightforward protocols.\n\n\n\n\n\n\n Consider this\n\n\n\nEnvironmental samples cover a wide range of ecosystems and habitats and spatiotemporal scales. Use examples to describe this variety of samples that can be used.\n\n\nGenerally, we refer to environmental DNA as DNA that is a “trace” of an organism in the environment, not the organisms itself. However, in some cases the same methods used to characterize biological communities using environmental DNA are applied to community DNA.\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast eDNA and community DNA and argue whether you would consider a gut content analysis to be eDNA or community DNA.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nDNA can be isolated from bulk-extracted from mixtures of organisms isolated from an environmental sample (Community DNA).\n\n\n\neDNA has high potential as a complementary method to characterize and monitor biological communities but users must be aware of biases and caveats to analyze the resulting data sets in a meaningful way.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the potential of eDNA in terms of applications and contrast this with some of the challenges that still need to be overcome."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#our-data-set",
    "href": "C_bioinformatics-eDNA.html#our-data-set",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Our Data set",
    "text": "Our Data set\nThe data set that we will be exploring used metabarcoding to characterize fungi communities in soil samples taken from different depths (soil horizons) in forests dominated by different trees and their mutualistic mycorrhizal fungi to explore how this affects resource competition with free-living saptrotrophs.\nHere is the abstract from (Carteron et al. 2021).\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7.\n\nIn temperate and boreal forests, competition for soil resources between free-living saprotrophs and ectomycorrhizal (EcM) fungi has been suggested to restrict saprotrophic fungal dominance to the most superficial organic soil horizons in forests dominated by EcM trees. By contrast, lower niche overlap with arbuscular mycorrhizal (AM) fungi could allow fungal saprotrophs to maintain this dominance into deeper soil horizons in AM-dominated forests. Here we used a natural gradient of adjacent forest patches that were dominated by either AM or EcM trees, or a mixture of both to determine how fungal communities characterized with highthroughput amplicon sequencing change across organic and mineral soil horizons. We found a general shift from saprotrophic to mycorrhizal fungal dominance with increasing soil depth in all forest mycorrhizal types, especially in organic horizons. Vertical changes in soil chemistry, including pH, organic matter, exchangeable cations, and extractable phosphorus, coincided with shifts in fungal community composition. Although fungal communities and soil chemistry differed among adjacent forest mycorrhizal types, variations were stronger within a given soil profile, pointing to the importance of considering horizons when characterizing soil fungal communities. Our results also suggest that in temperate forests, vertical shifts from saprotrophic to mycorrhizal fungi within organic and mineral horizons occur similarly in both ectomycorrhizal and arbuscular mycorrhizal forests."
  },
  {
    "objectID": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "href": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "title": "Data/Shark wrangling",
    "section": "Essential fish habitat: Shark Nurseries",
    "text": "Essential fish habitat: Shark Nurseries\nThe Magnuson-Stevens Act (1996) defined essential fish habitat as “those waters and substrate necessary to fish for spawning, breeding, feeding or growth to maturity”, i.e. they are habitats necessary for an organism to complete their life cycle. Identifying essential fish habitats is critical for management and conservation plans because it enables policy makers to prioritize certain ecosystems.\nWhile some elasmobranchs (sharks, rays, skates) inhabit estuaries year round, many use the estuaries for specific purposes such as feeding, mating, gestation, parturition or as nurseries and only inhabit them during specific life history stages. Estuaries are heavily impacted by humans - overfishing, pollution, habitat destruction and altered flow regimes all affect the biological communities they support.\nBroadly, shark nurseries are areas where young are born and/or reside in during maturation. Typically, these would areas that provide additional protection (e.g. mangroves for hiding) and plenty of food.\nShark Nurseries have three defining criteria(Heupel et al. 2018; Heupel, Carlson, and Simpfendorfer 2007):\n\nHeupel, Michelle R., Shiori Kanno, Ana P. B. Martins, Colin A. Simpfendorfer, Michelle R. Heupel, Shiori Kanno, Ana P. B. Martins, and Colin A. Simpfendorfer. 2018. “Advances in Understanding the Roles and Benefits of Nursery Areas for Elasmobranch Populations.” Marine and Freshwater Research 70 (7): 897–907. https://doi.org/10.1071/MF18081.\n\nHeupel, Michelle R., John K. Carlson, and Colin A. Simpfendorfer. 2007. “Shark Nursery Areas: Concepts, Definition, Characterization and Assumptions.” Marine Ecology Progress Series 337 (May): 287–97. https://doi.org/10.3354/meps337287.\n\nan area where sharks are more commonly encountered within compared to outside of.\nan area in which Young-of-the-year (YOY)/juveniles remain in or return to for extended periods of time.\nan area that is repeatedly used across years.\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe how you could design a study to identify estuaries that are shark nurseries."
  },
  {
    "objectID": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "href": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "title": "Data/Shark wrangling",
    "section": "Identifying shark nurseries on the Texas Coast",
    "text": "Identifying shark nurseries on the Texas Coast\nTexas Parks and Wildlife (TPWD) defines eight major estuaries along the Texas coastline and performs regular shore-based gill net surveys for 10 week periods in April - June and September to November.\n\n\n\nFigure 1: Map of major estuaries located along the Texas coast in the northwest Gulf of Mexico (Plumlee et al. 2018).\n\n\nAnalysis of this survey has identify eight elasmobranch species present in these ecosystems (Plumlee et al. 2018):\n\nPlumlee, Jeffrey D., Kaylan M. Dance, Philip Matich, John A. Mohan, Travis M. Richards, Thomas C. TinHan, Mark R. Fisher, and R. J. David Wells. 2018. “Community Structure of Elasmobranchs in Estuaries Along the Northwest Gulf of Mexico.” Estuarine, Coastal and Shelf Science 204 (May): 103–13. https://doi.org/10.1016/j.ecss.2018.02.023.\n\nBull shark\nBonnethead\nCownose ray\nBlacktip shark\nAtlantic stingray\nAtlantic sharpnose shark\nSpinner shark\nScalloped hammerhead\nFinetooth shark\nLemon shark\n\nGill nets generally exclude individuals &gt; 2m.\nMore recently, a multi-year open water long-lining study targeting elasmobranchs was performed in three estuarine locations near Corpus Christi, TX that are considered putative shark nurseries. Here, the sampling period lasted from May to November (Swift and Portnoy 2021).\n\nSwift, Dominic G., and David S. Portnoy. 2021. “Identification and Delineation of Essential Habitat for Elasmobranchs in Estuaries on the Texas Coast.” Estuaries and Coasts 44 (3): 788–800. https://doi.org/10.1007/s12237-020-00797-y.\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether or not you would expect to get similar results from both studies and what factors could result in differences.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere are some things to consider:\n\ngear-bias (hook size, net-size)\nseasonality (peak use for many coastal elasmobranchs is May - Oct)\nspatial (land-based vs open ocean)\n\n\n\n\nThis study wanted to answer four questions to further understand whether these locations should be considered shark nurseries and therefore as essential fish habitat for specific elasmobranch species.\n\nHow does the composition of elasmobranch communities compare across sites?\nHow does the catch-per-unit-effort (CPUE) per species and life history compare across sites?\nWhat do the sex ratios look like?\nWhat environmental predictors can we use to predict presence of elasmobranchs?\n\nIn this module we will interact with the data set generated for this study to learn how to wrangle data sets using R in the tidyverse and then we will apply those skills to answer these questions."
  },
  {
    "objectID": "14_impacts.html#a-grammar-of-graphics",
    "href": "14_impacts.html#a-grammar-of-graphics",
    "title": "14  Climate change: Impacts",
    "section": "14.1 A grammar of graphics",
    "text": "14.1 A grammar of graphics\nNow that we are confident(ish) in our data wrangling the next step is learning how we can visualize our data for exploration and for communication. We have already used figures using custom functions implemented in DADA2 that are based on the grammar of graphics/wrap functions from ggplot2 and we also familiarized ourselves with the syntax for using ggplot2 in the previous chapter.\nBeing able to visualize your data is an important skill throughout the process of data science. For example, in our next module we will dig into how to complete an exploratory analysis which is an important component of data science as it not only gives you an overview of what information is contained in a data set and can also help you refine the question you are asking. Finally, once you have refined and completed an analysis, visualizations are a key component to communicate your results.\nggplot2 is the core package of the tidyverse used for visualization. Similar to tidyr and dplyr having been built on the concept of a tidy data set, ggplot2 was built on a framework which follows a layered approach to describe and/or build any type of visualization in a structured way termed the “grammar of graphics”.\nThe grammar of graphics breaks down the components of any visualization into seven components.\n\nData is the component of data set to visualize1.\nMapping or Aesthetics include variables to be plotted, plot positions (e.g. x or y axis), and encodings such as size, shape, color2.\nGeometric objects (or layers) are what you actually end up seeing in the visualization, e.g. points, lines, bars etc. to represent data. This can also include statistical transformations3.\nScales map the values in the aesthetic space you specified, this includes your scales for color, shape, and size, you can also explicitly specify scales for the x and y axis.\nCoordinate system, includes all the usual suspects such as cartesian or polar along with some you may have never really heard of.\nFacets, i.e. subplots based on multiple dimensions.\nStatistics, including confidence intervals, means, quantiles, may additionally be added to the plot.\n\n1 You will see that this is either explicitly specified as the data argument (ggplot(data = df)), or you can just specify the data.frame or tibble to plot without typing in data = as ggplot(df).2 These components are specified as the mapping argument using aes(). Again you can explicitly call the argument as mapping =, i.e. ggplot(data = df, mapping = aes()), or you can just specify the mapping aesthetics using aes() because you are using the arguments in the correct sequence (ggplot(df, aes())) in which case R assumes they apply to these various arguments.3 Functions for these geometric objects will all start with geom_* or stat_*Additionally, you can create and use themes that control additional aspects of your your figure is displayed including font size, background color, etc.4.4 The package ggthemes includes various themes but you will also find that you can create your custom themes in a pretty straightforward way and that many R users have creted themes that they have made accessible for other users.\nUntil now you’ve probably thought of each plot type as it’s own distinctive format (a scatter plot, a pie chart, etc) and have not considered that each plot can be broken down into these fundamental components, so at first this way of thinking of plots will likely not seem intuitive.\nBut if you commit to thinking about plots in this abstract way you will quickly learn how these components fit together and realize the flexibility that this framework will give you to quickly generate exploratory plots, optimize your visualizations and be able to generate pretty much any plot you can think up5.5 You will also see that keeping your data tidy will allow you to customize your plots using different aesthetics and mappings to group your data."
  },
  {
    "objectID": "14_impacts.html#building-plots",
    "href": "14_impacts.html#building-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.2 Building plots",
    "text": "14.2 Building plots\nY’all ready for this?\nThen, let’s go back to our global mean temperature anomaly data set to figure out how the grammar of graphics can be applied and plot some data!\n\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\", skip = 1) %&gt;%\n  replace(. == \"***\", NA) %&gt;%\n  mutate_if(is.character, as.numeric)\n\nRecall how we created a line plot:\n\nggplot(temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nWe can generalize this into a basic template that can be used for different kinds of plots, including scatter plots, bar plots, and box plots.\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;\n\nEssentially, the minimum components needed to make a plot are\n\nA data set (&lt;DATA&gt;), this will be a data.frame or a tibble6.\nBased on the data, we need to define what variables we are going to plot (&lt;MAPPINGS&gt;) and how we want them to be represented, i.e. what individual data points should look like and how they are encoded.\nWe need to define the geometries, i.e. what type of plot (&lt;GEOM_FUNCTION).\n\n6 Remember, when we use read_delim() our data is automatically a object both of the format tibble and data.frame; while a tibble has some additional properties but we can use them interchangeablyLet’s break down what is happening to get a better understanding of how these minimum components fit together to create individual plots.\nFirst, we use the ggplot() function to define the specific data.frame to use to build the plot using the data argument (&lt;DATA&gt;)7.7 Remember, that when we are using arguments in the specified sequence that they are defined you do not explicitly need to call them.\nIn our example that would be\n\nggplot(data = temperature)\n\n\n\n\nFigure 14.1: Empty canvas generated using ggplot() function.\n\n\n\n\nNot much happens when you execute that piece of code other than creating a blank canvas in your plot panel8.8 Remember, if you are using and Rmarkdown or quarto document and have Chunk Output Inline enabled it will print beneath your code chunk\nThat is because we still need to define an aesthetic mapping using the aes() function (&lt;MAPPING&gt;).\nHere, we are selecting the variables that we want to plot (columns in our data.frame). At minimum we need to specify what columns we want to plot on the x and y axis, but we can also define variables (columns) we want to use to encode using color, shapes, size etc.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`))\n\n\n\n\nFigure 14.2: Coordinate system plotted after specifying mapping aesthetics, specifically which variables to plot on each axis.\n\n\n\n\nWhen we execute that code you will now get an empty coordinate system. This still seems like slim pickings but patience, young grasshopper, we have not yet defined the geom, i.e. what type of plot (“geometries”, &lt;GEOM_FUNCTION&gt;) we want to use. For example, here we want generate a line plot (geom_line()).\n\n\nFor ggplot we add layers using the + operator, which is a pipe (similar to %&gt;%) which tells R “and now add this”.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nFigure 14.3: Line plot (geom_line()) showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNow, we’re playing!\nOne of the advantages of the layered framework of ggplot2 is that we can plot multiple layers in the same plot by adding additional geoms.\nFor example, we can plot a scatterplot and line plot in the same plot as such:\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nFigure 14.4: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point().\n\n\n\n\nNow that you know how to generate a simple plot, let’s think about how we can further modify it to improve your visualization and how well you can communicate your results.\nFor example, we can change the color, fill, size, and shape for each geom layer.\n\n\nIn this example we are using the arguments for individual geom functions to change how data points are represented using colors and shapes for each layer, we can also use aes() to set mapping aesthetics for the entire plots.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3)\n\n\n\n\nFigure 14.5: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified.\n\n\n\n\nShapes for points can be specified using numbers; note how some shapes are “solid”, i.e. specifying color will define the color of the shape. Others, like the one used in our example are filled, i.e. color will determine the color of the outline, and fill the color of the space inside. For “hollow” shapes (or e.g. X’s) color will determine the line color.\n\n\n\nNumeric codes for sympols.\n\n\nYou can specify colors either using the color names defined by R or using hex codes.\n\n\nSome exploring will also lead you to various color pallets, many of them put together by R users around the world. Wes Anderson fan? You can style your plots accordingly… dig the aesthetics of the old school National Park posters and images? R community got you covered..\nPreviously, we also used geom_smooth() to add a layer with a linear regression. If you take a look at the arguments for this function using ?geom_smooth you will see that this has additional arguments apart from the mapping aesthetics like choosing the type of regression, whether or not the confidence interval is shown etc. Let’s say we want to add a red, dashed regression line without the confidence interval.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3) +\n  geom_smooth(stat = \"smooth\", se = FALSE, color = \"red\", linetype = \"dashed\", size = 2)\n\n\n\n\nFigure 14.6: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified. Trend line fitted using geom_smooth().\n\n\n\n\nAdmittedly, this plot is now starting to look a little bit ridiculous and is probably make it less, not more easy for the viewer to undestand what they can learn from the data. Maybe we should think of alternative options for visualizing this data to explore some additional ggplot options.\nAnother way to visualize this data would be using a bar plot.\n\n\nCan we appreciate for a second how easy it was to change how the same data is plotted? If you were using excel you would have had to insert a new plot, define what data you wanted to plot, relabel the axis, etc. Here all you had to do was change a single line of code.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.7: Bar plot showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNotice, how this is still the exact same mapping aesthetics (time/year on the x-axis and the temperature anomaly on the y-axis) but it changes the way we see the data completely - the emphasis now is on the change of temperatures below or above the long-term mean and the clear pattern of a shift of those temperature differences going from negative to being positive.\nLet’s say we want really lean into demonstrating this drastic change in temperatures not only being above the long-term average but also steadily increasing. One way to do this is to use color. We could modify this bar plot to have all of our bars representing years with global mean temperatures below the 1951-1980 average in one color and those above in another.\nTo do this, we need a column that encodes that information… that of course is not an issue for us as pretty much professional data wranglers! We can use a simple conditional mutate using an ifelse()9 statement to add a column (year_type) that indicates whether temperatures are above or below the the long-term average.9 recall, that we previously used case_when() for a conditional mutate, when the condtion is binary (true/false or this/that) using ifelse() makes for a much shoerte syntax.\nWe can manipulate our data.frame using the dplyr functions we are already familiar with. We can even use the %&gt;% pipe to pass the data argument directly to the ggplot() function1010 When you do this, remember that you need to switch back to using the + operator once you are adding your geom layers.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.8: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set.\n\n\n\n\nAnything that we add to the ggplot() layer as a mapping is a universal plot setting that will apply to all subsequent layers. You can override these global settings is you specify aesthetics like color, shape or size specifically in an separate layer. For example if we do the following, we will override our color coding even though we specified it in our ggplot() layer.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"blue\")\n\n\n\n\nFigure 14.9: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set. Settings in geom_bar() override the general plot specifications.\n\n\n\n\nLet’s see what else we can visualize using ggplot. Recall from when you explored our temperature anomaly data set, that this data set also contains information for seasons and individual months.\nLet’s take a look at the distribution of mean global winter temperatures (DJF = December, January, February) using a histogram, (geom_histogram()). Note that we only need to define our x-axis to plot a histogram, this is because ggplot2 will count how many observations fall into each bin for you … this geom is a mixture of a geometry (bar plot) and a statistical transformation (binning data points into ranges and the plotting those as bar plots).].\n\nggplot(temperature, aes(x = DJF)) +\n  geom_histogram()\n\n\n\n\nFigure 14.10: Histogram summarizing the distribution of mean global winter temperatures.\n\n\n\n\nWe can refine our plot by choosing our own bin width and by manipulating fill and color.\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\")\n\n\n\n\nFigure 14.11: Histogram displaying distribution of mean global winter temperatures with customized bin width and coloration.\n\n\n\n\nFor our example we might also want to add a vertical line (geom_vline()) to indicate 0 (i.e. the long-term global temperature mean calculated for 1951 - 1980).\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\") +\n  geom_vline(xintercept = 0, color = \"darkred\", linetype = \"dashed\", size = 1)\n\n\n\n\nFigure 14.12: Histogram displaying distribution of mean global winter temperatures with customized binwidth and coloration and added vertical line (red).\n\n\n\n\nIf we want to compare multiple distributions, box plots can be more helpful than histograms. Let’s say we wanted to compare the distribution of mean global temperature for each season.\nHow can we go about this?\nCurrently, our seasons are in individual columns, so our first step would be to create a tidy data set.\n\ntidy_season &lt;- temperature %&gt;%\n  select(Year, DJF, MAM, JJA, SON) %&gt;%\n  pivot_longer(names_to = \"season\", values_to = \"temperature\", 2:5)\n\nNow we can plot our seasons on the x-axis and the distribution of temperatures on the y-axis using geom_boxplot().\n\nggplot(tidy_season, aes(x = season, y = temperature)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.13: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons.\n\n\n\n\nAgain, you can add additional information using the mapping aesthetics to e.g. color code the boxes by season.\n\nggplot(tidy_season, aes(x = season, y = temperature, fill = season)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.14: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons color-coded by season."
  },
  {
    "objectID": "14_impacts.html#faceting-plots",
    "href": "14_impacts.html#faceting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.3 Faceting plots",
    "text": "14.3 Faceting plots\nOne of the advantages of ggplot2 being based not only on the grammar of graphics but also around the concept of a tidy data set is being able to create faceted plots. A faceted plot involves splitting a single plot into a matrix of panels, where each panel shows a different subset of the data. This is especially helpful during exploratory analysis where you might first plot all your data points in a single graph but then want to look at whether or not individual subsets within the data set behave the same.\nLet’s look at an example to better understand what faceting plots looks like. For example, let’s say we wanted to create individual plots of our bar plots showing our deviations of global temperatures from the 1951 - 1980 mean for each season.\nHow could you create individual plots with the methods you are already familiar with?\nHere is how you can do it using facet_grid().\nNow we can plot our data and using facet_grid() we can specify that we want individual panels by month in separate rows.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season))\n\n\n\n\nFigure 14.15: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nYou could also specify that you want the individual plots to be separated into columns.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(cols = vars(season))\n\n\n\n\nFigure 14.16: Change in temperature anomaly relative to long-term avarage per season (columns)\n\n\n\n\nYou can also create faceted plot where subset your data by two variables and so you end up with one variable defining the rows and one the columns and you can use a simpler syntax row-variable ~ column-variable.\nYou can also use this syntax if you are only faceting by one variable as we are doing in this example by specifying the variable and leaving the other one “blank” using a ..\nFor example, to plot this faceted data set in rows, you would use the following syntax -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(season ~ .)\n\n\n\n\nFigure 14.17: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nAnd to plot this data set column-wise you would specify it like so -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(. ~ season)\n\n\n\n\nFigure 14.18: Change in temperature anomaly relative to long-term avarage per season (columns)"
  },
  {
    "objectID": "14_impacts.html#customizing-plots",
    "href": "14_impacts.html#customizing-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.4 Customizing plots",
    "text": "14.4 Customizing plots\nWe have already played around a little bit with the options that we have in terms of customizing plots using color, fill, shapes, and sizes. But we’ve barely scratched the surface.\nYou have probably noticed that there are some default settings like the gray background and white grid lines, font sizes of label axis, what the axis labels are, and even color schemes that are automatically used. Even using the defaults we get pretty clean plots that are visually appealing. This is super helpful during exploratory analysis because even though you playing around with the data you still have nicely formatted and easy to interpret figures.\nOnce you have identified the central plots that you want to use to communicate the results and conclusions of your data analysis you will want to further customize your visualization to optimize communication, this includes how you encode data using color and shape but also making sure that everything is well labeled and clear to the person who is reading your report or listening to your presentation.\nOne of the first things we frequently want to changes is the axis labels. ggplot2 does handily use the column names to automatically label your axes, so you will always have a label which is great during exploratory analysis but generally you will want to customize that for your final figure. The function labs() can be used to specify a title, subtitle, axis labels and additional annotations (caption) below the figure.\nFor example we could customize our faceted figure to look like this:\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\")\n\n\n\n\nFigure 14.19: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nPretty much every component of a ggplot figure can be further customized using theme(), this includes things like font size, background and line colors, grids, legend position … ggplot2 odes have some pre-defined themes that you can call up that will change the layout.\nThe default theme is theme_grey().\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_grey()\n\n\n\n\nFigure 14.20: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nHere, we explicitly specified it using theme_grey(); if you do not specify a theme this is the theme that will be used for your plot. There are other themes that are part of the ggplot package that include theme_bw(), theme_minimal(), theme_classic() or theme_light().\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRe-plot the same figure using the four themes specified above.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nYou will likely find a theme that you like but still want or need to make additional tweaks. In that case the function theme() is your friend.\nLet’s go back to our bar plot of the global temperature anomaly to look at an example of likely the three most common things you will want to adjust which is the legend position, changing font size, color, turning x-axis labels by 90 degrees.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\n\n\n\nFigure 14.21: Change in mean global temperature relative to long-term avaerage with customized font sizes and legend positions.\n\n\n\n\nThat’s starting to look pretty slick.\nAs you start your visualization adventure the R Cookbook is a good resource. It is written in a helpful style that starts with a concrete plotting problem and then walks you through a solution."
  },
  {
    "objectID": "14_impacts.html#arranging-plots",
    "href": "14_impacts.html#arranging-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.5 Arranging plots",
    "text": "14.5 Arranging plots\nWe’ve already seen that we can create multi-panel plots in a very straightforward way when we are plotting the same plot for different subsets of the data (facetted plots).\nBut what if we want to generate individual plots that might fit together thematically but aren’t subsets that we can facet but we still want to be able to present them together? Fear not, this too can be solved; one option is using patchwork, a package designed for exactly this purpose. Let’s generate a few additional plots so we can try this out.\nIn our data folder there is a tab-delimited file with monthly mean CO2 concentrations (parts per million) from Maunua Loa. Let’s read in that data set and create line plot with a regression line.\n\ncarbon &lt;- read_delim(\"data/CO2_monthly.txt\", delim = \"\\t\")\n\nggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\n\n\n\nFigure 14.22: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory from 1958 - 2021.\n\n\n\n\nWe also have a data set from the Global Carbon Project downloaded from the Our World in Data repository that contains atmospheric CO2 emissions. We can plot that as a simple line plot with a regression.\n\nemissions &lt;- read_delim(\"data/emissions.txt\", delim = \"\\t\") %&gt;%\n  filter(iso_code == \"OWID_WRL\")\n\nggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"CO2 Emissions over time.\",\n       subtitle = \"Global emissions atmospheric emissions by year.\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\n\n\n\nFigure 14.23: Global atmospheric CO2 emissions.\n\n\n\n\nTo be able to plot multiple plots in one using patchwork we need to assign our figures as objects.\n\np1 &lt;- ggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\np2 &lt;- ggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2 Emissions\",\n       subtitle = \"global emissions\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\nNow we can combine them side by side using a simple syntax.\n\np1 + p2\n\n\n\n\nFigure 14.24: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (left panel) and Global atmospheric CO2 emissions (right panel).\n\n\n\n\nSimilarly, we can plot them underneath each other like so -\n\np1 / p2\n\n\n\n\nFigure 14.25: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (top panel) and Global atmospheric CO2 emissions (bottom panel).\n\n\n\n\nOh, it gets better. Let’s say we wanted to plot our global mean temperatures in the top row and our two emissions plots below.\n\np3 &lt;- temperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\np3 / (p1 | p2)\n\n\n\n\nFigure 14.26: Comparison of change in mean global temperature (top panel), atmospheric CO2 concentrations (bottom left), and atmospheric CO2 emissions (bottom right).\n\n\n\n\nYou can create complex compositions in patchwork using syntax combining +, |, and \\. To see how you can control the layout even further you can check out the documentation for patchwork."
  },
  {
    "objectID": "14_impacts.html#exporting-plots",
    "href": "14_impacts.html#exporting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.6 Exporting Plots",
    "text": "14.6 Exporting Plots\nThe last thing we still have to figure out is how to save plots to file. You have a few options.\nThe quickest and dirtiest option is to simply right click on the plot pane after plotting a figure and then save the figure using Save image as. This works and is easy to do but gives you very little control over the dimensions, resolution, file format etc.\nFor more control over the format of your figure you can use the Export tab in the Plot pane which will allow you to adjust the dimensions and the the file format.\nFinally, the ggplot2 has a function called ggsave() that will allow you to determine the dimensions (width, height), resolution (dpi), and format (device).\nBy default it will save the last plot that was plotted. If you specify the format in the file name (e.g. *.svg, *.jpg, *.png) it will automatically recognize the format11.11 Remember that we have designed our research compendium to keep raw and processed data, and results separate? Figures are considered results so you should always save them to the results folder.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\nggsave(\"results/global_temp.png\", dpi = 300, width = 15, height = 10)\n\n\n\n\nFigure 14.27: Change in mean global temperature realtive to long-term average.\n\n\n\n\nCheck your results folder to see if you were successful!\nIf you assign your plot to an object you can use the plot argument of the ggsave() function to export it. This also works for figures that consist of multiple panels combined using patchwork."
  },
  {
    "objectID": "14_impacts.html#more-plots",
    "href": "14_impacts.html#more-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.7 More Plots!",
    "text": "14.7 More Plots!\nReady to take the training wheels off?\nIncreasing global temperatures are effecting change across the different components of the climate system. There are several data sets in your data folder. Read in each data set as a data frame, use View() to take a look at what information is contained in the data set and the use your new found visualization skills to plot the data using ggplot.\nWe have looked at rising mean global temperatures and discovered that rapidly increasing emissions and atmospheric concentrations of CO2 are consistent with increased concentrations of greenhouse gases driving temperature change.\nThe effects of global warming are observed throughout components of the climate system including the atmosphere, the hydrosphere (marine and freshwater systems), cryosphere (land and sea ice), and lithosphere (earth’s surface/crust). Let’s look at a few data sets that illustrate the impact of rising temperatures.\nFor all the plots, comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIncreasing atmospheric temperatures result in land ice melting and the ocean heat content rising. The latter leads to thermal expansion which together with the increase freshwater inflow results in rising sea levels.\nThe first data set is from the NOAA Laboratory for Satellite Altimetry and contains records from coastal sea level tide gauges. Start by reading in the data set:\n\nsealevel &lt;- read_delim(\"data/sealevel.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set12\nPlot the change in mean sea level over time as a line plot and color code the line(s) by method (see column names). Chose a theme and position your legend below the figure.\nFigure out how to use geom_hline() to add a line at 0.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n12 Include things like the number of columns, what data is contained in each sample, what time periods are included etc. Practice deducing these types of things from the context of the data set description above and by browsing the content.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s an example of what tour final figure could look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOur next data set is from the National Snow and Ice Data Center and contains the Arctic (Northern Hemisphere) July sea ice extent.\n\nice_arctic &lt;- read_delim(\"data/arctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a theme. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s take a look at the equivalent data set depicting sea ice extent in the Antarctic (Southern Hemisphere.)\n\nice_arctic &lt;- read_delim(\"data/antarctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a new theme from what you’ve been using previously. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOne more data set. This one is from the NOAA Hurricane Research Division. While interpreting the observed patterns in sea level and ice extent and modeling future projections is pretty straightforward, understanding whether extreme events are getting “worse” is a little bit more complicated13.\nNote: For this coding challenge you will need to combine your data wrangling and visualization skills!\n\nhurricane &lt;- read_delim(\"data/hurricanes.txt\", delim = \"\\t\")\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific14.\nPlot 1:\nWhen thinking about hurricanes there are two metrics to consider when determining whether or not “hurricanes have gotten worse”. First, we can look at whether or not hurricanes have become more common by plotting the number of hurricanes for each year.\n\nCreate a faceted plot with individual line plots showing the number of named storms (this would include not only hurricanes put also tropical storms), the number of hurricanes, the number of major hurricanes, and the number of hurricanes in the United States.\nAdd a linear regression.\nChose a custom theme.\n\n\n\nExtra Challenge: We’ve previously used facet_grid() to create faceted plots which requires us to specify how panels should be laid out in rows and/or columns. Figure out how to use facet_wrap() to just specify which variable to use to create individual panels with out them all needing to be laid out next to each other in columns or underneath each other in rows.\n\nPlot 2:\nThe other way to evaluate whether or not “hurricanes have gotten worse” is to not look at the number of storms but to determine whether individual storms or storms as a whole have become more intense and/or destructive. One of the columns you may not have been able to fully figure out what information it contains is the column containing information on the “accumulated cyclone energy”15.\n\nCreate a bar plot showing the ACE for each year.\nChose a theme and customize the color of your bars16 3. Extra Challenge: Use geom_hline() to add a horizontal line indicating the mean ACE for the recorded time period.\nWrite a short description for each figure summarizing the key results displayed and argue whether or not you think “hurricanes have gotten worse”.\n\n16 You can specify color and fill for bar plots. Try setting the color and fill to the same and to different colors to see what happens when you have a bar plot with this many individual bars.\n15 Now that you have this information gp ahead and update your notes to add it to your short data set description.14 Note, several of the columns start with ‘Revised’ this just has to do with the entire data set having revised (most recent) definitions applied for the metrics included. You can ignore that component.13 We will take a closer look at this aspect when we combine our data wrangling and visualization skills to learn how to perform an exploratory analysis\nYou will have to reformat the data set in order to plot it in this fashion. It can be helpful to first think about what your ggplot code should look like to create the plot described above and then format your data set to fit those needs. Recall that ggplot is designed not only around the grammar of graphics but also the principles of tidy data… so, do we currently have a tidy data set? Or what would it need to look like for this plotting challenge?.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot 1:\nPlot 2:\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChoose five of the plots we have generated in this chapter and combine them into a single plot using patchwork; include at least two columns and two rows in your layout17.\nSave your plot in a way that retains proper proportions to make it easy for somebody else to easily see the details and discern the general patterns. Share your multi-panel plot in the #assignment channel of our slack workspace.\nUse your the figure you have created to support argue that “the observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures”. Draw from the descriptions you made for the figures throughout this challenges to summarize what you observe to support this claim. In your answer you will want to consider which of the data sets refer to which component of the earth-climate system and why the data sets you have looked at are consistent with increasing mean global temperatures.\nExtra challenge: You can use #| fig-height and #| fig-width; code chunk options to specify the dimensions of the output of a figure in the knitted document18. When you initially render your quarto document you will find that the default dimensions will not format your you multi-panel plot in a way that does not make it super legible. Manipulate these parameters and re-knit your html to display your figure in an appropriate way.\n\n\n18 Yes, this does mean that you will need to check the format of your html output and potentially change it before you submit. Checking your html report before you submit is a good habit to have just to make sure that everything is rendering as expected17 This can include your temperature anomaly data set as well.\n\n\n\n\n\n Did it!\n\n\n\nFigure\nThe observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nConsider the following and write a short paragraph (7-10 sentences):\n\nThink back over the analyses we have performed for this and the previous chapter and argue whether the analysis you have performed should be considered descriptive, exploratory, inferential/predictive, or causal/mechanistic.\nDescribe both what you actually did, and then also briefly sketch out what your experimental design/analysis would need to look like if you had performed analysis in the other categories.\nCausal/mechanistic analyses are the most involved frequently also the most difficult to perform. Argue whether or not your results are less meaningful or informative if they are not mechanistic/causal or whether they are still useful to support the initial thesis that climate change is unprecedented, unequivocally anthropogenic, and the impacts are already impacting all components of the earth-climate system.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "12_community-diversity.html#compile-data-sets",
    "href": "12_community-diversity.html#compile-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.1 Compile data sets",
    "text": "12.1 Compile data sets\nWe are going to import the data you will need for this chapter exploring how to characterize biological communities. We’ll start by loading to objects into your environment. One is the ASV table and the second is the corresponding taxonomy table from the fungi data set we were working on in the previous chapter.\nWe are going to read in a data set that contains information about the soil plots from which the fungi eDNA was isolated.\n\n# load sample data\nsoil &lt;- read_delim(\"data/soil.csv\", delim = \";\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to take a look at the soil data set and the describe what information it contains (typical things you want to check are row, column numbers, column names, determining if it is a tidy data set, figuring out what information/variables are in the data set).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThe package phyloseq that we installed has various utility functions to deal with metabarcoding data set. It has a specific object class that allows us to store the asv table, taxonomy table, and sample meta data in different slots. Various functions can then be used to pull information from those slots. We can load the object ps directly into your environment using the following code.\n\n# create phyloseq object\nload(file = \"data/ps.rdata\")"
  },
  {
    "objectID": "12_community-diversity.html#data-filtering-and-transformation",
    "href": "12_community-diversity.html#data-filtering-and-transformation",
    "title": "12  Characterizing community diversity",
    "section": "12.2 Data filtering and transformation",
    "text": "12.2 Data filtering and transformation\nBefore we can start exploring our data set we have a few steps to complete to transform it into containing the data we want in the format we want tit.\nLet’s start by taking a looking at how many taxa are currently present in the data set using phyloseq::ntaxa()\n\nntaxa(ps)\n\n[1] 546\n\n\nWe only want ASVs that were assigned as fungi in our reference database. We can use phyloseq::subset_taxa to filter by kingdom.\n\nps_fungi &lt;- subset_taxa(ps, Kingdom == \"k__Fungi\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many non-fungi groups were filtered.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNext, we want to remove any singletons or doubletons. These are ASVs that are only represented by one or two reads - we can assume these are artifacts.\n\nps_fungi_nosd &lt;- filter_taxa(ps_fungi, function(x) sum(x) &gt; 2, TRUE)\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many taxonomic groups are still in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nIn many situations, metabarcoding eDNA samples is semi-quantitative2. The number of DNA templates in the environment are correlated to the number of individuals/biomass of a given taxonomic group. While you cannot necessarily back calculate the absolute abundance or number of specimen present in an environment you can use the proportion of reads assigned to a taxonomic group as a metric of relative abundance.2 We can always confidentally use metbarcoding data as qualitative data, i.e. as presence/absence data. Though even here we should be careful about whether “not detected” should be interpreted as absent.\nOne way to convert species abundance from absolute to relative is using a Hellinger transformation which standardizes the abundances to the sample totals and then square roots them. The function phyloseq::transform_sample_counts() allows us to apply a function to transform sample counts.\n\nps_fungi_nosd_hel &lt;- transform_sample_counts(ps_fungi_nosd, function(x) sqrt(x/sum(x)))\n\n\n\n\n\n\n\n Consider this\n\n\n\nOur data set could still contain multiple ASVs that have been assigned to the same species. Explain why this is an expected out come when using ASVs but would be rare/non-existant if you are using OTUs as the output of your bioinformatics pipeline.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nTo complete our data filtering and transformation we will use phyloseq::tax_glom() to collapse ASVs asigned to the same taxonomic group at the species level.\n\nps_transf = tax_glom(ps_fungi_nosd_hel, \"Species\", NArm = FALSE)\n\nLet’s explore our final transformed data set\n\n\n\n\n\n\n Give it a whirl\n\n\n\nApply the functions ntaxa(), nsamples(), rank_names(), and sample_variable() to our final transformed phyloseq object. Look up what each function does, make sure to comment/annotate your code and then briefly describe what you’ve learned about our data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nRecall that our phyloseq object contains a slot that holds our ASVs table and our taxonomic table that we can access at as such.\n\notu_table(ps_transf)[1:2, 1:2]\n\nOTU Table:          [2 taxa and 2 samples]\n                     taxa are columns\n    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                                            0.00000000\nS11                                                                                                                                                                                                                                                                                                                                                            0.08178608\n    ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                               0.0000000\nS11                                                                                                                                                                                                                                                                                                                                               0.2085144\n\ntax_table(ps_transf)[1:2, 1:2]\n\nTaxonomy Table:     [2 taxa by 2 taxonomic ranks]:\n                                                                                                                                                                                                                                                                                                                                                                      Kingdom   \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"k__Fungi\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"k__Fungi\"\n                                                                                                                                                                                                                                                                                                                                                                      Phylum            \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"p__Basidiomycota\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"p__Mucoromycota\" \n\n\n\n\n\n\n\n\n Consider this\n\n\n\nExplain how the notation [1:2, 1:2] modifies the output\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "12_community-diversity.html#combine-data-sets",
    "href": "12_community-diversity.html#combine-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.3 Combine data sets",
    "text": "12.3 Combine data sets\nWe have three explanatory variables that could be driving differences in fungal communities among samples.\n\nSoil samples were taken in differnt forest plots that were classified as dominated by Acer saccharum (AS) or Fagus grandifolia (FG) or mixed with other small trees and shrubs present (mixed).\nSoil samples where taken from different soil horizons (depths): L, F, H, Ae, or B\nSoil chemistry (carbon, nitrogen, pH)\n\nThis information is stored in our soil dataframe.\nNow that we’ve filtered and transformed our data set, let’s pull it back out to create a dataframe as the object you are more familiar with in terms of being able to manipulate it.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s pull out our taxonomic table and transform it into a dataframe. Comment the following code line by line to describe what each function/arguments is doing.\n\nasv_tax &lt;- tax_table(ps_transf) %&gt;%  #\n  as.data.frame() %&gt;%            #\n  rownames_to_column(\"asv\")      #\n\n# write out interim file\nwrite_delim(asv_tax, \"results/asv_tax.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s pull out our information on how many times each ASV is observed in a each sample. Comment the following code line by line to describe what each function/arguments is doing. You may need to look up some of the functions.\n\nasv_counts &lt;- otu_table(ps_transf) %&gt;%  #\n  t() %&gt;%                               #\n  as.data.frame() %&gt;%                   #\n  rownames_to_column(\"asv\")             #\n\n# write out interim file\nwrite_delim(asv_counts, \"results/asv_counts.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a look at how your asv_tax and asv_count objects are now formatted and briefly describe it (remember, key things are number or rows, columns, what those columns are).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nReally, what we want is for our asv_counts table to contain the taxonomic information contained in the asv_tax table. Combine those two data sets into an object called tax_count. Remove the ASV sequence from the dataframe and arrange the remaining columns to first have all the taxonomic information, then the number of occurrences in each sample. Print the first few lines to the console when you are done.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like\n\n\n   Kingdom           Phylum                 Class             Order\n1 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n2 k__Fungi  p__Mucoromycota c__Umbelopsidomycetes o__Umbelopsidales\n3 k__Fungi    p__Ascomycota    c__Dothideomycetes  o__Mytilinidales\n4 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n5 k__Fungi    p__Ascomycota    c__Sordariomycetes     o__Xylariales\n6 k__Fungi    p__Ascomycota                  &lt;NA&gt;              &lt;NA&gt;\n                Family           Genus       Species       S10        S11\n1  f__Tricholomataceae       g__Mycena          &lt;NA&gt; 0.0000000 0.08178608\n2   f__Umbelopsidaceae   g__Umbelopsis   s__dimorpha 0.0000000 0.20851441\n3        f__Gloniaceae   g__Cenococcum  s__geophilum 0.0000000 0.32414749\n4    f__Hygrophoraceae    g__Hygrocybe s__flavescens 0.2929066 0.40028795\n5 f__Amphisphaeriaceae g__Polyscytalum s__algarvense 0.0000000 0.00000000\n6                 &lt;NA&gt;            &lt;NA&gt;          &lt;NA&gt; 0.5073291 0.22398041\n        S12        S13       S14        S15       S16        S17       S18\n1 0.0000000 0.09578263 0.2379155 0.04867924 0.7712319 1.25129257 0.4761444\n2 0.2721655 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n3 0.5233063 0.13545709 0.0000000 0.23676137 0.2667853 0.00000000 0.0000000\n4 0.0000000 0.00000000 0.0000000 0.06884284 0.0000000 0.00000000 0.0000000\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.3165055\n6 0.1200137 0.19156526 0.0000000 0.13299415 0.2226355 0.07392213 0.5152174\n          S1       S20        S21        S22        S23       S24       S25 S26\n1 0.07372098 0.1543370 0.15971703 0.04598005 0.17325923 0.1726902 0.2505837   0\n2 0.24818179 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n3 0.36422877 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n4 0.24077171 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n5 0.00000000 0.6046415 0.06052275 0.34039602 0.00000000 0.4626177 0.3245927   0\n6 0.41216069 0.5922461 0.25094334 0.47371431 0.04331481 0.3527087 0.5436833   0\n        S27        S28        S29        S2       S30       S31        S33\n1 0.9263177 0.04145133 0.27471034 0.1142577 0.4917893 0.4379003 0.07124705\n2 0.0000000 0.00000000 0.00000000 0.1842351 0.0000000 0.0000000 0.00000000\n3 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.19991094\n4 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.00000000\n5 0.0000000 0.13747852 0.04729838 0.0000000 0.3446360 0.3636152 0.00000000\n6 0.0000000 0.33819215 0.17651995 0.7626946 0.1953884 0.8801878 0.12162632\n        S34        S35       S36        S37       S39         S3       S41\n1 0.0000000 0.04393748 0.0559017 0.09449112 0.1474420 0.06516352 0.1036952\n2 0.3815359 0.00000000 0.2091650 0.18898224 0.6158141 0.36572936 0.0000000\n3 0.0000000 0.77616588 0.1118034 0.33838581 0.2197935 0.00000000 0.0000000\n4 0.5209007 0.00000000 0.0000000 0.00000000 0.0695048 0.14497221 0.2375955\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n6 0.0000000 0.00000000 0.0000000 0.29827034 0.4928534 0.00000000 1.1342749\n         S42        S43       S44       S45        S46       S47       S48\n1 0.07808688 0.05123155 0.1952834 0.1954906 0.08441499 0.0000000 0.1773317\n2 0.19908326 0.00000000 0.0000000 0.0000000 0.09747404 0.0000000 0.0000000\n3 0.17460757 0.61471931 0.1301889 0.0000000 0.21037942 0.5733137 0.6634328\n4 0.68553927 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n5 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n6 0.33809195 0.33116596 0.0000000 0.1382327 0.10897929 0.3473466 0.0000000\n         S49         S4       S50       S51        S52       S53        S54\n1 0.00000000 0.08962214 0.1162476 0.1076244 0.09853293 0.1624354 0.08980265\n2 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.15554275\n3 0.78598070 0.50232050 0.4739717 0.0000000 0.30722902 0.0000000 0.77815937\n4 0.00000000 0.00000000 0.0000000 0.6499631 1.03997108 0.3456639 0.00000000\n5 0.00000000 0.00000000 0.0000000 0.2107167 0.00000000 0.0000000 0.00000000\n6 0.09724333 0.00000000 0.0000000 0.2818244 0.19706586 0.7353688 0.27679036\n         S55        S56       S58        S59         S5        S60        S61\n1 0.07722833 0.00000000 0.0000000 0.00000000 0.08436491 0.05852057 0.09072184\n2 0.00000000 0.35948681 0.2656845 0.00000000 0.00000000 0.00000000 0.06415003\n3 0.14788099 0.09607689 0.3510009 0.07800765 0.00000000 0.00000000 0.00000000\n4 0.16683226 0.13587324 0.0000000 0.00000000 0.00000000 1.19488153 0.57239218\n5 0.00000000 0.00000000 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.29652140 0.24485651 0.2425356 0.00000000 0.17896500 0.34818263 0.46770187\n        S62        S63        S64        S65        S66        S67        S68\n1 0.1528942 0.00000000 0.04252433 0.09911197 0.04756515 0.00000000 0.08119979\n2 0.0000000 0.31596888 0.17533229 0.36009167 0.30082842 0.06356417 0.09376145\n3 0.1441500 0.04045567 0.00000000 0.00000000 0.11651035 0.00000000 0.38923215\n4 0.0000000 0.12793206 0.22443986 0.00000000 0.33580828 0.07784989 0.25246042\n5 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.2573043 0.45294784 0.33911838 0.48952099 0.28824313 0.39950351 0.06629935\n         S69        S6        S72        S73       S74      S75       S76\n1 0.12149135 0.0412393 0.06765101 0.08753762 0.1849937 0.000000 0.1237179\n2 0.06074567 0.0000000 0.31001587 0.27681827 0.0000000 0.000000 0.0000000\n3 0.00000000 0.1797580 0.22941573 0.00000000 0.1807754 1.006459 0.6441024\n4 0.22146362 0.8184629 0.00000000 0.00000000 0.2061156 0.000000 0.0000000\n5 0.00000000 0.0000000 0.00000000 0.27681827 0.0000000 0.000000 0.0000000\n6 0.33253266 0.0000000 0.23310641 0.53134923 0.2188566 0.134840 0.0000000\n         S77        S78        S79         S7        S80       S81        S8\n1 0.31370944 0.11909827 0.24942152 0.04719292 0.16464639 0.0433555 0.0000000\n2 0.00000000 0.00000000 0.00000000 0.00000000 0.07761505 0.3386173 0.0000000\n3 0.19649437 0.56472318 0.08971226 0.23596459 0.52008893 0.0000000 0.7211103\n4 0.06213698 0.65223431 0.08971226 0.00000000 0.05488213 0.0000000 0.0000000\n5 0.19649437 0.00000000 0.00000000 0.00000000 0.05488213 0.0000000 0.0000000\n6 0.43896637 0.08421519 0.16353430 0.13348173 0.26499437 0.2410773 0.0000000\n          S9\n1 0.08596024\n2 0.00000000\n3 0.26193862\n4 0.00000000\n5 0.00000000\n6 0.24814583\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your resulting data set and determine if it is a tidy data set or not. Then describe how you would transform it into a tidy data set and explain why those changes make it fulfill all the conditions for it to be tidy.\nSpoiler alert: It’s not tidy … go ahead and create an object called tidy_counts that’s a tidy data frame.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (it’s probably clear why tidy data sets are also referred to as long data sets at this point …)\n\n\n# A tibble: 6 × 9\n  Kingdom  Phylum           Class        Order Family Genus Species ID     count\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0     \n2 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818\n3 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0     \n4 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958\n5 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238 \n6 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHm, if only our tidy_counts object also contained the sample meta-data currently in our dataframe soil. Go ahead and add that information to our tidy_counts data frame. Print the first few rows of your data frame to the console to make sure this worked.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (are we having fun yet?)… if you are having problems combining these data frames recall that you need one column in common. It’s easiest if those columns also share the same column name, but you can look up the function to see if there is a way around it if they don’t match up.\n\n\n# A tibble: 6 × 17\n  Kingdom  Phylum   Class Order Family Genus Species ID     count sampleID plot \n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n1 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0      M_FS_1/… M_FS…\n2 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818 M_FS_1/… M_FS…\n3 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0      M_FG_FS… M_FG…\n4 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958 M_FS_1/… M_FS…\n5 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238  M_AS_FS… M_AS…\n6 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487 M_FS_1/… M_FS…\n# ℹ 6 more variables: block &lt;dbl&gt;, forest &lt;chr&gt;, horizon &lt;chr&gt;, Carbon &lt;dbl&gt;,\n#   Nitrogen &lt;dbl&gt;, pH &lt;dbl&gt;"
  },
  {
    "objectID": "12_community-diversity.html#characterize-community-diversity",
    "href": "12_community-diversity.html#characterize-community-diversity",
    "title": "12  Characterizing community diversity",
    "section": "12.4 Characterize community diversity",
    "text": "12.4 Characterize community diversity\nOkay… now we’re ready to have some fun.\nFirst, let’s find out what taxonomic groups are present in the entire data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPrint a series of tables to the console and in your html document3 that comprise a single column each that show all the phyla, classes, orders, famiies, genera, and species in the data set in alphabetical order, respectively. Note that the values in those columns have a prefix indicate the taxonomic level. Get rid of that in your output.\n\n\n3 use the function kable() to print the entire data frame in a pretty table\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\n\n\n\n\nAscomycota\n\n\nBasidiomycota\n\n\nGlomeromycota\n\n\nMortierellomycota\n\n\nMucoromycota\n\n\nOlpidiomycota\n\n\nRozellomycota\n\n\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s compare patterns across different forest plot types. Create separate tables and print them to the console/have them print neatly in your rendered html files for easy comparison of the mean relative abundance for phylum, order, and family for each forest plot type. Print the first four digits only.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\nAS\nFG\nMixed\n\n\n\n\np__Ascomycota\n0.0248\n0.0236\n0.0254\n\n\np__Basidiomycota\n0.0143\n0.0139\n0.0132\n\n\np__Glomeromycota\n0.0121\n0.0000\n0.0000\n\n\np__Mortierellomycota\n0.0178\n0.0159\n0.0101\n\n\np__Mucoromycota\n0.0250\n0.0332\n0.0419\n\n\np__Olpidiomycota\n0.0000\n0.0000\n0.0044\n\n\np__Rozellomycota\n0.0041\n0.0113\n0.0042\n\n\nNA\n0.0381\n0.0257\n0.0276\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPick one forest plot type that you are interested in exploring further. Create tables that make it easy to compare the mean relative abundance for phylum, order, and family across the different soil horizons, and print those three table to the console and to your rendered html report.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nMeasures of diversity enable us to quantify the complexity of biological communities in a way that allows us to objectively compare communities across space and time. Measures of alpha diversity describe the diversity of a single sample and are based on the number of observed taxonomic groups or their relative abundance, beta diversity describes the variation between samples, gamma diversity describes the global diversity observed across a large number of communities.\nCommon alpha diversity statistics include:\n\nobserved richness the number of observed taxa.\nShannon Index (Shannon-Weaver or Shannon-Wiener Index) quantifies how difficult it is to predict the identity of a randomly sampled individual. Ranges from 0 (total certainty) to log(S) (total uncertainty).\nSimpson Index quantifies the probability that two randomly chosen individuals are the same taxonomic group (ranges from 0 to 1).\nInverse Simpson Index is defined as the number of species needed to have the same Simpson index value as the observed community assuming a theoretical community where all species are equally abundant.\n\nThe Shannon and Simpson Diversity are entropy-based indices that measure the disorder (diversity) of a system. In information theory entropy is used to describe the fact that we can quantify the degree of uncertainty associated with with predicted pieces of information. Applied to ecology this means that when describing diversity using these indices we are determining whether or not individuals randomly drawn from a community are the same or different species (or other taxonomic group).\nThe relationship between species richness and Shannon diversity is non-linear, i.e. at higher levels of species richness, communities appear more similar in terms of the magnitude of the index compared to lower levels of species richness - which is counter intuitive to the way species richness works. The solution to this is to linearize the indices. As a result, more recently, diversity indices have been proposed where diversity values are converted into equivalent (or effective numbers) of species (jost_entropy_2006?). The effective number of species is the number of species in a theoretical community where all species (taxonomic groups) are equally abundant that would produce the same observed value of diversity (a similar principle is applies in genetics for the concept of effective population sizes). While the definition of effective numbers is not as intuitive as the entropy-based ones the values that we yield are. Not only are the “units” species (instead of being a unitless index), but they have properties that we intuitively understand. For example, effective numbers obey the doubling principle: If you have two communities with equally abundant but totally distinct species and combine them, that new community would have a diversity that is 2x that of the original ones.\nThe package vegan has several functions implemented that allow us to calculate these diversity stats. Look up any functions you are not familiar with in the following code and comment it to describe what each line of code is doing.\n\ndiversity &lt;- tidy_counts %&gt;%\n  group_by(ID, forest, horizon) %&gt;%\n  summarize(richness = specnumber(count),\n            shannon = diversity(count, index = \"shannon\"),\n            simpson = diversity(count, index = \"simpson\"),\n            invsimpson = diversity(count, index = \"invsimpson\")) %&gt;%\n  pivot_longer(cols = c(richness, shannon, simpson, invsimpson),\n               names_to = \"metric\")\n\nWe can create a series of plots that compare the different diversity stats for each forest plot type and soil horizon location4.4 Well, I can but you will be able to soon, too\n\nggplot(diversity, aes(x = forest, y = value, color = forest)) +\n  geom_boxplot(aes(color = forest), outlier.shape = NA, fill = \"transparent\", size = 1) +\n  geom_point(aes(fill = forest),\n             position = position_jitterdodge(jitter.width = .3),\n             shape = 21, color = \"black\", size = 3) +\n  facet_grid(metric ~ horizon, scales = \"free\") +\n  labs(x = \" \", y = \" \")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html",
    "href": "11_bioinformatics.html",
    "title": "11  Bioinformatics",
    "section": "",
    "text": "12 Infer amplicon sequence variants (ASVs) and create sequence table\nRunning this step on all samples at the same time is computationally more intensive than running the pipeline one sample at a time but increases the functions ability to resolve low-abundance ASV, including singletons."
  },
  {
    "objectID": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "href": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "title": "11  Bioinformatics",
    "section": "11.1 Next-generation sequence data enable metabarcoding to describe community composition",
    "text": "11.1 Next-generation sequence data enable metabarcoding to describe community composition\nHigh-throughput sequencing data, often referred to as next-generation sequencing (NGS) data, is a type of biological data generated through advanced sequencing technologies that allow for the rapid and simultaneous sequencing of a large number of DNA or RNA molecules. These technologies have revolutionized genomics, transcriptomics, and other related fields by enabling the efficient and cost-effective analysis of genetic material on a massive scale. There are various high-throughput sequencing platforms available, including Illumina, Ion Torrent, Pacific Biosciences (PacBio), Oxford Nanopore Technologies (ONT), and others. High-throughput sequencing has become more cost-effective over time, making it accessible to a broader range of researchers and enabling large-scale genomics projects.\nThe key difference between high-throughput sequencing platforms compared to traditional Sanger sequencing3 is that they are capable of simultaneously sequencing thousands to millions of DNA fragments or RNA molecules in a single sequencing run. This parallel processing greatly increases the speed and efficiency of sequencing compared to traditional Sanger sequencing.3 anger sequencing, also known as dideoxy sequencing or chain-termination sequencing, is a traditional DNA sequencing method that was developed by Frederick Sanger and his colleagues in the late 1970s. It was the primary method for DNA sequencing for several decades before the advent of high-throughput next-generation sequencing (NGS) technologies. Sanger sequencing is still used today for specific applications, such as sequencing individual genes or validating NGS results.\nNGS platforms produce a vast amount of data. A single sequencing run can generate gigabytes to terabytes of raw sequencing data, depending on the instrument’s capacity and the type of experiment being conducted4. Typically, high-throughput sequencing generates short sequences, referred to as “reads.” The length of these reads can vary depending on the sequencing platform but is typically in the range of 50 to 300 base pairs. Usually you get one forward and one reverse read per sequenced template DNA.4  Don’t worry, we are going to use a data set that consists of samples have been modified to reduce size but still preserve realistic sequence variation and errors to create small example data set that your laptops can handle.\nIn the context of metabarcoding of environmental DNA this means that we can use universal primers to amplify template DNA present in the environmental sample that potentially are from different organisms5 because we can sequence all the amplified template DNA strands at once.5 If you used Sanger sequencing you would just get a bunch of noise, because the sequencer would not be able to make a distinct base call for any position"
  },
  {
    "objectID": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "href": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "title": "11  Bioinformatics",
    "section": "11.2 Bioinformatics pipelines are important tools to processing NGS data",
    "text": "11.2 Bioinformatics pipelines are important tools to processing NGS data\nAnalyzing high-throughput sequencing data requires sophisticated bioinformatics tools and pipelines. Bioinformatics pipelines are widely used in genomics and related fields to streamline and standardize data analysis workflows. They comprise a series of structured and automated series of computational and data analysis steps designed to process and analyze biological data. Frequently, data takes on of several paths through the pipeline where the output from one step becomes the input for the next step.\nGenerally, a bioinformatics pipeline takes a specific type(s) or raw input data, such as DNA or RNA sequence data, frequently generated from high-throughput techniques. This data set then goes through the first stage of the pipeline where it is pre-processed for analysis steps, including quality control, data cleaning, and format conversion to ensure the data set is robust and meets a standardized format for downstream analysis. Pipelines frequently consists of a series of analysis steps or modules tailored to specific research questions/tasks, for example sequence alignment, variant calling, gene expression quantification, protein structure prediction.\nBecause pipelines are designed to allow for a high degree of automation to reduce error and maximize efficiency. Frequently, users use scripts or specific workflow management tools to execute each analysis step in a predefined errors. Because of this automation, it can be tempting to “blackbox” the analysis where an inexperienced user can run complex data analysis without understanding what is happening at each stage. However, for an analysis appropriate to a specific data set it is critical that scientists understand how to configure parameters and options for each analysis step to customize the pipeline for the specific data set and research objectives.\nBioinformatics tools are often run from the command line. Linux is frequently the operating system of choice6 as it provides a powerful and standardized command-line interface that makes it easy to automate tasks, write scripts, and integrate various tools into analysis pipelines. It also offers a high degree of customization and flexibility due to its open source nature7. In addition, it is known for its efficiency and stability which is critical when running resource-intensive analysis.6 You’re laptop is likely MacOS or Windows and Linux would be the third common OS, however, it is not designed to be as user-friendly as Mac/Windows and has a steep learning curve.7 This also means its free!\nHigh performance clusters allow for efficient handling of large data set by allowing multiple tasks to be executed in parallel8.8 Your laptop probably has two to four cores, which means you could run tasks in parallel on up to two threads. Many linux servers have upward of 20 threads to run things in parallel on, for high performance clusters (HPCs you may be looking at 100s)\nDon’t worry, you’re not going to have to learn how to use a Linux Terminal. We will be using a pipeline that runs in R.\nDada2 (Callahan et al. 2016) is a bioinformatics pipeline which implements functions that can be used for the quality control, denoising, and analysis of amplicon sequencing data, particularly data generated from high-throughput sequencing technologies like Illumina’s MiSeq or HiSeq platforms. It is widely used in microbiome research and is becoming increasingly popular for the analysis of environmental DNA (eDNA) metabarcoding data sets.\n\nCallahan, Benjamin J., Paul J. McMurdie, Michael J. Rosen, Andrew W. Han, Amy Jo A. Johnson, and Susan P. Holmes. 2016. “DADA2: High-resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7, 7): 581–83. https://doi.org/10.1038/nmeth.3869.\nThe key modules (steps) include\n\nQuality Filtering: DADA2 begins by assessing the quality of each sequence read in the dataset. Low-quality reads, which may contain sequencing errors or noise, are filtered out. This step helps improve the accuracy of downstream analysis.\nSequence Dereplication: Identical sequences are collapsed into unique sequence variants (also known as amplicon sequence variants or ASVs). This step reduces redundancy in the dataset and accounts for potential PCR or sequencing errors.\nDenoising: DADA2 employs a statistical model to distinguish between true biological sequence variants and sequencing errors. By modeling the error profile of the sequencing data, it can identify and correct errors, resulting in more accurate sequence variants.\nChimera Removal: DADA2 identifies and removes chimera sequences, which are artificial sequences generated during PCR that can lead to incorrect biological interpretations.\nPhylogenetic Assignment: After denoising, DADA2 assigns taxonomy to the sequence variants by comparing them to reference databases, allowing researchers to identify the taxa present in the sample.\n\nIn contrast to other pipelines used to process metabarcoding data sets, DADA2 generates ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nExplain what the difference between an OTU and and ASV (include what each abbreviation stands for). You will want to revisit this question after having completed this tutorial and having a better understanding of the methods implemented in DADA2.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nEnough chit chat! Let’s go!"
  },
  {
    "objectID": "11_bioinformatics.html#demultiplexing",
    "href": "11_bioinformatics.html#demultiplexing",
    "title": "11  Bioinformatics",
    "section": "11.3 Demultiplexing",
    "text": "11.3 Demultiplexing\nGenerally, multiple samples are pooled and run on the same NGS sequencing lane. The amplified sequences (amplicons) of each sample are tagged with the same unique molecular barcode or index. This means that the first step is to take the entire output of a sequencing run and demultiplex it, i.e. each sequence read is assigned back to its respective sample based on the barcode/index combination.\n\n\nFASTQ files are a common file format used to store biological sequence data, particularly data generated by next-generation sequencing (NGS) technologies like Illumina sequencing platforms. In contrast to FASTA files which contain only sequence data, FASTQ files contain information about the sequences of DNA or RNA fragments, associated quality scores, and optional sequence identifiers.\nThis data set has already been demultiplexed and the individual fastq files are in your data folder."
  },
  {
    "objectID": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "href": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "title": "11  Bioinformatics",
    "section": "11.4 Quality check and filter sequences",
    "text": "11.4 Quality check and filter sequences\nIf you look in the data folder your project directory, you will see that you have a folder called seq that contains a folder for your raw sequences and also one that we will use to hold your sequences once we have done some quality filtering and trimming.\nBefore we get started we are going to create a series of vectors that contain our file paths and sample names that we can use throughout the analysis9. We are also going to extract the sample names and save them in a variable. We are able to do this because all of our filenames have a similar pattern.9 There are some key advantages to setting up vectors in this way. First, you only need to type in the information once, and then you can just call the vector which is a lot shorter and easier to type in. Second, you make the code more easily reusable for another analysis because you only need to change the file paths in one location and don’t have to find all the spots in your code where the file path needs to be changed. Finally, as you will see you are working with a large number of individual files, so when we create sample names based on the file names we don’t have to type them in all by hand which saves time and also minimizes the chances of typos.\n\n# designate file paths ----\n\n# create variable with initially trimmed reads\nraw &lt;- \"data/seq/raw\"\n\n# path for filtered reads\nfilt &lt;- \"data/seq/filt\"\n\n\n# create lists of matched forward & reverse fastq files ----\n\n# forward reads\nfnFs &lt;- sort(list.files(raw, pattern = \"_R1.fastq\", full.names = TRUE))\n\n# reverse reads\nfnRs &lt;- sort(list.files(raw, pattern = \"_R2.fastq\", full.names = TRUE))\n\n# create vector with sample names\nsample.names &lt;- substr(basename(fnFs), 1, (nchar(fnFs)-25))\n\n\n# create file names & paths for filtered reads ----\n\n# named vectors of forward reads \nfiltFs &lt;- file.path(filt, glue(\"{sample.names}_R1.fastq.gz\"))\nnames(filtFs) &lt;- sample.names\n\n# named vectors of reverse reads \nfiltRs &lt;- file.path(filt, glue(\"{sample.names}_R2.fastq.gz\"))\nnames(filtRs) &lt;- sample.names\n\nThere are 74 samples in the data set.\nNow that we have all of our file paths set up our next step is to take a look at the quality profile of the sequences. We could look at all of them, however, usually it is sufficient to spot check a few sequences to get an idea of what the quality looks like to give us an idea of what threshold values we should be using for quality filtering.\nDADA2 has a built in function plotQualityProfile() that pulls the information from the fastq files which contains the quality information for each nucleotide call. Quality is measured as PHRED scores, a score of 40 should be interpreted as an expected 1 in 10,000 error rate, 20 would be a 1 in 100 chance of a base call being incorrect.\n\n\nWe are wrapping the plotting function in ggplotly() which is built on the plotly package to generate interactive figures. It will take a second for your figure to pup up in the Viewer pane on the bottom left of Rstudio. If you hover over different points of the plot with your mouse you will see a little pop that lists the exact values.\nLet’s go ahead an plot the quality sequence for the first sample We can do this by passing the first file path in the fnFs vector using [1] to indicate the first element of the vector.\n\nggplotly(plotQualityProfile(fnFs[1]))\n\n\n\n\n\n\nFigure 11.1: Quality scores for the forward reads of the first sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is em quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nAs you can see there is a lot going on in this figure. The x-axis indicates the cycle number which is equivalent to the base position in the sequence (one base call is added per cycle) while the y-axis indicates the quality score. In the bottom left you can see the number of reads (sequences) in the file indicated. You can see grey shading which is a heatmap indicating the frequency of each score at each position. Darker colors indicate that a certain quality score is more common at that position across all the reads in the sample. The green line is the mean quality score, orange lines are the quartiles. The read lines indicates the proportion of reads that extend to that position.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nGo ahead and plot the reverse reads for the 12th sample in the data set. Then describe the quality patterns you are seeing for your forward and reverse reads. You can use the table below that summarizes how to interpret the quality scores.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplotly(plotQualityProfile(fnRs[12]))\n\n\n\n\n\n\nFigure 11.2: Quality scores for the reverse reads in 12th sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nReverse reads typically have lower quality and larger drop-off compared to forward reads. Trimming too conservatively can result in downstream issues when forward and reverse reads cannot be merged due to a lack of overlap.\n\n\n\n\n\n\n\n\n\n\nQuality score of base call\nConfidence of base call being correct\n\n\n\n\n10\n90\n\n\n20\n99\n\n\n30\n99.9\n\n\n40\n99.99"
  },
  {
    "objectID": "11_bioinformatics.html#trim-sequences",
    "href": "11_bioinformatics.html#trim-sequences",
    "title": "11  Bioinformatics",
    "section": "11.5 Trim sequences",
    "text": "11.5 Trim sequences\nWe will use the information from the quality scores to make decision on appropriate threshold values to use to trim our sequences.\nWe are going to use a series of filters to remove low quality reads from the samples. Specifically, we will remove any basecalls that were too ambiguous to call,10 and any remaining PhiX reads still in the data set11. Next, the first and last xx bases are trimmed for each read as these are usually low quality. Additionally, reads are truncated at first instance of a quality score &lt; 6. Finally, reads &lt; 35 bp after trimming are removed.10 N are unknown nucleotides. If the signal for a base is too ambiguous to make a call, the Illumina platform will call it N. DADA2 assumes there are no NNN in the data set so we have to remove them.11 PhiX is a bacteriophage. It’s DNA is spiked into libraries being sequenced to improve the quality sequencing run by increasing the sequence diversity\nWe are assuming matching order of forward and reverse reads for each sample.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the filterAndTrim() function and annotate each line of the code to indicate what each argument does, include both what the argument controls in general and then specifically what this means for this example.\n\n\n\n# filter reads\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, #\n                     truncLen = c(280, 280),     #\n                     trimLeft = c(18, 2),        #\n                     truncQ = 6,                 #\n                     maxEE = c(2,2),             #\n                     minLen = 35,                #\n                     rm.phix = TRUE,             #\n                     matchIDs = FALSE,           #\n                     compress = TRUE)            #\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nHere are descriptions of the key arguments that set thresholds for quality filters\n\ntruncQ: sets a minimum Q score. At the first instance of a quality score less than or equal to truncQ, the sequence is truncated.\ntruncLen: sets the length at which the sequences will be truncated. Sequences shorter than the length are eliminated.\ntrimLeft: sets the length that will be removed on the 5’ side of the reads. This allows you to remove the primers if it has not been done beforehand12.\nmaxEE: sets the maximum number of “expected errors” allowed in a read. This filter is based on the Q index. The more the number is increased, the less strict we are.\n\n12 Primers used are ITS3_KYO2: GATGAAGAACGYAGYRAA = 18bp ITS4: TCCTCCGCTTATTGATATGC = 20b\n\n\nLet’s take a look at what that function has done. We assigned the output to a variable called out.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse the functions class() and head() to get an idea of what type of object we have created and then briefly describe what information is contained in that object.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWait - that function has created a matrix array (a type of object you could describe as a multidimensional vector or a poor man’s data frame) that tells us how many reads per sample went into a the filters and then who many came out.\nBut where did our filtered reads go? Remember, we gave the function the file paths for both the input files (data/seq/raw/*.fastq.gz) and where to write the filtered files and what to name them (data/seq/fil/*.fastq.gz). If you use the file navigation pane you should see that it now contains those files.\nThe object we created that is now in our environment holds the record of what occurred during filtering. It is very common that bioinformatics pipelines generate output files at various steps that let the user track what is happening that can be used to ensure everything is going as expected and also allow them to pick up on unusual patterns that might be indicative of the fact that your threshold values might not be appropriate, that there is something odd going on with the data, or that perhaps commands where not properly formulated and therefore didn’t take place the way the researcher expected them to.\n\n\n\nLet’s take a look to see how many reads where removed from the data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThis code chunk converts the matrix into a dataframe. Use your coding skills to create a new column called perc_lost that contains the percent of reads that were lost in this filtering step. Then calculate the mean and standard deviation of the percent reads lost and the mean and standard deviation of the number of reads that remain per sample.\nThen use kable() to display your function in the console/your rendered html report.\nNote the inline code below that is pulling the key pieces of information directly from your data frame. This can be really helpful because e.g. if you are rerunning code on an updated data set you would not have to dig through your output to make sure that you updated all the results, instead R can do that for you. You can indicate that it is inline code using backticks and then indicating the engine using r. You will see that notation through out this document.\nThe function pull() allows you to extract (pull) a single value from a column.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n# summarize proportion of reads lost during trimming\ncheck &lt;- as.data.frame(out) %&gt;%\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nmean_loss\nstd_loss\nmean_reads\nstd_reads\n\n\n\n\n49.05541\n8.624355\n509.4459\n86.24355\n\n\n\nTable 11.1: Mean and standard deviation of the proportion of reads lost due to quality filtering and the mean and standard deviation of the number of remaining reads per sample post filtering.\n\n\n\n\n\nAfter quality trimming samples contain approx. 509% (+/- 86) reads. Quality trimming removed 49.1% (+/- 8.6%) reads from each sample.\nWe will track how many reads we “lose” at each step to understand how different steps affect the number of reads that remain in the sample.\nNext to knowing how many low quality reads where removed we will want to take a look at the quality of the remaining reads after we have trimmed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPlot the quality scores for the forward and reverse of one random sample and compare your results to the raw data. Make sure to add a figure caption using the code chunk options!\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what the plot for your forward reads should look like.\n\n\n\n\n\nFigure 11.3: Quality scores of forward reads for 16 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nRemember to plot the reverse reads for the same sample.\n\n\n\n\n\nFigure 11.4: Quality scores of reverse reads for 12 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position."
  },
  {
    "objectID": "11_bioinformatics.html#generate-error-model",
    "href": "11_bioinformatics.html#generate-error-model",
    "title": "11  Bioinformatics",
    "section": "11.6 Generate error model",
    "text": "11.6 Generate error model\nNext, we need to be able to distinguish between error due to for example PCR or sequencing error and actual biological differences among sequences. We can train DADA2 to be able to do this using a subset of our data as a training set using a machine learning approach to establish a parametric error model13 to estimate error rates.13 Note that the error rate is specific to a sequencing run\nError rates are generally expected to drop with increased quality. By default 100 Million bases are used to generate the error model, this number can be increased for a better estimate.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the code below to generate the error models. This will take a few seconds. While you wait, pull up the description of what this function does in the help pane and use comments to indicate what the function as a whole does and what the different arguments control.\n\n# forward reads\nerrF &lt;- learnErrors(filtFs,               # \n                    nbases = 1e8,         # \n                    multithread = FALSE)  # \n\n9877138 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n# reverse reads\nerrR &lt;- learnErrors(filtRs, \n                    nbases = 1e8,\n                    multithread = FALSE)\n\n10480322 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n\n\n\nMachine learning estimates and observed values should show close overlap to indicate the quality (fit) of the model. The observed error rates should be consistent with the expected learned error rates for the 16 possible base transitions. If they do not, the number of bases used can be increased to allow the ML algorithm to train on a larger subset of the data.\nWe can use the function dada2::plotErrors() to compare the observed and estimated error plots as an indication of how good our error models are. We will plot both the forward and reverse error model and assign those to objects so we can use the patchwork package to plot them next to each other in a single plot14.14 the package patchwork allows us to combine multiple plots in one file. You can read up on the basic layout options using +, \\ and | here and more a sneak peak of more complex layouts here\n\n# Visualize estimated error\np1 &lt;- plotErrors(errF, nominalQ = TRUE)\n\np2 &lt;- plotErrors(errR, nominalQ = TRUE)\n\np1 / p2\n\n\n\n\nFigure 11.5: Error rates for each possible transition for forward (top) and reverse reads (bottom). Observed (grey points) and estimated (black line) error rates for each consensus quality score. Expected error rates for nominal definition of the quality score are in red.\n\n\n\n\n\n\nThe plot will pop up in the Plot pane. You can change the size of the pane to resize and make it larger, it can also be helpful to use the zoom button to create a popout window for an even better look.\nNote also that you can use the code chunk option fig-height: to control the size in your rendered html output file.\nAs you can see this creates a series of plots - one for each possible transition e.g. the error frequency for an A being mistakenly called a C (A2C) etc. The black points are the observed error rates for each consensus quality scores. The black line is the estimated error rate after the model has converged. The red line indicates the expected error rates under the nominal definition of the Q-value for Illumina technology.\n\n\n\n\n\n\n Consider this\n\n\n\nUse the figures to briefly describe how well our error models capture the observed data and how this compares to the expected error rates.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "href": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "title": "11  Bioinformatics",
    "section": "12.1 Dereplicate, infer, and merge ASVs",
    "text": "12.1 Dereplicate, infer, and merge ASVs\nNow we are ready to identify the ASVs present in our data set.\nThe first step to do this is dereplication, which means that we are collapsing all identical reads into a set of unique sequences. Dereplicating or denoising data is an important step in amplicon processing workflows, instead of keeping all identical sequences only one is kept for processing and the number of sequences represented is stored a long with it. A new consensus quality score profile is calculated for each unique sequence based on the average quality score of each base for all sequences that are replicates of that given amplicon. These quality profiles Dereplication makes downstream processing a lot more efficient and less memory intense by eliminating redundant comparisons.\n\n\nThe function lapply() is similar to a for loop, where it applies the same function to every file in the vector containing the filtered samples.\n\nderepFs &lt;- lapply(filtFs,      \n                  derepFastq,      \n                  verbose = FALSE)  \n\nderepRs &lt;- lapply(filtRs, \n                  derepFastq, \n                  verbose = FALSE)\n\nIn the next step, sequence variants are inferred using the dereplicated data and the inferred error rates. Using the consensus quality profiles significantly increases DADA2’s accuracy.\n\n\nAs we’ve mentioned previously, many metabarcoding pipelines use OTUs instead of ASVs. For OTU’s reads that are at least 97% similar are clustered. There are two arguments for doing this, the first is many species have more than one haplotype for the same locus and so if you set the threshold for clustering at 100% you are oversplitting i.e. multiple unique sequences can “belong” to the same species. The second is that sequences from the same species might end up with different sequences not because of true biological differences but because of errors in the process of PCR amplifying sequences or during sequencing. However, ASVs are not the same thing as OTUs with a 100% threshold for clustering, instead, DADA2 uses the information from the quality profiles and the error models to distinguish true biological differences that separate unique amplicons vs amplicons that are only different due to errors (i.e. base differences are artifacts).\n\ndadaFs &lt;- dada(derepFs,            \n               err = errF,         \n               selfConsist = FALSE,\n               multithread = TRUE) \n\ndadaRs &lt;- dada(derepRs,\n               err = errR,\n               selfConsist = FALSE,\n               multithread = TRUE)\n\n\n\nDifferent sequence platforms and sequencing kits are limited by how long the reads can be. For example, this data set was sequenced on a 2x300bp Miseq platform. This means that each amplicon was sequenced 300 bp in the 5’ to 3’ direction and then 300bp in the 3’ to 5’ direction. This means that we can sequence inserts longer than 300 bp and merging the two strands (forward and reverse reads) increase the confidence in the reliability of the sequence in the overlapped region.\nOur last step is that we need to merge our forward and reverse reads to reconstruct the full target amplicon. Forward and reverse-complement of corresponding reverse sequences are aligned and merging requires an overlap of 12 sequences and there are no mismatches permitted in the overlapping region. Paired reads that do not exactly overlap are removed to reduce spurious output.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the mergePairs() function in the help pane and comment the code below to indicate what the function as a whole and each argument does.\n\nmergeASVs &lt;- mergePairs(dadaFs, filtFs, \n                        dadaRs, filtRs,\n                        minOverlap = 12,  \n                        maxMismatch = 0)\n\n\n\nWe can take a look at the results. The function returns a list16. Let’s take a look at the first element.16 Remember, we can think of a list as an object that is like a shelf where each shelf holds one element in this case each sample is an element\n\nhead(mergeASVs[1])\n\n$S1\n                                                                                                                                                                                                                                                                                                                                                                                                            sequence\n1             ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCCTTGGTATTCCGAGGGGCACACCCGTTTGAGTGTCGTGAATACTCTCAACCTTCTTGGTTTCTTTGACCACGAAGGCTTGGACTTTGGAGGTTTTTCTTGCTGGCCTCTTTAGAAGCCAGCTCCTCCTAAATGAATGGGTGGGGTCCGCTTTGCTGATCCTCGACGTGATAAGCATCTCTTCTACGTCTCAGTGTCAGCTCGGAACCCCGCTTTCCAACCGTCTTTGGACAAAGACAATGTTCGAGTTGCGACTCGACCTTACAAACCTTGACCTCAAATCGGGTGAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n2                                                                                             ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATCCCGAGGGGCATGCCTGTTCGAGCGTCATTTCACCACTCAAGCCTGGCTTGGTGTTGGGCGACGTCCCCTTTTGGGGACGCGTCTCGAAACGCTCGGCGGCGTGGCACCGGCTTTAAGCGTAGCAGAATCTTTCGCTTTGAAAGTCGGGGCCCCGTCTGCCGGAAGACCTACTCGCAAGGTTGACCTCGGATCAGGCAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n3                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTATAACCACTCAAGCCCCGGCTTGGTCTTGGGGTTCGCGGTCCGCGGCCCTTAAACTCAGTGGCGGTGCCGTCTGGCTCTAAGCGCAGTAATTCTCTCGCTATAGTGTCTAGGTGGTTGCTTGCCATAATCCCCCAATTTTTTACGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n4                                          ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n5          ATGCGATACGTAATGTGAATTGCAGAATTCCGTGAATCATTGAATCTTTGAACGCATCTTGCGCCTCTTGGTATTCCGAGGGGCATGCCTGTTTGAGTGTCATTAGAACTATCAAAAAAATAGATGATTTCAATCGTTAATTTTTTTGGAATTGGAGGTGGTGCTGGTCTTTTTCCATTAATGGCCCAAGCTCCTCCGAAATGCATTAGCGAATGCAGTGCACTTTTTCTCCTTGCTTTTTCTGGGCATTGATAGTTTACTCTCATGCCCTAAGCTGGTAGGGAGGAAGTCACAGAATGCTTCCCGCTCCTGAATGTAATACAAAACTTGACGATCAAACCCCTCAAATCAGGCAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n7                                                                                 ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATGATCAACCATCAAGCCTGGCTTGTCGTTGGACCCTGTTGTCTCTGGGCGACAGGTCCGAAAGATAATGACGGTGTCATGGCAACCCCGAATGCAACGAGCTTTTTTATAGGCACGCATTTAGTGGTTGGCAAGGCCCCCTCGTGCGTTATTATTTTCTTACGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n8                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTGCAACCCTCAAGCATTGCTTGGTATTGGGCTCCGCTGCTCACCCAGCGGGCCTTAAAATCAGTGGCGGTGCCGTCGAGGCCCTGAGCGTAGTAAATATCCTCGCTATAGGGACTCGGTGGACGCTGGCCATTAACCCCCAACTTTCTAAGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n9                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCCTCTGGTATTCCAGGGGGTATGCCTGTTCGAGCGTCATTACAACCCTCAAGCACTGCTTGGTATTGGATGTCAACCATTGGTGGTGCATCTCAAAAGTATTGGCAGTAGCATTTAGCTTCTAGTGTAGTAAATTTCTCGCTTTGGAGTCAAGTGTCTAATTGCTAGATAGAACCCCTAATTTATCAAAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n10                                                                                                   ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCGCTCGCGGCCGGCCCTAAAGACAGTGGCGGCAGCGTCTGGCTCCAAGCGTAGTACAATCCTCGCTCTGGTGCTAGGCGGTGGCCTGCCAGAACCCCCCTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n11                                                                                               ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCTTCTGGTATTCTGGGAGGTATGCCTGTTCGAGCGTCATTGCAACCATCAAGCCTAGGCTTGGTATTGGATGCCACCGCTTGGTGCATTTCAAAATTAGTGGCGGTGCCATTCAGCTTCAAGCGTAGTAAATTTCTCGCTCCTGGAGTTTGTATGTTGTCTGCTAGAACCCCCTAATTTATCAAGGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n12                                                                                                                       GCGCGATAGGTATTGTGAATTGCAGAATTGTGAATCATCGAATTTTTGAACGCACATTGCACCCATTGGTATTCCGATGGGTATACTTGTTTGAGCGTCATTTCATTCTCCTTTTGGGTTTTGGCATGAATATTTCTTGCTGAATTATAATGGTGTGGCTACCAGACTACAACGTGATAGATATTTCGTTGGATGTGACTGGGATTGCTCACCTTAAAAACATTGTATAGACCTCAAATCAAGCAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n13 ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCGCTCTTTGGTATTCCGAAGAGCATGCTTGTTTGAGTATCAGTAAACACCTCAACCTCCTCTTGTTTTTTCAAAAGGAGGGTGGACTTGAGCTATCCCAACAACCTTCACCGGTAGGCGGGCGGCTTGAAATGCAGGTGCAGCTGGACTTTTATCTGAGCTAAAAGCATATCTATTTAGTCCTCGTCAAACAGGATTATTACTATTGCTGCAGCTAACATAAAGGATAATTGTCCTCATTGCTGACTGATGCAGGATTTTACGACACTTTATGTGTTGTTCAACTCGATCTCAAATCAAGTAAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n14                                                                             ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCAACCATCAAGCCTGGCTTGTCGTTGGACCTCTTTGCCAATGAAATATGTGGCAGGTCCGAAAGATAATGACGGCGTCGTGTTTGACCCTAGATGCAACGAGCTTTTTATAGCACGCATTGATGTGGTCGGGCGACCCAGTCTTTAACCATTATTTTCTAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n15                                                                                 ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCTCCTGGTATTCCGGGAGGCATGCCTGTTCGAGCGTCATTAAAGACCACTCAAGCGATTTTGCTTGGTATTGGAAGAAGAGTGCCTCTGGCCCTCCCTTCCGAAATCCAATGGCGGAAAGTCTCACGTGCCCCGGCGTAGTAAGTTTATCTTTCGCTTGGACCCTGAGGCGTTCTCGCCCTCAAATCCCCAATACTATAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n16                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACCCTCTGGTCCATTCCAGAGGGTATGCCTGTTTGAGTGTCATTAATGTATCAAACCACCAAGCTTGCTTGGTTGGTCTTGGATGTTGAGGGTTGCTGGGGTTATAATGATCAGCTCCCTTTAAATGCATTAGCTTGGAATGTATAAGCCATTTTAGCTTAGGCTGATATGAATACAGCGTATTAAATGCTTTTGCTAAAGTGTAGCTTGTCTGGGCTTATAACTGTCTCTAGCTGAGACTGTCTTTTGACATTGTTAAATCATGATCATGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n17                                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCACCCTTTGGTATTCCGAAGGGTATGCCTGTTTGAGTGTCATTAAATTCTCAACTTCAAATTGAATTTTGAAGCTTGGACTTTGGAGGTTTGCTGGTGTCACTATCGGCTCCTCTTAAATTCATTAGCGGAACTGTAAGGACCGGCTTTGGTTTGATAGCTAACATTATCTATGCCGTTGCTGTGACCTTTGTGTTTGGCTTCTAATGGTCATTTTGTTGACTGTCTCTGCTTTGAGGCATACACTTTTAAGCTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n18                                                                                                    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCGGTGGTATTCCGCCGGGCATGCCTATTCGAGCGTCATTACAACCCTCACGCCCCGCGTGGTCTTGGGCCGAGCCCCCCGGGCTGGCCTCAAAAGCAGTGGCGGTGCCTCTGGGTCCTGAGCGTAGTAACACTTCCGCTACAGGGCTCCCGAGCGTGCTGGCCGAACCCCAACCCTTCAGGTTGACCTCGGATTAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n19                                                                                  ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCACAGTATCAAGCTTGGCTTGTCGTTGGGCCCTTTGTCACCTGGTGACAGGTCCCAAAGAGAATGACTGGTGTCGTAAAGACTCTAAATGCAACGAGCTTATAACAGCACGCATCTAGTAGTAATATGGCCCGGTTCTCACCTCTTTATTTCTCAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n21                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTACAACCCTCAAGCAATGCTTGGTGTTGGGCCGCGCCGCTAACCCGGCGGGCCCTAAAACCAGTGGCGGTGCCGTCGGGCTCTGAGCGTAGTAATTCTTCTCGCTATAGAGCCCCGGCGGATGCTAGCCAGCAACCCCCAATTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n22     ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCTCTTTGGTATTCCGAAGAGCATGCCTGTTTGAGTGTCATGAAAATATCAACCTTGACTTGGGTTTAGTGCTCTTGTCTTGGCTTGGATTTGGCTGTTTGCCGCTCGAAAGAGTCGGCTCAGCTTAAAAGTATTAGCTGGATCTGTCTTTGAGACTTGGTTTGACTTGGCGTAATAAGTTATTTCGCTGAGGACAATCTTCGGATTGGCCGAGTTTCTGGGACGTTTGTCCGCTTTCTAATACAAGTTCTAGCTTGCTAGACATGACTTTTTTATTATCTGGCCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n23                                                                                                    ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCCCGTGGCCGGCCCCAAAGTCAGTGGCGGTGCCGTCCGGCTCTAAGCGTAGTACATCTCTCGCTCTAGGGTCCCGCGGTGGCCTGCCAGAACCCCAACTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n24                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTAACCACTCAAGCCTAGCTTGGTATTGGGGCACGCGGTCTCGCGGCCCTTAAAATCAGTGGCGGCGCCGGTGGGCTCTAAGCGTAGTACATACTCCCGCTATAGAGTTCCCTCGGTGGCTCGCCAGAACCCCTAATTTTTACAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n25                                           ATGCGATACGTAATGTGAATTGCAGATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACTCTTTGGTATTCCGAAGAGTATGCCTGTTTCAGTATCATGAAAAACCTCACAAATTCAATTTTGGCTTTGTGGACTTGAGCATTTTGCGGCTTTGTTGCTGCTGGCTTAAAATATATTTCTTGGATAGCATATTATGGCTTTCGAAACTCGGCTTAATAGTTTTGGCTTTTGGTCAAATCTTTAGCTCTTTTCAAAGTCTTCAAGTTATTCAAAAGTTTTATACGAACACTTTCTCAATTTTGATCTGAAATCAGGTAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n26                                                                                               CTGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGGGGGGCATGCCTGTTCGAGCGTCACTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCTGCCGGCGACGGCAGGCCTTAAAACCAGTGGCGGCGCCGCTGGGCCCTGAGCGTAGTAATACTCCTCGCTACTGGGCCCCAGCGGATGCCTGCCAGCAAACCCAACTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n   abundance forward reverse nmatch nmismatch nindel prefer accept\n1        172       1       1    150         0      0      2   TRUE\n2         47       2       4    230         0      0      1   TRUE\n3         43       3       3    234         0      0      2   TRUE\n4         34       5       5    179         0      0      2   TRUE\n5         32       6       2    147         0      0      2   TRUE\n7         21       4       6    218         0      0      1   TRUE\n8         17       8       9    229         0      0      2   TRUE\n9         17       9       7    229         0      0      2   TRUE\n10        13       7      10    237         0      0      1   TRUE\n11        12      20      21    233         0      0      1   TRUE\n12        12      10       8    257         0      0      2   TRUE\n13        10      11      12    139         0      0      1   TRUE\n14         9      19      20    215         0      0      1   TRUE\n15         9      15      13    219         0      0      2   TRUE\n16         6      17      22    154         0      0      1   TRUE\n17         6      16      23    170         0      0      1   TRUE\n18         6      13      16    238         0      0      1   TRUE\n19         5      23      27    220         0      0      1   TRUE\n21         4      26      25    230         0      0      2   TRUE\n22         4      14      11    143         0      0      1   TRUE\n23         4      22      26    238         0      0      1   TRUE\n24         3      21      24    235         0      0      1   TRUE\n25         3      18      15    181         0      0      2   TRUE\n26         2      25      30    233         0      0      1   TRUE\n\n\nThis contains a dataframe where for each sequence (ASV) in that sample we get information on what the sequence looks like, the number of reads corresponding to this forward/reverse combination (abundance) etc.\nFor example we can query the largest and smallest overlap like this.\n\n# Largest overlap \nmax(mergeASVs[[1]]$nmatch) \n\n[1] 257\n\n# Smallest overlap\nmin(mergeASVs[[1]]$nmatch) \n\n[1] 139"
  },
  {
    "objectID": "11_bioinformatics.html#create-a-sequence-table",
    "href": "11_bioinformatics.html#create-a-sequence-table",
    "title": "11  Bioinformatics",
    "section": "12.2 Create a sequence table",
    "text": "12.2 Create a sequence table\nNow we have a fully denoised set of sequences that can be used to generate a sequence table for further analysis that contains the merged sequence, abundance and indices of forward and reverse sequence variants that were merged.\n\nseqtab &lt;- makeSequenceTable(mergeASVs)\n\nLet’s take a quick look at what this data set looks like.\n\ndim(seqtab)\n\n[1]  74 701\n\nseqtab[,1]\n\n S1 S10 S11 S12 S13 S14 S15 S16 S17 S18  S2 S20 S21 S22 S23 S24 S25 S26 S27 S28 \n  0 218   0   0 124   0  48   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS29  S3 S30 S31 S33 S34 S35 S36 S37 S39  S4 S41 S42 S43 S44 S45 S46 S47 S48 S49 \n  0   0   0   0   0   0  19   0 148   0   0   0   0  70   0   0   0   0 113  32 \n S5 S50 S51 S52 S53 S54 S55 S56 S58 S59  S6 S60 S61 S62 S63 S64 S65 S66 S67 S68 \n  0   0   0  46   0 117   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS69  S7 S72 S73 S74 S75 S76 S77 S78 S79  S8 S80 S81  S9 \n  0   0   0   0   0 234   0   0 109   0  43   0   0  40 \n\n\nWe can see that it has 74 rows (each sample is a row) and 701 columns (each ASV is a row). If we print just the first column (ASV) you can see that the way a sequence table is organized is that for each sample the number of times an ASV is observed is recorded."
  },
  {
    "objectID": "11_bioinformatics.html#remove-chimeras",
    "href": "11_bioinformatics.html#remove-chimeras",
    "title": "11  Bioinformatics",
    "section": "12.3 Remove chimeras",
    "text": "12.3 Remove chimeras\nThe DADA2 algorithm accounts for indel errors and substitutions when inferring ASVs but before taxonomic assignment we also need to check for chimeras17. DADA2 identifies sequences that are likely chimeras by aligning each sequence with sets of sequences that were recovered in higher abundance and then determining if any lower-abundant sequences can be made by mixing left and right sequences from two of the more abundant ones.17 Chimeras are non-biological sequences were the left- and right segment of the merged sequence are from two or more parent sequences.\nWe can use the function removeBimeraDenovo() to identify and remove Chimeras and then creating and updated sequence table.\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab,\n                                    method = \"consensus\", \n                                    multithread = FALSE, \n                                    verbose = TRUE)\n\nAfter removing chimeras, the data set consists of 700 unique sequences across 74 samples.\nEven though chimeric sequences can frequently make up a large part of sequence variants and therefore initially make the data set seem more variable than it is, overall once you account for abundance, they should only be a very small component of the merged sequence reads. Here 0.1 % of merged sequence variants are chimeras, though once you account for abundance of these variants, overall 99.9% of merged sequences are not chimeric.\nLet’s take a look at the distribution of our non-chimeric ASV lengths.\n\n\n\n\n\nFigure 12.1: Distribution of sequence length for merged ASVs. The red dotted line indicates sequences that are 100-105bp long. We are targeting a 106 bp amplicon and are using 2x150 bp sequencing. Primer sites are approx 25bp each."
  },
  {
    "objectID": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "href": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "title": "11  Bioinformatics",
    "section": "12.4 Summary of read filtering & processing for QC",
    "text": "12.4 Summary of read filtering & processing for QC\nAt this point, we will want to take a look at how many reads where lost at each step to determine if those patterns look as expected. We can pull that information from various output files by counting the reads and putting them all in a dataframe.\n\n# custom function to get read numbers\ngetN &lt;- function(x) sum(getUniques(x))\n\n# create data table with number of reads per sample at each step\ntrack &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergeASVs, getN), rowSums(seqtab.nochim)) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"sample\") %&gt;%\n  rename(trimmed = reads.out,\n         denoisedF = V3,\n         denoisedR = V4,\n         merged = V5,\n         rm_chimera = V6) %&gt;%\n  pivot_longer(names_to = \"filter\", values_to = \"reads\", cols = 2:7) %&gt;%\n  mutate(filter = ordered(filter, levels = c(\"reads.in\", \"trimmed\", \"denoisedF\", \"denoisedR\", \"merged\", \"rm_chimera\")),\n         k_reads = reads/1000)\n\n# plot distribution\nggplot(track, aes(x = filter, y = k_reads)) +\n  geom_boxplot(fill = \"darkorange\") +\n  labs(x = \"filter/processing step\", y = \"thousand reads per sample\") +\n  theme_standard\n\n\n\n\nFigure 12.2: Comparison of change in the number of reads per sample at each filtering & processing stage. Red dotted line indicates targeted 150k reads per sample.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at the boxplot and describe how the different processing steps have impacted the number of reads remaining in the data set.\nRemember that this is a modified data set where each sample has been subsampled to contain 1000 reads - normally that initial box describing the distribution of samples would be wider as different samples would contain different numbers of reads. Generally, researchers target &gt; 75,000 - 150,000 reads per sample to make sure that all taxa present can be recovered in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOutside of the initial filtering there should not be any steps at which a substantial amount of reads are lost, the majority of reads should merge. If this is not the case, then trimming is likely to conservative and should be revisited. Similarly, only a small proportion should be chimeric. If primers are not completely removed, ambiguous nucleotides can interfere during chimera ID and that step of quality filtering should be revisited."
  },
  {
    "objectID": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "href": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "title": "11  Bioinformatics",
    "section": "12.5 Taxonomic classification for ASVs present in each sample.",
    "text": "12.5 Taxonomic classification for ASVs present in each sample.\nWe are now in the final stretch. Now that we have our ASVs and our sequence table the only thing that we still need to do is figure out which species (or other taxonomic groups) our ASVs correspond to.\nThis brings us to an important limitation of metabarcoding studies which is that our results are only ever as good as our reference database to which we can match our ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nOur next step is to match the sequences to a list of sequences that have taxonomic information. In our case study this would be a sequenced fungi. Describe your expectations of the results - do you expect every ASV in the data set to find a match in the reference? What other issues could result in results being ambiguous?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nDADA2 has a built in function of a naive Bayesian classification menthod (assignTaxonomy()) that takes the sequences to be classified as the input along with a fasta file that contains the reference sequences18.18 fasta files consist of a header line that starts with &gt; and can contain any information about the sequence and then the sequence itself is in the next line. A multifasta file can contain multiple sequences, the program using the file determines the start of a new sequence by the fact that it “sees” the &gt; of the next header file.\nDepending on the project, scientists will pull sequences available from public databases such as genbank or if you are working with species that do not have a lot of sequences available you would need to create your own database by sampling individuals representing the different species/taxonomic groups you expect to encounter and sequencing them.\nWe are using the UNITE database of references which is designed to gather available ITS sequences to identify Eukaryotes.\nRunning this function is going to take several minutes - depending on the speed of your computer. Note that this code has the chunk option eval set as false so that it won’t run. Instead you can see that using the function save() I have writen the object out to your results folder. Then we can load it into your R environment in the next code chunk using load().\n\ntaxotab &lt;- assignTaxonomy(seqtab.nochim,\n                          refFasta = \"data/sh_general_release_dynamic_25.07.2023.fasta\",\n                          minBoot = 50, \n                          multithread = FALSE)\n\nsave(taxotab, \n     file = \"results/taxotab.rds\")\n\nWe can take a look at our results… there is a lot of information in this table so let’s just pull the first 5 ASV’s taxonomy without the ASV sequence for a better viewing experience.\n\n# load object into environment\nload(\"results/taxotab.rds\")\n\n# look at first 5 ASVs' taxonomy without the ASV sequence\nwrite.table(taxotab[1:5,], row.names = FALSE)\n\n\"Kingdom\" \"Phylum\" \"Class\" \"Order\" \"Family\" \"Genus\" \"Species\"\n\"k__Plantae\" NA NA NA NA NA NA\n\"k__Fungi\" \"p__Basidiomycota\" \"c__Agaricomycetes\" \"o__Agaricales\" \"f__Tricholomataceae\" \"g__Mycena\" NA\n\"k__Fungi\" \"p__Mucoromycota\" \"c__Umbelopsidomycetes\" \"o__Umbelopsidales\" \"f__Umbelopsidaceae\" \"g__Umbelopsis\" \"s__dimorpha\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Sordariomycetes\" \"o__Xylariales\" \"f__Amphisphaeriaceae\" \"g__Polyscytalum\" \"s__algarvense\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Dothideomycetes\" \"o__Mytilinidales\" \"f__Gloniaceae\" \"g__Cenococcum\" \"s__geophilum\"\n\n\nWe can also pull the unique species identified.\n\nunique(unname(taxotab[,7])) \n\n  [1] NA                    \"s__dimorpha\"         \"s__algarvense\"      \n  [4] \"s__geophilum\"        \"s__flavescens\"       \"s__terricola\"       \n  [7] \"s__elongatum\"        \"s__punicea\"          \"s__sylvestris\"      \n [10] \"s__humilis\"          \"s__subvinosa\"        \"s__opacum\"          \n [13] \"s__abramsii\"         \"s__ericae\"           \"s__terminalis\"      \n [16] \"s__variata\"          \"s__rexiana\"          \"s__zollingeri\"      \n [19] \"s__deciduus\"         \"s__album\"            \"s__chlamydosporicum\"\n [22] \"s__saponaceum\"       \"s__rufescens\"        \"s__populi\"          \n [25] \"s__podzolica\"        \"s__reidii\"           \"s__asperellum\"      \n [28] \"s__microspora\"       \"s__simile\"           \"s__acicola\"         \n [31] \"s__subsulphurea\"     \"s__camphoratus\"      \"s__pseudozygospora\" \n [34] \"s__heterochroma\"     \"s__brunneoviolacea\"  \"s__verrucosa\"       \n [37] \"s__auratus\"          \"s__mutabilis\"        \"s__chlorophana\"     \n [40] \"s__fuckelii\"         \"s__miniata\"          \"s__lignicola\"       \n [43] \"s__pilicola\"         \"s__phyllophila\"      \"s__australis\"       \n [46] \"s__citrina\"          \"s__fragilis\"         \"s__conica\"          \n [49] \"s__lubrica\"          \"s__pygmaeum\"         \"s__isabellina\"      \n [52] \"s__var._bulbopilosa\" \"s__finlandica\"       \"s__echinulatum\"     \n [55] \"s__lacmus\"           \"s__trabinellum\"      \"s__reginae\"         \n [58] \"s__spadicea\"         \"s__myriocarpa\"       \"s__physaroides\"     \n [61] \"s__calyptrata\"       \"s__nigrella\"         \"s__carneum\"         \n [64] \"s__vagans\"           \"s__metachroides\"     \"s__fumosa\"          \n [67] \"s__cantharellus\"     \"s__laetior\"          \"s__fusiformis\"      \n [70] \"s__spirale\"          \"s__pullulans\"        \"s__crocea\"          \n [73] \"s__sublilacina\"      \"s__acerinum\"         \"s__macrocystis\"     \n [76] \"s__vrijmoediae\"      \"s__changbaiensis\"    \"s__cygneicollum\"    \n [79] \"s__hymenocystis\"     \"s__dioscoreae\"       \"s__alnicola\"        \n [82] \"s__difforme\"         \"s__bicolor\"          \"s__spurius\"         \n [85] \"s__griseoviride\"     \"s__rebaudengoi\"      \"s__rufum\"           \n [88] \"s__globulifera\"      \"s__skinneri\"         \"s__sindonia\"        \n [91] \"s__verhagenii\"       \"s__maius\"            \"s__anomalovelatus\"  \n [94] \"s__diversispora\"     \"s__fellea\"           \"s__splendens\"       \n [97] \"s__coccinea\"         \"s__nitrata\"          \"s__risigallina\"     \n[100] \"s__juniperi\"         \"s__columbetta\"       \"s__rhododendri\"     \n[103] \"s__cinereus\"         \"s__fusispora\"        \"s__scaurus\"         \n[106] \"s__soppittii\"        \"s__grovesii\"         \"s__atropurpureum\"   \n[109] \"s__renispora\"        \"s__pura\"             \"s__foliicola\"       \n[112] \"s__phaeococcinea\"    \"s__rosea\"            \"s__stuposa\"         \n[115] \"s__minima\"           \"s__atrovirens\"       \"s__canadensis\"      \n[118] \"s__silvestris\"       \"s__sepiacea\"         \"s__pyriforme\"       \n[121] \"s__bulbillosa\"       \"s__glutinosum\"       \"s__cylichnium\"      \n[124] \"s__aeria\"            \"s__veluwensis\"       \"s__epicalamia\"      \n[127] \"s__hyalina\"          \"s__cylindrica\"       \"s__miyabei\"         \n[130] \"s__terrestris\"       \"s__rimosissimus\"     \"s__acuta\"           \n[133] \"s__myxotrichoides\"   \"s__physodes\"         \"s__alpina\"          \n[136] \"s__fallax\"           \"s__fumosibrunneus\"   \"s__albicastaneus\"   \n[139] \"s__mors-panacis\"     \"s__glacialis\"        \"s__acerina\"         \n[142] \"s__flavidum\"         \"s__ocularis\"         \"s__exigua\"          \n[145] \"s__piceae\"           \"s__verzuoliana\"      \"s__alliacea\"        \n[148] \"s__entomopaga\"       \"s__hyalocuspica\"     \"s__umbrosum\"        \n[151] \"s__bombacina\"        \"s__boeremae\"         \"s__fortinii\"        \n[154] \"s__miyagiana\"       \n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to additionally pull the unique genera, families, and orders contained in the data set and describe our results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#from-the-beginning",
    "href": "11_bioinformatics.html#from-the-beginning",
    "title": "11  Bioinformatics",
    "section": "12.6 From the beginning …",
    "text": "12.6 From the beginning …\n\n\n\n\n\n\n Consider this\n\n\n\nNow that we have been through all the steps of processing next generation sequencing data set in a metabarcoding processing pipeline go back over the entire process, make sure you have understand what the key steps are and then outline that process below. Be sure to list the key steps in an organized manner and describe what occurs at each step in 2-3 sentences.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#acknowledgements",
    "href": "11_bioinformatics.html#acknowledgements",
    "title": "11  Bioinformatics",
    "section": "12.7 Acknowledgements",
    "text": "12.7 Acknowledgements\nThis chapter borrows heavily from the original DADA2 tutorial as well as Alexis Carteron & Simon Morvan’s tutorial based on the subset sequences from (Carteron et al. 2021).\n\n\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7."
  }
]