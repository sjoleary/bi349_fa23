[
  {
    "objectID": "D_clim-change.html#investigating-climate-change",
    "href": "D_clim-change.html#investigating-climate-change",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Investigating Climate Change",
    "text": "Investigating Climate Change\nThe most recent IPCC Asessement has made two very clear statements regarding climate change:\n\nClimate change is “unequivocally” caused by humans.\nThe impact of anthropogenic climate change is “unprecedented”.\n\nBased on their assessment of studies having looked at global warming and climate change to date, the global mean warming is estimated at 1.1°C compared to the pre-industrial record. Additionally, under every emission scenario and average warming of 1.5 °C is likely to be reached in in next 20 years. Finally, some effects such as melting glaciers, ice sheets and the permafrost have been deemed irreversible.\nIn this module we are going to learn about data visualization using ggplot2 as we explore drivers and impacts of climate change. For our exploration of the drivers of climate change we will pull data sets describing temperature and greenhouse gas concentrations past and present. As we explore the impacts, we will consider whether the effects of climate change are something we will observe in the future if no action is taken or if we are already experiences widespread impacts changing the earth-climate system in ways that we can currently observe. And while we’re on the topic of visualizations and climate change we will then also explore how visualizations are used for misinformation."
  },
  {
    "objectID": "D_clim-change.html#the-earth-climate-system",
    "href": "D_clim-change.html#the-earth-climate-system",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The earth-climate system",
    "text": "The earth-climate system\nThe Earth-climate system, also known as the Earth’s climate system, refers to the complex and interconnected set of physical, chemical, biological, and atmospheric processes that govern the Earth’s climate. It describes all the components and interactions that determine the planet’s climate patterns and conditions over time.\nThe climate system is comprise of five major components: The hydrosphere, cryosphere, atmosphere, lithosphere, and biosphere.\n\nThe atmosphere consists of a mixture of gases, primarily nitrogen and oxygen, along with trace gases like carbon dioxide (CO2), methane (CH4), and water vapor (H2O). These gases play a critical role in regulating the planet’s temperature through the greenhouse effect.\nThe hydrosphere encompasses all of Earth’s water, including oceans, lakes, rivers, glaciers, and groundwater. The movement and distribution of water in the hydrosphere have a significant impact on climate patterns and weather events.\nThe cryosphere includes all of Earth’s frozen water, such as polar ice caps, glaciers, and permafrost. Changes in the cryosphere, such as melting ice, can have profound effects on sea levels and regional climate.\nThe lithosphere refers to Earth’s solid outer layer, including the continents, ocean floors, and the Earth’s crust. It plays a role in the redistribution of heat and the formation of land forms that can influence climate patterns.\nThe biosphere consists of all living organisms on Earth, including plants, animals, and microorganisms. Biological processes, such as photosynthesis and respiration, influence the composition of greenhouse gases in the atmosphere and affect climate.\n\nYou can think of the biosphere as the global ecosystem composed of all living organisms and the abiotics factors they derive energy and nutrients from. Another way to think of it is all the regions of the lithosphere, atmosphere, hydrosphere, and cryosphere occupied by living organisms. The fact that it is comprised of living organisms (biotic factors) sets it apart from the other components of the earth climate system.\nOverall, the climate system is an interactive system acted on by internal and external forcing mechanisms.\nThe components of the climate system are open systems with the freedom to exchange mass, heat, and momentum. For example the ocean and atmosphere exchange gases like carbon dioxide as the ocean acts a large carbon sink. Similarly, we observe an exchange of mass through the water cycle that links the atmosphere and hydrosphere through processes such as evaporation and condensation/precipitation. Even here in New England you can observe the exchange of heat as summer (air) temperatures cause the ocean to warm (minimally). Finally, surface waves are the result of the exchange of momentum as the wind causes the surface waters to move.\nNext to these internal mechanisms of the different components of the earth climate system impacting each other from within, the earth climate system is additionally impacted by external factors, primarily solar radiation. For example, the output of the sun heats the hydropshere and atmosphere. As the lithosphere encompasses the rigid outer part of the earth consisting of the crust and upper mantle, plate tectonics and volcanic eruptions are frequently also considered external mechanisms."
  },
  {
    "objectID": "D_clim-change.html#climate-regimes",
    "href": "D_clim-change.html#climate-regimes",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Climate regimes",
    "text": "Climate regimes\nWeather is the condition of the atmosphere for a specific time & place – climate is a long-term statistical portrait of a specific place, region, or the entire planet.\nWeather is a snapshot of atmopsheric conditions at a specific time for a specific place. It is directly observable and can be broken down into readily measureable, discrete characteristics including temperature and precipitation but also extending to include among others wind speed and direction, cloud cover and type, visibility, or air pressure.\nBy contrast, climate comprises the statistical averages of weather of long-term timescales & involves behavior of entire complex earth system. Generally, climate refers to the long-term patterns and average weather conditions along with extremes in a specific place, region or on Earth as a whole based on a single or multiple stations (locations). It represents the statistical summary of weather patterns over an extended period, typically 30 years or more.\n\n\n\nChanges in weather, climate variability, and climate change occur on very different time scales.\n\n\nBoth weather and climate do vary over time for natural reasons but on very different time scales. While weather can change at a moment’s notice2, climate variability describes (natural) shifts in climate conditions on decadal time scales. Finally, climate change describes long-term changes on scales of centuries to millenia.2 And as we all know the length of the “moment’s notice” is inversely proportional to the probability of you wearing an umbrella/having a jacket with you."
  },
  {
    "objectID": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "href": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The energy budget and global temperatures",
    "text": "The energy budget and global temperatures\nTemperature is a primary determinant of climate. Overall, earth maintains a stable average temperature (climate) by balancing energy received from the sun with energy emitted by earth back into space. Global temperature is a function of how much energy the earth receives and stores which in turn is influenced by three major factors:\n\nThe amount of energy received from the sun.\nReflection of energy by earth’s surface.\nAtmospheric composition (greenhouse gas effect).\n\nThe Earth’s energy budget is a concept that describes the balance between the incoming energy from the Sun and the outgoing energy radiated back into space from the Earth. It provides a framework for understanding how energy flows into and out of the Earth’s climate system. The Earth’s energy budget is essential for maintaining the planet’s temperature and climate.\n\n\n\nThe atmospheric energy budget source: weather.gov\n\n\nA material may transmit, reflect, emit or absorb radiation, and generally does more than one of these at a time. Earth’s energy budget consists of two different form of radiation\n\nIncoming shortwave radiation from the sun (Insolation): This is the energy received from the Sun. Sunlight, or solar radiation, is the primary source of energy for the Earth’s climate system. It includes both wavelengths in the visible and non-visible range, primarily UV.\nOutgoing longwave radiation: As the Earth’s surface and atmosphere absorb solar energy, they emit heat in the form of infrared radiation. This outgoing longwave radiation is a crucial part of the Earth’s energy budget.\n\nWhile some gases such as Ozone absorb shortwave (UV) radiation, Greenhouse gases such as water vapor, CO2, and Methane are defined by their property that they transmit short-wave radiation but absorb longwave radiation. This means that the greenhouse gases let through the incoming solar radiation but absorb large parts of the longwave radiation being emitted from earth’s surface. This so called greenhouse effect is a crucial component of the Earth’s climate system as it turns the atmosphere into a “warm blanket”. Ultimately, the energy absorption by the atmosphere stores more energy near the earth’s surface than if there was no atmosphere, making life on the planet possible in the first place.\nHowever, human activities have led to an enhance greenhouse effect. The reason why the IPCC describes current climate change as anthropogenic is that increasing levels of atmospheric CO2 and other greenhouse gases since the Industrial Revolution are driving the rapid increase in temperatures. Earth absorbs incoming solar radiation at its surface and emits long-wave radiation to maintain the energy balance at the surface. Only as small portion of that emitted radiation goes directly into space, most of it is absorbed by greenhouse gases (e.g. CO2) in the atmosphere. For the atmosphere to maintain its energy balance it emits radiation to space and back to earth. With increasing concentrations of GHG, the atmosphere absorbs and re-emits increasingly more energy. This creates an imbalance at earth’s surface and as a response earth continues to emit more energy to re-balance the budget and as a result global temperatures increase.\nIn this module we will look at several data sets that support the fact that the currently observed climate change is indeed unprecedented in the rate of change, that it is correlated to rapidly increasing greenhouse gases and therefore consistent with the description of being unequivocally human-caused and that the impacts of climate change are currently being observed across the earth climate system in a manner consistent with rapidly increasing global temperatures."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "href": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Genetic barcoding",
    "text": "Genetic barcoding\nGenetic barcoding, also known as DNA barcoding, is a molecular biology technique used to identify and distinguish between different species of organisms by analyzing a short and standardized DNA sequence. This method is based on the idea that each species has a unique DNA barcode or genetic signature within a particular region of its genome and is particularly useful for rapid species identification, especially when dealing with complex or difficult-to-identify specimens even from small, damaged, or industrial processed material.\nGenetic barcoding relies on the ability to amplify a standard gene (locus) across a wide range of taxonomic groups using universal primers. A good barcoding locus is typically a short, conserved section of DNA that contains enough genetic variation to differentiate between species (intraspecific variance) but remains relatively constant within a species (little to no intraspecific variance). The mitochondrial cytochrome c oxidase subunit 1 (COI) gene is a commonly used barcode region in animals, while other genes or regions may be used for plants, fungi, and microorganisms. The data set we will look at is fungi for which ITS2 is commonly used.\nRegardless of which genetic region is actually implemented the key steps remain the same:\n\nDNA Extraction: Genetic material (usually DNA) is extracted from the biological sample of interest, this can be tissue, cells, or even environmental DNA (eDNA) extracted from water, soil, or other sources.\nPCR Amplification: Polymerase chain reaction (PCR) is employed to selectively amplify the barcode region from the extracted DNA. Generally, a set of universal primers are used that will amplify in a wide range of taxonomic groups.\nSequencing: The PCR-amplified DNA fragments (Amplicons) are subjected to DNA sequencing, typically using Sanger sequencing. The resulting sequence data contain the barcode information.\nComparison to reference database: The obtained DNA barcode sequence is compared to a reference database containing sequences from known species. Bioinformatics tools and algorithms are used to search for matches or close matches in the database.\nSpecies Identification: Based on the comparison results, researchers can identify the species of the specimen. If the sequence closely matches a known barcode sequence in the database, the specimen can be confidently identified."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#metabarcoding",
    "href": "C_bioinformatics-eDNA.html#metabarcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Metabarcoding",
    "text": "Metabarcoding\nMetabarcoding of samples with mixed DNA templates allows us to characterize biological communities. Barcoding generally required there to be only one species present in the extracted DNA in order to get a clean sequence for comparison and taxonomic assignment. However, for many applications it would be useful to amplify and sequence DNA that potentially contains DNA from multiple species. Advances in sequencing technology (high throughput sequencing and next generation sequencing) has allowed us to perform metabarcoding studies for which the same steps apply except that during sequencing a large number of reads are produced. These sequences then need to be analyzed to identify the unique sequences present in the data set and then those are matched to a database."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#edna",
    "href": "C_bioinformatics-eDNA.html#edna",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "eDNA",
    "text": "eDNA\nEnvironmental DNA is DNA captured from an environmental sample without the need for pre-isolating specific targets. Macroorganisms shed DNA as cellular or extracellular material into the environment. Typical sources include mucous, the excretion of bodily fluids (feces, urine), and the sloughing off of skin cells, scales or other tissue. This means that we can capture DNA from an environmental sample without pre-isolating specific target organisms. For example, we take a water sample (1 – 5l) and then use a nitrocellulose filter to trap the DNA and then extract that DNA using very straightforward protocols.\n\n\n\n\n\n\n Consider this\n\n\n\nEnvironmental samples cover a wide range of ecosystems and habitats and spatiotemporal scales. Use examples to describe this variety of samples that can be used.\n\n\nGenerally, we refer to environmental DNA as DNA that is a “trace” of an organism in the environment, not the organisms itself. However, in some cases the same methods used to characterize biological communities using environmental DNA are applied to community DNA.\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast eDNA and community DNA and argue whether you would consider a gut content analysis to be eDNA or community DNA.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nDNA can be isolated from bulk-extracted from mixtures of organisms isolated from an environmental sample (Community DNA).\n\n\n\neDNA has high potential as a complementary method to characterize and monitor biological communities but users must be aware of biases and caveats to analyze the resulting data sets in a meaningful way.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the potential of eDNA in terms of applications and contrast this with some of the challenges that still need to be overcome."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#our-data-set",
    "href": "C_bioinformatics-eDNA.html#our-data-set",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Our Data set",
    "text": "Our Data set\nThe data set that we will be exploring used metabarcoding to characterize fungi communities in soil samples taken from different depths (soil horizons) in forests dominated by different trees and their mutualistic mycorrhizal fungi to explore how this affects resource competition with free-living saptrotrophs.\nHere is the abstract from (Carteron et al. 2021).\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7.\n\nIn temperate and boreal forests, competition for soil resources between free-living saprotrophs and ectomycorrhizal (EcM) fungi has been suggested to restrict saprotrophic fungal dominance to the most superficial organic soil horizons in forests dominated by EcM trees. By contrast, lower niche overlap with arbuscular mycorrhizal (AM) fungi could allow fungal saprotrophs to maintain this dominance into deeper soil horizons in AM-dominated forests. Here we used a natural gradient of adjacent forest patches that were dominated by either AM or EcM trees, or a mixture of both to determine how fungal communities characterized with highthroughput amplicon sequencing change across organic and mineral soil horizons. We found a general shift from saprotrophic to mycorrhizal fungal dominance with increasing soil depth in all forest mycorrhizal types, especially in organic horizons. Vertical changes in soil chemistry, including pH, organic matter, exchangeable cations, and extractable phosphorus, coincided with shifts in fungal community composition. Although fungal communities and soil chemistry differed among adjacent forest mycorrhizal types, variations were stronger within a given soil profile, pointing to the importance of considering horizons when characterizing soil fungal communities. Our results also suggest that in temperate forests, vertical shifts from saprotrophic to mycorrhizal fungi within organic and mineral horizons occur similarly in both ectomycorrhizal and arbuscular mycorrhizal forests."
  },
  {
    "objectID": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "href": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "title": "Data/Shark wrangling",
    "section": "Essential fish habitat: Shark Nurseries",
    "text": "Essential fish habitat: Shark Nurseries\nThe Magnuson-Stevens Act (1996) defined essential fish habitat as “those waters and substrate necessary to fish for spawning, breeding, feeding or growth to maturity”, i.e. they are habitats necessary for an organism to complete their life cycle. Identifying essential fish habitats is critical for management and conservation plans because it enables policy makers to prioritize certain ecosystems.\nWhile some elasmobranchs (sharks, rays, skates) inhabit estuaries year round, many use the estuaries for specific purposes such as feeding, mating, gestation, parturition or as nurseries and only inhabit them during specific life history stages. Estuaries are heavily impacted by humans - overfishing, pollution, habitat destruction and altered flow regimes all affect the biological communities they support.\nBroadly, shark nurseries are areas where young are born and/or reside in during maturation. Typically, these would areas that provide additional protection (e.g. mangroves for hiding) and plenty of food.\nShark Nurseries have three defining criteria(Heupel et al. 2018; Heupel, Carlson, and Simpfendorfer 2007):\n\nHeupel, Michelle R., Shiori Kanno, Ana P. B. Martins, Colin A. Simpfendorfer, Michelle R. Heupel, Shiori Kanno, Ana P. B. Martins, and Colin A. Simpfendorfer. 2018. “Advances in Understanding the Roles and Benefits of Nursery Areas for Elasmobranch Populations.” Marine and Freshwater Research 70 (7): 897–907. https://doi.org/10.1071/MF18081.\n\nHeupel, Michelle R., John K. Carlson, and Colin A. Simpfendorfer. 2007. “Shark Nursery Areas: Concepts, Definition, Characterization and Assumptions.” Marine Ecology Progress Series 337 (May): 287–97. https://doi.org/10.3354/meps337287.\n\nan area where sharks are more commonly encountered within compared to outside of.\nan area in which Young-of-the-year (YOY)/juveniles remain in or return to for extended periods of time.\nan area that is repeatedly used across years.\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe how you could design a study to identify estuaries that are shark nurseries."
  },
  {
    "objectID": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "href": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "title": "Data/Shark wrangling",
    "section": "Identifying shark nurseries on the Texas Coast",
    "text": "Identifying shark nurseries on the Texas Coast\nTexas Parks and Wildlife (TPWD) defines eight major estuaries along the Texas coastline and performs regular shore-based gill net surveys for 10 week periods in April - June and September to November.\n\n\n\nFigure 1: Map of major estuaries located along the Texas coast in the northwest Gulf of Mexico (Plumlee et al. 2018).\n\n\nAnalysis of this survey has identify eight elasmobranch species present in these ecosystems (Plumlee et al. 2018):\n\nPlumlee, Jeffrey D., Kaylan M. Dance, Philip Matich, John A. Mohan, Travis M. Richards, Thomas C. TinHan, Mark R. Fisher, and R. J. David Wells. 2018. “Community Structure of Elasmobranchs in Estuaries Along the Northwest Gulf of Mexico.” Estuarine, Coastal and Shelf Science 204 (May): 103–13. https://doi.org/10.1016/j.ecss.2018.02.023.\n\nBull shark\nBonnethead\nCownose ray\nBlacktip shark\nAtlantic stingray\nAtlantic sharpnose shark\nSpinner shark\nScalloped hammerhead\nFinetooth shark\nLemon shark\n\nGill nets generally exclude individuals &gt; 2m.\nMore recently, a multi-year open water long-lining study targeting elasmobranchs was performed in three estuarine locations near Corpus Christi, TX that are considered putative shark nurseries. Here, the sampling period lasted from May to November (Swift and Portnoy 2021).\n\nSwift, Dominic G., and David S. Portnoy. 2021. “Identification and Delineation of Essential Habitat for Elasmobranchs in Estuaries on the Texas Coast.” Estuaries and Coasts 44 (3): 788–800. https://doi.org/10.1007/s12237-020-00797-y.\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether or not you would expect to get similar results from both studies and what factors could result in differences.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere are some things to consider:\n\ngear-bias (hook size, net-size)\nseasonality (peak use for many coastal elasmobranchs is May - Oct)\nspatial (land-based vs open ocean)\n\n\n\n\nThis study wanted to answer four questions to further understand whether these locations should be considered shark nurseries and therefore as essential fish habitat for specific elasmobranch species.\n\nHow does the composition of elasmobranch communities compare across sites?\nHow does the catch-per-unit-effort (CPUE) per species and life history compare across sites?\nWhat do the sex ratios look like?\nWhat environmental predictors can we use to predict presence of elasmobranchs?\n\nIn this module we will interact with the data set generated for this study to learn how to wrangle data sets using R in the tidyverse and then we will apply those skills to answer these questions."
  },
  {
    "objectID": "14_impacts.html#a-grammar-of-graphics",
    "href": "14_impacts.html#a-grammar-of-graphics",
    "title": "14  Climate change: Impacts",
    "section": "14.1 A grammar of graphics",
    "text": "14.1 A grammar of graphics\nNow that we are confident(ish) in our data wrangling the next step is learning how we can visualize our data for exploration and for communication. We have already used figures using custom functions implemented in DADA2 that are based on the grammar of graphics/wrap functions from ggplot2 and we also familiarized ourselves with the syntax for using ggplot2 in the previous chapter.\nBeing able to visualize your data is an important skill throughout the process of data science. For example, in our next module we will dig into how to complete an exploratory analysis which is an important component of data science as it not only gives you an overview of what information is contained in a data set and can also help you refine the question you are asking. Finally, once you have refined and completed an analysis, visualizations are a key component to communicate your results.\nggplot2 is the core package of the tidyverse used for visualization. Similar to tidyr and dplyr having been built on the concept of a tidy data set, ggplot2 was built on a framework which follows a layered approach to describe and/or build any type of visualization in a structured way termed the “grammar of graphics”.\nThe grammar of graphics breaks down the components of any visualization into seven components.\n\nData is the component of data set to visualize1.\nMapping or Aesthetics include variables to be plotted, plot positions (e.g. x or y axis), and encodings such as size, shape, color2.\nGeometric objects (or layers) are what you actually end up seeing in the visualization, e.g. points, lines, bars etc. to represent data. This can also include statistical transformations3.\nScales map the values in the aesthetic space you specified, this includes your scales for color, shape, and size, you can also explicitly specify scales for the x and y axis.\nCoordinate system, includes all the usual suspects such as cartesian or polar along with some you may have never really heard of.\nFacets, i.e. subplots based on multiple dimensions.\nStatistics, including confidence intervals, means, quantiles, may additionally be added to the plot.\n\n1 You will see that this is either explicitly specified as the data argument (ggplot(data = df)), or you can just specify the data.frame or tibble to plot without typing in data = as ggplot(df).2 These components are specified as the mapping argument using aes(). Again you can explicitly call the argument as mapping =, i.e. ggplot(data = df, mapping = aes()), or you can just specify the mapping aesthetics using aes() because you are using the arguments in the correct sequence (ggplot(df, aes())) in which case R assumes they apply to these various arguments.3 Functions for these geometric objects will all start with geom_* or stat_*Additionally, you can create and use themes that control additional aspects of your your figure is displayed including font size, background color, etc.4.4 The package ggthemes includes various themes but you will also find that you can create your custom themes in a pretty straightforward way and that many R users have creted themes that they have made accessible for other users.\nUntil now you’ve probably thought of each plot type as it’s own distinctive format (a scatter plot, a pie chart, etc) and have not considered that each plot can be broken down into these fundamental components, so at first this way of thinking of plots will likely not seem intuitive.\nBut if you commit to thinking about plots in this abstract way you will quickly learn how these components fit together and realize the flexibility that this framework will give you to quickly generate exploratory plots, optimize your visualizations and be able to generate pretty much any plot you can think up5.5 You will also see that keeping your data tidy will allow you to customize your plots using different aesthetics and mappings to group your data."
  },
  {
    "objectID": "14_impacts.html#building-plots",
    "href": "14_impacts.html#building-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.2 Building plots",
    "text": "14.2 Building plots\nY’all ready for this?\nThen, let’s go back to our global mean temperature anomaly data set to figure out how the grammar of graphics can be applied and plot some data!\n\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\", skip = 1) %&gt;%\n  replace(. == \"***\", NA) %&gt;%\n  mutate_if(is.character, as.numeric)\n\nRecall how we created a line plot:\n\nggplot(temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nWe can generalize this into a basic template that can be used for different kinds of plots, including scatter plots, bar plots, and box plots.\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;\n\nEssentially, the minimum components needed to make a plot are\n\nA data set (&lt;DATA&gt;), this will be a data.frame or a tibble6.\nBased on the data, we need to define what variables we are going to plot (&lt;MAPPINGS&gt;) and how we want them to be represented, i.e. what individual data points should look like and how they are encoded.\nWe need to define the geometries, i.e. what type of plot (&lt;GEOM_FUNCTION).\n\n6 Remember, when we use read_delim() our data is automatically a object both of the format tibble and data.frame; while a tibble has some additional properties but we can use them interchangeablyLet’s break down what is happening to get a better understanding of how these minimum components fit together to create individual plots.\nFirst, we use the ggplot() function to define the specific data.frame to use to build the plot using the data argument (&lt;DATA&gt;)7.7 Remember, that when we are using arguments in the specified sequence that they are defined you do not explicitly need to call them.\nIn our example that would be\n\nggplot(data = temperature)\n\n\n\n\nFigure 14.1: Empty canvas generated using ggplot() function.\n\n\n\n\nNot much happens when you execute that piece of code other than creating a blank canvas in your plot panel8.8 Remember, if you are using and Rmarkdown or quarto document and have Chunk Output Inline enabled it will print beneath your code chunk\nThat is because we still need to define an aesthetic mapping using the aes() function (&lt;MAPPING&gt;).\nHere, we are selecting the variables that we want to plot (columns in our data.frame). At minimum we need to specify what columns we want to plot on the x and y axis, but we can also define variables (columns) we want to use to encode using color, shapes, size etc.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`))\n\n\n\n\nFigure 14.2: Coordinate system plotted after specifying mapping aesthetics, specifically which variables to plot on each axis.\n\n\n\n\nWhen we execute that code you will now get an empty coordinate system. This still seems like slim pickings but patience, young grasshopper, we have not yet defined the geom, i.e. what type of plot (“geometries”, &lt;GEOM_FUNCTION&gt;) we want to use. For example, here we want generate a line plot (geom_line()).\n\n\nFor ggplot we add layers using the + operator, which is a pipe (similar to %&gt;%) which tells R “and now add this”.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nFigure 14.3: Line plot (geom_line()) showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNow, we’re playing!\nOne of the advantages of the layered framework of ggplot2 is that we can plot multiple layers in the same plot by adding additional geoms.\nFor example, we can plot a scatterplot and line plot in the same plot as such:\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nFigure 14.4: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point().\n\n\n\n\nNow that you know how to generate a simple plot, let’s think about how we can further modify it to improve your visualization and how well you can communicate your results.\nFor example, we can change the color, fill, size, and shape for each geom layer.\n\n\nIn this example we are using the arguments for individual geom functions to change how data points are represented using colors and shapes for each layer, we can also use aes() to set mapping aesthetics for the entire plots.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3)\n\n\n\n\nFigure 14.5: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified.\n\n\n\n\nShapes for points can be specified using numbers; note how some shapes are “solid”, i.e. specifying color will define the color of the shape. Others, like the one used in our example are filled, i.e. color will determine the color of the outline, and fill the color of the space inside. For “hollow” shapes (or e.g. X’s) color will determine the line color.\n\n\n\nNumeric codes for sympols.\n\n\nYou can specify colors either using the color names defined by R or using hex codes.\n\n\nSome exploring will also lead you to various color pallets, many of them put together by R users around the world. Wes Anderson fan? You can style your plots accordingly… dig the aesthetics of the old school National Park posters and images? R community got you covered..\nPreviously, we also used geom_smooth() to add a layer with a linear regression. If you take a look at the arguments for this function using ?geom_smooth you will see that this has additional arguments apart from the mapping aesthetics like choosing the type of regression, whether or not the confidence interval is shown etc. Let’s say we want to add a red, dashed regression line without the confidence interval.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3) +\n  geom_smooth(stat = \"smooth\", se = FALSE, color = \"red\", linetype = \"dashed\", size = 2)\n\n\n\n\nFigure 14.6: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified. Trend line fitted using geom_smooth().\n\n\n\n\nAdmittedly, this plot is now starting to look a little bit ridiculous and is probably make it less, not more easy for the viewer to undestand what they can learn from the data. Maybe we should think of alternative options for visualizing this data to explore some additional ggplot options.\nAnother way to visualize this data would be using a bar plot.\n\n\nCan we appreciate for a second how easy it was to change how the same data is plotted? If you were using excel you would have had to insert a new plot, define what data you wanted to plot, relabel the axis, etc. Here all you had to do was change a single line of code.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.7: Bar plot showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNotice, how this is still the exact same mapping aesthetics (time/year on the x-axis and the temperature anomaly on the y-axis) but it changes the way we see the data completely - the emphasis now is on the change of temperatures below or above the long-term mean and the clear pattern of a shift of those temperature differences going from negative to being positive.\nLet’s say we want really lean into demonstrating this drastic change in temperatures not only being above the long-term average but also steadily increasing. One way to do this is to use color. We could modify this bar plot to have all of our bars representing years with global mean temperatures below the 1951-1980 average in one color and those above in another.\nTo do this, we need a column that encodes that information… that of course is not an issue for us as pretty much professional data wranglers! We can use a simple conditional mutate using an ifelse()9 statement to add a column (year_type) that indicates whether temperatures are above or below the the long-term average.9 recall, that we previously used case_when() for a conditional mutate, when the condtion is binary (true/false or this/that) using ifelse() makes for a much shoerte syntax.\nWe can manipulate our data.frame using the dplyr functions we are already familiar with. We can even use the %&gt;% pipe to pass the data argument directly to the ggplot() function1010 When you do this, remember that you need to switch back to using the + operator once you are adding your geom layers.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.8: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set.\n\n\n\n\nAnything that we add to the ggplot() layer as a mapping is a universal plot setting that will apply to all subsequent layers. You can override these global settings is you specify aesthetics like color, shape or size specifically in an separate layer. For example if we do the following, we will override our color coding even though we specified it in our ggplot() layer.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"blue\")\n\n\n\n\nFigure 14.9: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set. Settings in geom_bar() override the general plot specifications.\n\n\n\n\nLet’s see what else we can visualize using ggplot. Recall from when you explored our temperature anomaly data set, that this data set also contains information for seasons and individual months.\nLet’s take a look at the distribution of mean global winter temperatures (DJF = December, January, February) using a histogram, (geom_histogram()). Note that we only need to define our x-axis to plot a histogram, this is because ggplot2 will count how many observations fall into each bin for you … this geom is a mixture of a geometry (bar plot) and a statistical transformation (binning data points into ranges and the plotting those as bar plots).].\n\nggplot(temperature, aes(x = DJF)) +\n  geom_histogram()\n\n\n\n\nFigure 14.10: Histogram summarizing the distribution of mean global winter temperatures.\n\n\n\n\nWe can refine our plot by choosing our own bin width and by manipulating fill and color.\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\")\n\n\n\n\nFigure 14.11: Histogram displaying distribution of mean global winter temperatures with customized bin width and coloration.\n\n\n\n\nFor our example we might also want to add a vertical line (geom_vline()) to indicate 0 (i.e. the long-term global temperature mean calculated for 1951 - 1980).\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\") +\n  geom_vline(xintercept = 0, color = \"darkred\", linetype = \"dashed\", size = 1)\n\n\n\n\nFigure 14.12: Histogram displaying distribution of mean global winter temperatures with customized binwidth and coloration and added vertical line (red).\n\n\n\n\nIf we want to compare multiple distributions, box plots can be more helpful than histograms. Let’s say we wanted to compare the distribution of mean global temperature for each season.\nHow can we go about this?\nCurrently, our seasons are in individual columns, so our first step would be to create a tidy data set.\n\ntidy_season &lt;- temperature %&gt;%\n  select(Year, DJF, MAM, JJA, SON) %&gt;%\n  pivot_longer(names_to = \"season\", values_to = \"temperature\", 2:5)\n\nNow we can plot our seasons on the x-axis and the distribution of temperatures on the y-axis using geom_boxplot().\n\nggplot(tidy_season, aes(x = season, y = temperature)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.13: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons.\n\n\n\n\nAgain, you can add additional information using the mapping aesthetics to e.g. color code the boxes by season.\n\nggplot(tidy_season, aes(x = season, y = temperature, fill = season)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.14: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons color-coded by season."
  },
  {
    "objectID": "14_impacts.html#faceting-plots",
    "href": "14_impacts.html#faceting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.3 Faceting plots",
    "text": "14.3 Faceting plots\nOne of the advantages of ggplot2 being based not only on the grammar of graphics but also around the concept of a tidy data set is being able to create faceted plots. A faceted plot involves splitting a single plot into a matrix of panels, where each panel shows a different subset of the data. This is especially helpful during exploratory analysis where you might first plot all your data points in a single graph but then want to look at whether or not individual subsets within the data set behave the same.\nLet’s look at an example to better understand what faceting plots looks like. For example, let’s say we wanted to create individual plots of our bar plots showing our deviations of global temperatures from the 1951 - 1980 mean for each season.\nHow could you create individual plots with the methods you are already familiar with?\nHere is how you can do it using facet_grid().\nNow we can plot our data and using facet_grid() we can specify that we want individual panels by month in separate rows.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season))\n\n\n\n\nFigure 14.15: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nYou could also specify that you want the individual plots to be separated into columns.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(cols = vars(season))\n\n\n\n\nFigure 14.16: Change in temperature anomaly relative to long-term avarage per season (columns)\n\n\n\n\nYou can also create faceted plot where subset your data by two variables and so you end up with one variable defining the rows and one the columns and you can use a simpler syntax row-variable ~ column-variable.\nYou can also use this syntax if you are only faceting by one variable as we are doing in this example by specifying the variable and leaving the other one “blank” using a ..\nFor example, to plot this faceted data set in rows, you would use the following syntax -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(season ~ .)\n\n\n\n\nFigure 14.17: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nAnd to plot this data set column-wise you would specify it like so -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(. ~ season)\n\n\n\n\nFigure 14.18: Change in temperature anomaly relative to long-term avarage per season (columns)"
  },
  {
    "objectID": "14_impacts.html#customizing-plots",
    "href": "14_impacts.html#customizing-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.4 Customizing plots",
    "text": "14.4 Customizing plots\nWe have already played around a little bit with the options that we have in terms of customizing plots using color, fill, shapes, and sizes. But we’ve barely scratched the surface.\nYou have probably noticed that there are some default settings like the gray background and white grid lines, font sizes of label axis, what the axis labels are, and even color schemes that are automatically used. Even using the defaults we get pretty clean plots that are visually appealing. This is super helpful during exploratory analysis because even though you playing around with the data you still have nicely formatted and easy to interpret figures.\nOnce you have identified the central plots that you want to use to communicate the results and conclusions of your data analysis you will want to further customize your visualization to optimize communication, this includes how you encode data using color and shape but also making sure that everything is well labeled and clear to the person who is reading your report or listening to your presentation.\nOne of the first things we frequently want to changes is the axis labels. ggplot2 does handily use the column names to automatically label your axes, so you will always have a label which is great during exploratory analysis but generally you will want to customize that for your final figure. The function labs() can be used to specify a title, subtitle, axis labels and additional annotations (caption) below the figure.\nFor example we could customize our faceted figure to look like this:\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\")\n\n\n\n\nFigure 14.19: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nPretty much every component of a ggplot figure can be further customized using theme(), this includes things like font size, background and line colors, grids, legend position … ggplot2 odes have some pre-defined themes that you can call up that will change the layout.\nThe default theme is theme_grey().\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_grey()\n\n\n\n\nFigure 14.20: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nHere, we explicitly specified it using theme_grey(); if you do not specify a theme this is the theme that will be used for your plot. There are other themes that are part of the ggplot package that include theme_bw(), theme_minimal(), theme_classic() or theme_light().\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRe-plot the same figure using the four themes specified above.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nYou will likely find a theme that you like but still want or need to make additional tweaks. In that case the function theme() is your friend.\nLet’s go back to our bar plot of the global temperature anomaly to look at an example of likely the three most common things you will want to adjust which is the legend position, changing font size, color, turning x-axis labels by 90 degrees.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\n\n\n\nFigure 14.21: Change in mean global temperature relative to long-term avaerage with customized font sizes and legend positions.\n\n\n\n\nThat’s starting to look pretty slick.\nAs you start your visualization adventure the R Cookbook is a good resource. It is written in a helpful style that starts with a concrete plotting problem and then walks you through a solution."
  },
  {
    "objectID": "14_impacts.html#arranging-plots",
    "href": "14_impacts.html#arranging-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.5 Arranging plots",
    "text": "14.5 Arranging plots\nWe’ve already seen that we can create multi-panel plots in a very straightforward way when we are plotting the same plot for different subsets of the data (facetted plots).\nBut what if we want to generate individual plots that might fit together thematically but aren’t subsets that we can facet but we still want to be able to present them together? Fear not, this too can be solved; one option is using patchwork, a package designed for exactly this purpose. Let’s generate a few additional plots so we can try this out.\nIn our data folder there is a tab-delimited file with monthly mean CO2 concentrations (parts per million) from Maunua Loa. Let’s read in that data set and create line plot with a regression line.\n\ncarbon &lt;- read_delim(\"data/CO2_monthly.txt\", delim = \"\\t\")\n\nggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\n\n\n\nFigure 14.22: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory from 1958 - 2021.\n\n\n\n\nWe also have a data set from the Global Carbon Project downloaded from the Our World in Data repository that contains atmospheric CO2 emissions. We can plot that as a simple line plot with a regression.\n\nemissions &lt;- read_delim(\"data/emissions.txt\", delim = \"\\t\") %&gt;%\n  filter(iso_code == \"OWID_WRL\")\n\nggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"CO2 Emissions over time.\",\n       subtitle = \"Global emissions atmospheric emissions by year.\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\n\n\n\nFigure 14.23: Global atmospheric CO2 emissions.\n\n\n\n\nTo be able to plot multiple plots in one using patchwork we need to assign our figures as objects.\n\np1 &lt;- ggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\np2 &lt;- ggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2 Emissions\",\n       subtitle = \"global emissions\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\nNow we can combine them side by side using a simple syntax.\n\np1 + p2\n\n\n\n\nFigure 14.24: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (left panel) and Global atmospheric CO2 emissions (right panel).\n\n\n\n\nSimilarly, we can plot them underneath each other like so -\n\np1 / p2\n\n\n\n\nFigure 14.25: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (top panel) and Global atmospheric CO2 emissions (bottom panel).\n\n\n\n\nOh, it gets better. Let’s say we wanted to plot our global mean temperatures in the top row and our two emissions plots below.\n\np3 &lt;- temperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\np3 / (p1 | p2)\n\n\n\n\nFigure 14.26: Comparison of change in mean global temperature (top panel), atmospheric CO2 concentrations (bottom left), and atmospheric CO2 emissions (bottom right).\n\n\n\n\nYou can create complex compositions in patchwork using syntax combining +, |, and \\. To see how you can control the layout even further you can check out the documentation for patchwork."
  },
  {
    "objectID": "14_impacts.html#exporting-plots",
    "href": "14_impacts.html#exporting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.6 Exporting Plots",
    "text": "14.6 Exporting Plots\nThe last thing we still have to figure out is how to save plots to file. You have a few options.\nThe quickest and dirtiest option is to simply right click on the plot pane after plotting a figure and then save the figure using Save image as. This works and is easy to do but gives you very little control over the dimensions, resolution, file format etc.\nFor more control over the format of your figure you can use the Export tab in the Plot pane which will allow you to adjust the dimensions and the the file format.\nFinally, the ggplot2 has a function called ggsave() that will allow you to determine the dimensions (width, height), resolution (dpi), and format (device).\nBy default it will save the last plot that was plotted. If you specify the format in the file name (e.g. *.svg, *.jpg, *.png) it will automatically recognize the format11.11 Remember that we have designed our research compendium to keep raw and processed data, and results separate? Figures are considered results so you should always save them to the results folder.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\nggsave(\"results/global_temp.png\", dpi = 300, width = 15, height = 10)\n\n\n\n\nFigure 14.27: Change in mean global temperature realtive to long-term average.\n\n\n\n\nCheck your results folder to see if you were successful!\nIf you assign your plot to an object you can use the plot argument of the ggsave() function to export it. This also works for figures that consist of multiple panels combined using patchwork."
  },
  {
    "objectID": "14_impacts.html#more-plots",
    "href": "14_impacts.html#more-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.7 More Plots!",
    "text": "14.7 More Plots!\nReady to take the training wheels off?\nIncreasing global temperatures are effecting change across the different components of the climate system. There are several data sets in your data folder. Read in each data set as a data frame, use View() to take a look at what information is contained in the data set and the use your new found visualization skills to plot the data using ggplot.\nWe have looked at rising mean global temperatures and discovered that rapidly increasing emissions and atmospheric concentrations of CO2 are consistent with increased concentrations of greenhouse gases driving temperature change.\nThe effects of global warming are observed throughout components of the climate system including the atmosphere, the hydrosphere (marine and freshwater systems), cryosphere (land and sea ice), and lithosphere (earth’s surface/crust). Let’s look at a few data sets that illustrate the impact of rising temperatures.\nFor all the plots, comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIncreasing atmospheric temperatures result in land ice melting and the ocean heat content rising. The latter leads to thermal expansion which together with the increase freshwater inflow results in rising sea levels.\nThe first data set is from the NOAA Laboratory for Satellite Altimetry and contains records from coastal sea level tide gauges. Start by reading in the data set:\n\nsealevel &lt;- read_delim(\"data/sealevel.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set12\nPlot the change in mean sea level over time as a line plot and color code the line(s) by method (see column names). Chose a theme and position your legend below the figure.\nFigure out how to use geom_hline() to add a line at 0.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n12 Include things like the number of columns, what data is contained in each sample, what time periods are included etc. Practice deducing these types of things from the context of the data set description above and by browsing the content.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s an example of what tour final figure could look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOur next data set is from the National Snow and Ice Data Center and contains the Arctic (Northern Hemisphere) July sea ice extent.\n\nice_arctic &lt;- read_delim(\"data/arctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a theme. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s take a look at the equivalent data set depicting sea ice extent in the Antarctic (Southern Hemisphere.)\n\nice_arctic &lt;- read_delim(\"data/antarctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a new theme from what you’ve been using previously. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOne more data set. This one is from the NOAA Hurricane Research Division. While interpreting the observed patterns in sea level and ice extent and modeling future projections is pretty straightforward, understanding whether extreme events are getting “worse” is a little bit more complicated13.\nNote: For this coding challenge you will need to combine your data wrangling and visualization skills!\n\nhurricane &lt;- read_delim(\"data/hurricanes.txt\", delim = \"\\t\")\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific14.\nPlot 1:\nWhen thinking about hurricanes there are two metrics to consider when determining whether or not “hurricanes have gotten worse”. First, we can look at whether or not hurricanes have become more common by plotting the number of hurricanes for each year.\n\nCreate a faceted plot with individual line plots showing the number of named storms (this would include not only hurricanes put also tropical storms), the number of hurricanes, the number of major hurricanes, and the number of hurricanes in the United States.\nAdd a linear regression.\nChose a custom theme.\n\n\n\nExtra Challenge: We’ve previously used facet_grid() to create faceted plots which requires us to specify how panels should be laid out in rows and/or columns. Figure out how to use facet_wrap() to just specify which variable to use to create individual panels with out them all needing to be laid out next to each other in columns or underneath each other in rows.\n\nPlot 2:\nThe other way to evaluate whether or not “hurricanes have gotten worse” is to not look at the number of storms but to determine whether individual storms or storms as a whole have become more intense and/or destructive. One of the columns you may not have been able to fully figure out what information it contains is the column containing information on the “accumulated cyclone energy”15.\n\nCreate a bar plot showing the ACE for each year.\nChose a theme and customize the color of your bars16 3. Extra Challenge: Use geom_hline() to add a horizontal line indicating the mean ACE for the recorded time period.\nWrite a short description for each figure summarizing the key results displayed and argue whether or not you think “hurricanes have gotten worse”.\n\n16 You can specify color and fill for bar plots. Try setting the color and fill to the same and to different colors to see what happens when you have a bar plot with this many individual bars.\n15 Now that you have this information gp ahead and update your notes to add it to your short data set description.14 Note, several of the columns start with ‘Revised’ this just has to do with the entire data set having revised (most recent) definitions applied for the metrics included. You can ignore that component.13 We will take a closer look at this aspect when we combine our data wrangling and visualization skills to learn how to perform an exploratory analysis\nYou will have to reformat the data set in order to plot it in this fashion. It can be helpful to first think about what your ggplot code should look like to create the plot described above and then format your data set to fit those needs. Recall that ggplot is designed not only around the grammar of graphics but also the principles of tidy data… so, do we currently have a tidy data set? Or what would it need to look like for this plotting challenge?.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot 1:\nPlot 2:\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChoose five of the plots we have generated in this chapter and combine them into a single plot using patchwork; include at least two columns and two rows in your layout17.\nSave your plot in a way that retains proper proportions to make it easy for somebody else to easily see the details and discern the general patterns. Share your multi-panel plot in the #assignment channel of our slack workspace.\nUse your the figure you have created to support argue that “the observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures”. Draw from the descriptions you made for the figures throughout this challenges to summarize what you observe to support this claim. In your answer you will want to consider which of the data sets refer to which component of the earth-climate system and why the data sets you have looked at are consistent with increasing mean global temperatures.\nExtra challenge: You can use #| fig-height and #| fig-width; code chunk options to specify the dimensions of the output of a figure in the knitted document18. When you initially render your quarto document you will find that the default dimensions will not format your you multi-panel plot in a way that does not make it super legible. Manipulate these parameters and re-knit your html to display your figure in an appropriate way.\n\n\n18 Yes, this does mean that you will need to check the format of your html output and potentially change it before you submit. Checking your html report before you submit is a good habit to have just to make sure that everything is rendering as expected17 This can include your temperature anomaly data set as well.\n\n\n\n\n\n Did it!\n\n\n\nFigure\nThe observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nConsider the following and write a short paragraph (7-10 sentences):\n\nThink back over the analyses we have performed for this and the previous chapter and argue whether the analysis you have performed should be considered descriptive, exploratory, inferential/predictive, or causal/mechanistic.\nDescribe both what you actually did, and then also briefly sketch out what your experimental design/analysis would need to look like if you had performed analysis in the other categories.\nCausal/mechanistic analyses are the most involved frequently also the most difficult to perform. Argue whether or not your results are less meaningful or informative if they are not mechanistic/causal or whether they are still useful to support the initial thesis that climate change is unprecedented, unequivocally anthropogenic, and the impacts are already impacting all components of the earth-climate system.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "12_community-diversity.html#compile-data-sets",
    "href": "12_community-diversity.html#compile-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.1 Compile data sets",
    "text": "12.1 Compile data sets\nWe are going to import the data you will need for this chapter exploring how to characterize biological communities. We’ll start by loading to objects into your environment. One is the ASV table and the second is the corresponding taxonomy table from the fungi data set we were working on in the previous chapter.\nWe are going to read in a data set that contains information about the soil plots from which the fungi eDNA was isolated.\n\n# load sample data\nsoil &lt;- read_delim(\"data/soil.csv\", delim = \";\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to take a look at the soil data set and the describe what information it contains (typical things you want to check are row, column numbers, column names, determining if it is a tidy data set, figuring out what information/variables are in the data set).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThe package phyloseq that we installed has various utility functions to deal with metabarcoding data set. It has a specific object class that allows us to store the asv table, taxonomy table, and sample meta data in different slots. Various functions can then be used to pull information from those slots. We can load the object ps directly into your environment using the following code.\n\n# create phyloseq object\nload(file = \"data/ps.rdata\")"
  },
  {
    "objectID": "12_community-diversity.html#data-filtering-and-transformation",
    "href": "12_community-diversity.html#data-filtering-and-transformation",
    "title": "12  Characterizing community diversity",
    "section": "12.2 Data filtering and transformation",
    "text": "12.2 Data filtering and transformation\nBefore we can start exploring our data set we have a few steps to complete to transform it into containing the data we want in the format we want tit.\nLet’s start by taking a looking at how many taxa are currently present in the data set using phyloseq::ntaxa()\n\nntaxa(ps)\n\n[1] 546\n\n\nWe only want ASVs that were assigned as fungi in our reference database. We can use phyloseq::subset_taxa to filter by kingdom.\n\nps_fungi &lt;- subset_taxa(ps, Kingdom == \"k__Fungi\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many non-fungi groups were filtered.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNext, we want to remove any singletons or doubletons. These are ASVs that are only represented by one or two reads - we can assume these are artifacts.\n\nps_fungi_nosd &lt;- filter_taxa(ps_fungi, function(x) sum(x) &gt; 2, TRUE)\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many taxonomic groups are still in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nIn many situations, metabarcoding eDNA samples is semi-quantitative2. The number of DNA templates in the environment are correlated to the number of individuals/biomass of a given taxonomic group. While you cannot necessarily back calculate the absolute abundance or number of specimen present in an environment you can use the proportion of reads assigned to a taxonomic group as a metric of relative abundance.2 We can always confidentally use metbarcoding data as qualitative data, i.e. as presence/absence data. Though even here we should be careful about whether “not detected” should be interpreted as absent.\nOne way to convert species abundance from absolute to relative is using a Hellinger transformation which standardizes the abundances to the sample totals and then square roots them. The function phyloseq::transform_sample_counts() allows us to apply a function to transform sample counts.\n\nps_fungi_nosd_hel &lt;- transform_sample_counts(ps_fungi_nosd, function(x) sqrt(x/sum(x)))\n\n\n\n\n\n\n\n Consider this\n\n\n\nOur data set could still contain multiple ASVs that have been assigned to the same species. Explain why this is an expected out come when using ASVs but would be rare/non-existant if you are using OTUs as the output of your bioinformatics pipeline.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nTo complete our data filtering and transformation we will use phyloseq::tax_glom() to collapse ASVs asigned to the same taxonomic group at the species level.\n\nps_transf = tax_glom(ps_fungi_nosd_hel, \"Species\", NArm = FALSE)\n\nLet’s explore our final transformed data set\n\n\n\n\n\n\n Give it a whirl\n\n\n\nApply the functions ntaxa(), nsamples(), rank_names(), and sample_variable() to our final transformed phyloseq object. Look up what each function does, make sure to comment/annotate your code and then briefly describe what you’ve learned about our data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nRecall that our phyloseq object contains a slot that holds our ASVs table and our taxonomic table that we can access at as such.\n\notu_table(ps_transf)[1:2, 1:2]\n\nOTU Table:          [2 taxa and 2 samples]\n                     taxa are columns\n    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                                            0.00000000\nS11                                                                                                                                                                                                                                                                                                                                                            0.08178608\n    ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                               0.0000000\nS11                                                                                                                                                                                                                                                                                                                                               0.2085144\n\ntax_table(ps_transf)[1:2, 1:2]\n\nTaxonomy Table:     [2 taxa by 2 taxonomic ranks]:\n                                                                                                                                                                                                                                                                                                                                                                      Kingdom   \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"k__Fungi\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"k__Fungi\"\n                                                                                                                                                                                                                                                                                                                                                                      Phylum            \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"p__Basidiomycota\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"p__Mucoromycota\" \n\n\n\n\n\n\n\n\n Consider this\n\n\n\nExplain how the notation [1:2, 1:2] modifies the output\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "12_community-diversity.html#combine-data-sets",
    "href": "12_community-diversity.html#combine-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.3 Combine data sets",
    "text": "12.3 Combine data sets\nWe have three explanatory variables that could be driving differences in fungal communities among samples.\n\nSoil samples were taken in differnt forest plots that were classified as dominated by Acer saccharum (AS) or Fagus grandifolia (FG) or mixed with other small trees and shrubs present (mixed).\nSoil samples where taken from different soil horizons (depths): L, F, H, Ae, or B\nSoil chemistry (carbon, nitrogen, pH)\n\nThis information is stored in our soil dataframe.\nNow that we’ve filtered and transformed our data set, let’s pull it back out to create a dataframe as the object you are more familiar with in terms of being able to manipulate it.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s pull out our taxonomic table and transform it into a dataframe. Comment the following code line by line to describe what each function/arguments is doing.\n\nasv_tax &lt;- tax_table(ps_transf) %&gt;%  #\n  as.data.frame() %&gt;%            #\n  rownames_to_column(\"asv\")      #\n\n# write out interim file\nwrite_delim(asv_tax, \"results/asv_tax.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s pull out our information on how many times each ASV is observed in a each sample. Comment the following code line by line to describe what each function/arguments is doing. You may need to look up some of the functions.\n\nasv_counts &lt;- otu_table(ps_transf) %&gt;%  #\n  t() %&gt;%                               #\n  as.data.frame() %&gt;%                   #\n  rownames_to_column(\"asv\")             #\n\n# write out interim file\nwrite_delim(asv_counts, \"results/asv_counts.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a look at how your asv_tax and asv_count objects are now formatted and briefly describe it (remember, key things are number or rows, columns, what those columns are).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nReally, what we want is for our asv_counts table to contain the taxonomic information contained in the asv_tax table. Combine those two data sets into an object called tax_count. Remove the ASV sequence from the dataframe and arrange the remaining columns to first have all the taxonomic information, then the number of occurrences in each sample. Print the first few lines to the console when you are done.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like\n\n\n   Kingdom           Phylum                 Class             Order\n1 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n2 k__Fungi  p__Mucoromycota c__Umbelopsidomycetes o__Umbelopsidales\n3 k__Fungi    p__Ascomycota    c__Dothideomycetes  o__Mytilinidales\n4 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n5 k__Fungi    p__Ascomycota    c__Sordariomycetes     o__Xylariales\n6 k__Fungi    p__Ascomycota                  &lt;NA&gt;              &lt;NA&gt;\n                Family           Genus       Species       S10        S11\n1  f__Tricholomataceae       g__Mycena          &lt;NA&gt; 0.0000000 0.08178608\n2   f__Umbelopsidaceae   g__Umbelopsis   s__dimorpha 0.0000000 0.20851441\n3        f__Gloniaceae   g__Cenococcum  s__geophilum 0.0000000 0.32414749\n4    f__Hygrophoraceae    g__Hygrocybe s__flavescens 0.2929066 0.40028795\n5 f__Amphisphaeriaceae g__Polyscytalum s__algarvense 0.0000000 0.00000000\n6                 &lt;NA&gt;            &lt;NA&gt;          &lt;NA&gt; 0.5073291 0.22398041\n        S12        S13       S14        S15       S16        S17       S18\n1 0.0000000 0.09578263 0.2379155 0.04867924 0.7712319 1.25129257 0.4761444\n2 0.2721655 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n3 0.5233063 0.13545709 0.0000000 0.23676137 0.2667853 0.00000000 0.0000000\n4 0.0000000 0.00000000 0.0000000 0.06884284 0.0000000 0.00000000 0.0000000\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.3165055\n6 0.1200137 0.19156526 0.0000000 0.13299415 0.2226355 0.07392213 0.5152174\n          S1       S20        S21        S22        S23       S24       S25 S26\n1 0.07372098 0.1543370 0.15971703 0.04598005 0.17325923 0.1726902 0.2505837   0\n2 0.24818179 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n3 0.36422877 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n4 0.24077171 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n5 0.00000000 0.6046415 0.06052275 0.34039602 0.00000000 0.4626177 0.3245927   0\n6 0.41216069 0.5922461 0.25094334 0.47371431 0.04331481 0.3527087 0.5436833   0\n        S27        S28        S29        S2       S30       S31        S33\n1 0.9263177 0.04145133 0.27471034 0.1142577 0.4917893 0.4379003 0.07124705\n2 0.0000000 0.00000000 0.00000000 0.1842351 0.0000000 0.0000000 0.00000000\n3 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.19991094\n4 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.00000000\n5 0.0000000 0.13747852 0.04729838 0.0000000 0.3446360 0.3636152 0.00000000\n6 0.0000000 0.33819215 0.17651995 0.7626946 0.1953884 0.8801878 0.12162632\n        S34        S35       S36        S37       S39         S3       S41\n1 0.0000000 0.04393748 0.0559017 0.09449112 0.1474420 0.06516352 0.1036952\n2 0.3815359 0.00000000 0.2091650 0.18898224 0.6158141 0.36572936 0.0000000\n3 0.0000000 0.77616588 0.1118034 0.33838581 0.2197935 0.00000000 0.0000000\n4 0.5209007 0.00000000 0.0000000 0.00000000 0.0695048 0.14497221 0.2375955\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n6 0.0000000 0.00000000 0.0000000 0.29827034 0.4928534 0.00000000 1.1342749\n         S42        S43       S44       S45        S46       S47       S48\n1 0.07808688 0.05123155 0.1952834 0.1954906 0.08441499 0.0000000 0.1773317\n2 0.19908326 0.00000000 0.0000000 0.0000000 0.09747404 0.0000000 0.0000000\n3 0.17460757 0.61471931 0.1301889 0.0000000 0.21037942 0.5733137 0.6634328\n4 0.68553927 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n5 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n6 0.33809195 0.33116596 0.0000000 0.1382327 0.10897929 0.3473466 0.0000000\n         S49         S4       S50       S51        S52       S53        S54\n1 0.00000000 0.08962214 0.1162476 0.1076244 0.09853293 0.1624354 0.08980265\n2 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.15554275\n3 0.78598070 0.50232050 0.4739717 0.0000000 0.30722902 0.0000000 0.77815937\n4 0.00000000 0.00000000 0.0000000 0.6499631 1.03997108 0.3456639 0.00000000\n5 0.00000000 0.00000000 0.0000000 0.2107167 0.00000000 0.0000000 0.00000000\n6 0.09724333 0.00000000 0.0000000 0.2818244 0.19706586 0.7353688 0.27679036\n         S55        S56       S58        S59         S5        S60        S61\n1 0.07722833 0.00000000 0.0000000 0.00000000 0.08436491 0.05852057 0.09072184\n2 0.00000000 0.35948681 0.2656845 0.00000000 0.00000000 0.00000000 0.06415003\n3 0.14788099 0.09607689 0.3510009 0.07800765 0.00000000 0.00000000 0.00000000\n4 0.16683226 0.13587324 0.0000000 0.00000000 0.00000000 1.19488153 0.57239218\n5 0.00000000 0.00000000 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.29652140 0.24485651 0.2425356 0.00000000 0.17896500 0.34818263 0.46770187\n        S62        S63        S64        S65        S66        S67        S68\n1 0.1528942 0.00000000 0.04252433 0.09911197 0.04756515 0.00000000 0.08119979\n2 0.0000000 0.31596888 0.17533229 0.36009167 0.30082842 0.06356417 0.09376145\n3 0.1441500 0.04045567 0.00000000 0.00000000 0.11651035 0.00000000 0.38923215\n4 0.0000000 0.12793206 0.22443986 0.00000000 0.33580828 0.07784989 0.25246042\n5 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.2573043 0.45294784 0.33911838 0.48952099 0.28824313 0.39950351 0.06629935\n         S69        S6        S72        S73       S74      S75       S76\n1 0.12149135 0.0412393 0.06765101 0.08753762 0.1849937 0.000000 0.1237179\n2 0.06074567 0.0000000 0.31001587 0.27681827 0.0000000 0.000000 0.0000000\n3 0.00000000 0.1797580 0.22941573 0.00000000 0.1807754 1.006459 0.6441024\n4 0.22146362 0.8184629 0.00000000 0.00000000 0.2061156 0.000000 0.0000000\n5 0.00000000 0.0000000 0.00000000 0.27681827 0.0000000 0.000000 0.0000000\n6 0.33253266 0.0000000 0.23310641 0.53134923 0.2188566 0.134840 0.0000000\n         S77        S78        S79         S7        S80       S81        S8\n1 0.31370944 0.11909827 0.24942152 0.04719292 0.16464639 0.0433555 0.0000000\n2 0.00000000 0.00000000 0.00000000 0.00000000 0.07761505 0.3386173 0.0000000\n3 0.19649437 0.56472318 0.08971226 0.23596459 0.52008893 0.0000000 0.7211103\n4 0.06213698 0.65223431 0.08971226 0.00000000 0.05488213 0.0000000 0.0000000\n5 0.19649437 0.00000000 0.00000000 0.00000000 0.05488213 0.0000000 0.0000000\n6 0.43896637 0.08421519 0.16353430 0.13348173 0.26499437 0.2410773 0.0000000\n          S9\n1 0.08596024\n2 0.00000000\n3 0.26193862\n4 0.00000000\n5 0.00000000\n6 0.24814583\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your resulting data set and determine if it is a tidy data set or not. Then describe how you would transform it into a tidy data set and explain why those changes make it fulfill all the conditions for it to be tidy.\nSpoiler alert: It’s not tidy … go ahead and create an object called tidy_counts that’s a tidy data frame.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (it’s probably clear why tidy data sets are also referred to as long data sets at this point …)\n\n\n# A tibble: 6 × 9\n  Kingdom  Phylum           Class        Order Family Genus Species ID     count\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0     \n2 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818\n3 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0     \n4 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958\n5 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238 \n6 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHm, if only our tidy_counts object also contained the sample meta-data currently in our dataframe soil. Go ahead and add that information to our tidy_counts data frame. Print the first few rows of your data frame to the console to make sure this worked.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (are we having fun yet?)… if you are having problems combining these data frames recall that you need one column in common. It’s easiest if those columns also share the same column name, but you can look up the function to see if there is a way around it if they don’t match up.\n\n\n# A tibble: 6 × 17\n  Kingdom  Phylum   Class Order Family Genus Species ID     count sampleID plot \n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n1 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0      M_FS_1/… M_FS…\n2 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818 M_FS_1/… M_FS…\n3 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0      M_FG_FS… M_FG…\n4 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958 M_FS_1/… M_FS…\n5 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238  M_AS_FS… M_AS…\n6 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487 M_FS_1/… M_FS…\n# ℹ 6 more variables: block &lt;dbl&gt;, forest &lt;chr&gt;, horizon &lt;chr&gt;, Carbon &lt;dbl&gt;,\n#   Nitrogen &lt;dbl&gt;, pH &lt;dbl&gt;"
  },
  {
    "objectID": "12_community-diversity.html#characterize-community-diversity",
    "href": "12_community-diversity.html#characterize-community-diversity",
    "title": "12  Characterizing community diversity",
    "section": "12.4 Characterize community diversity",
    "text": "12.4 Characterize community diversity\nOkay… now we’re ready to have some fun.\nFirst, let’s find out what taxonomic groups are present in the entire data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPrint a series of tables to the console and in your html document3 that comprise a single column each that show all the phyla, classes, orders, famiies, genera, and species in the data set in alphabetical order, respectively. Note that the values in those columns have a prefix indicate the taxonomic level. Get rid of that in your output.\n\n\n3 use the function kable() to print the entire data frame in a pretty table\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\n\n\n\n\nAscomycota\n\n\nBasidiomycota\n\n\nGlomeromycota\n\n\nMortierellomycota\n\n\nMucoromycota\n\n\nOlpidiomycota\n\n\nRozellomycota\n\n\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s compare patterns across different forest plot types. Create separate tables and print them to the console/have them print neatly in your rendered html files for easy comparison of the mean relative abundance for phylum, order, and family for each forest plot type. Print the first four digits only.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\nAS\nFG\nMixed\n\n\n\n\np__Ascomycota\n0.0248\n0.0236\n0.0254\n\n\np__Basidiomycota\n0.0143\n0.0139\n0.0132\n\n\np__Glomeromycota\n0.0121\n0.0000\n0.0000\n\n\np__Mortierellomycota\n0.0178\n0.0159\n0.0101\n\n\np__Mucoromycota\n0.0250\n0.0332\n0.0419\n\n\np__Olpidiomycota\n0.0000\n0.0000\n0.0044\n\n\np__Rozellomycota\n0.0041\n0.0113\n0.0042\n\n\nNA\n0.0381\n0.0257\n0.0276\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPick one forest plot type that you are interested in exploring further. Create tables that make it easy to compare the mean relative abundance for phylum, order, and family across the different soil horizons, and print those three table to the console and to your rendered html report.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nMeasures of diversity enable us to quantify the complexity of biological communities in a way that allows us to objectively compare communities across space and time. Measures of alpha diversity describe the diversity of a single sample and are based on the number of observed taxonomic groups or their relative abundance, beta diversity describes the variation between samples, gamma diversity describes the global diversity observed across a large number of communities.\nCommon alpha diversity statistics include:\n\nobserved richness the number of observed taxa.\nShannon Index (Shannon-Weaver or Shannon-Wiener Index) quantifies how difficult it is to predict the identity of a randomly sampled individual. Ranges from 0 (total certainty) to log(S) (total uncertainty).\nSimpson Index quantifies the probability that two randomly chosen individuals are the same taxonomic group (ranges from 0 to 1).\nInverse Simpson Index is defined as the number of species needed to have the same Simpson index value as the observed community assuming a theoretical community where all species are equally abundant.\n\nThe Shannon and Simpson Diversity are entropy-based indices that measure the disorder (diversity) of a system. In information theory entropy is used to describe the fact that we can quantify the degree of uncertainty associated with with predicted pieces of information. Applied to ecology this means that when describing diversity using these indices we are determining whether or not individuals randomly drawn from a community are the same or different species (or other taxonomic group).\nThe relationship between species richness and Shannon diversity is non-linear, i.e. at higher levels of species richness, communities appear more similar in terms of the magnitude of the index compared to lower levels of species richness - which is counter intuitive to the way species richness works. The solution to this is to linearize the indices. As a result, more recently, diversity indices have been proposed where diversity values are converted into equivalent (or effective numbers) of species(Jost 2006). The effective number of species is the number of species in a theoretical community where all species (taxonomic groups) are equally abundant that would produce the same observed value of diversity (a similar principle is applies in genetics for the concept of effective population sizes). While the definition of effective numbers is not as intuitive as the entropy-based ones the values that we yield are. Not only are the “units” species (instead of being a unitless index), but they have properties that we intuitively understand. For example, effective numbers obey the doubling principle: If you have two communities with equally abundant but totally distinct species and combine them, that new community would have a diversity that is 2x that of the original ones.\n\nJost, Lou. 2006. “Entropy and Diversity.” Oikos 113 (2): 363–75. https://doi.org/10.1111/j.2006.0030-1299.14714.x.\nThe package vegan has several functions implemented that allow us to calculate these diversity stats. Look up any functions you are not familiar with in the following code and comment it to describe what each line of code is doing.\n\ndiversity &lt;- tidy_counts %&gt;%\n  group_by(ID, forest, horizon) %&gt;%\n  summarize(richness = specnumber(count),\n            shannon = diversity(count, index = \"shannon\"),\n            simpson = diversity(count, index = \"simpson\"),\n            invsimpson = diversity(count, index = \"invsimpson\")) %&gt;%\n  pivot_longer(cols = c(richness, shannon, simpson, invsimpson),\n               names_to = \"metric\")\n\nWe can create a series of plots that compare the different diversity stats for each forest plot type and soil horizon location4.4 Well, I can but you will be able to soon, too\n\nggplot(diversity, aes(x = forest, y = value, color = forest)) +\n  geom_boxplot(aes(color = forest), outlier.shape = NA, fill = \"transparent\", size = 1) +\n  geom_point(aes(fill = forest),\n             position = position_jitterdodge(jitter.width = .3),\n             shape = 21, color = \"black\", size = 3) +\n  facet_grid(metric ~ horizon, scales = \"free\") +\n  labs(x = \" \", y = \" \")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html",
    "href": "11_bioinformatics.html",
    "title": "11  Bioinformatics",
    "section": "",
    "text": "12 Infer amplicon sequence variants (ASVs) and create sequence table\nRunning this step on all samples at the same time is computationally more intensive than running the pipeline one sample at a time but increases the functions ability to resolve low-abundance ASV, including singletons."
  },
  {
    "objectID": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "href": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "title": "11  Bioinformatics",
    "section": "11.1 Next-generation sequence data enable metabarcoding to describe community composition",
    "text": "11.1 Next-generation sequence data enable metabarcoding to describe community composition\nHigh-throughput sequencing data, often referred to as next-generation sequencing (NGS) data, is a type of biological data generated through advanced sequencing technologies that allow for the rapid and simultaneous sequencing of a large number of DNA or RNA molecules. These technologies have revolutionized genomics, transcriptomics, and other related fields by enabling the efficient and cost-effective analysis of genetic material on a massive scale. There are various high-throughput sequencing platforms available, including Illumina, Ion Torrent, Pacific Biosciences (PacBio), Oxford Nanopore Technologies (ONT), and others. High-throughput sequencing has become more cost-effective over time, making it accessible to a broader range of researchers and enabling large-scale genomics projects.\nThe key difference between high-throughput sequencing platforms compared to traditional Sanger sequencing3 is that they are capable of simultaneously sequencing thousands to millions of DNA fragments or RNA molecules in a single sequencing run. This parallel processing greatly increases the speed and efficiency of sequencing compared to traditional Sanger sequencing.3 anger sequencing, also known as dideoxy sequencing or chain-termination sequencing, is a traditional DNA sequencing method that was developed by Frederick Sanger and his colleagues in the late 1970s. It was the primary method for DNA sequencing for several decades before the advent of high-throughput next-generation sequencing (NGS) technologies. Sanger sequencing is still used today for specific applications, such as sequencing individual genes or validating NGS results.\nNGS platforms produce a vast amount of data. A single sequencing run can generate gigabytes to terabytes of raw sequencing data, depending on the instrument’s capacity and the type of experiment being conducted4. Typically, high-throughput sequencing generates short sequences, referred to as “reads.” The length of these reads can vary depending on the sequencing platform but is typically in the range of 50 to 300 base pairs. Usually you get one forward and one reverse read per sequenced template DNA.4  Don’t worry, we are going to use a data set that consists of samples have been modified to reduce size but still preserve realistic sequence variation and errors to create small example data set that your laptops can handle.\nIn the context of metabarcoding of environmental DNA this means that we can use universal primers to amplify template DNA present in the environmental sample that potentially are from different organisms5 because we can sequence all the amplified template DNA strands at once.5 If you used Sanger sequencing you would just get a bunch of noise, because the sequencer would not be able to make a distinct base call for any position"
  },
  {
    "objectID": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "href": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "title": "11  Bioinformatics",
    "section": "11.2 Bioinformatics pipelines are important tools to processing NGS data",
    "text": "11.2 Bioinformatics pipelines are important tools to processing NGS data\nAnalyzing high-throughput sequencing data requires sophisticated bioinformatics tools and pipelines. Bioinformatics pipelines are widely used in genomics and related fields to streamline and standardize data analysis workflows. They comprise a series of structured and automated series of computational and data analysis steps designed to process and analyze biological data. Frequently, data takes on of several paths through the pipeline where the output from one step becomes the input for the next step.\nGenerally, a bioinformatics pipeline takes a specific type(s) or raw input data, such as DNA or RNA sequence data, frequently generated from high-throughput techniques. This data set then goes through the first stage of the pipeline where it is pre-processed for analysis steps, including quality control, data cleaning, and format conversion to ensure the data set is robust and meets a standardized format for downstream analysis. Pipelines frequently consists of a series of analysis steps or modules tailored to specific research questions/tasks, for example sequence alignment, variant calling, gene expression quantification, protein structure prediction.\nBecause pipelines are designed to allow for a high degree of automation to reduce error and maximize efficiency. Frequently, users use scripts or specific workflow management tools to execute each analysis step in a predefined errors. Because of this automation, it can be tempting to “blackbox” the analysis where an inexperienced user can run complex data analysis without understanding what is happening at each stage. However, for an analysis appropriate to a specific data set it is critical that scientists understand how to configure parameters and options for each analysis step to customize the pipeline for the specific data set and research objectives.\nBioinformatics tools are often run from the command line. Linux is frequently the operating system of choice6 as it provides a powerful and standardized command-line interface that makes it easy to automate tasks, write scripts, and integrate various tools into analysis pipelines. It also offers a high degree of customization and flexibility due to its open source nature7. In addition, it is known for its efficiency and stability which is critical when running resource-intensive analysis.6 You’re laptop is likely MacOS or Windows and Linux would be the third common OS, however, it is not designed to be as user-friendly as Mac/Windows and has a steep learning curve.7 This also means its free!\nHigh performance clusters allow for efficient handling of large data set by allowing multiple tasks to be executed in parallel8.8 Your laptop probably has two to four cores, which means you could run tasks in parallel on up to two threads. Many linux servers have upward of 20 threads to run things in parallel on, for high performance clusters (HPCs you may be looking at 100s)\nDon’t worry, you’re not going to have to learn how to use a Linux Terminal. We will be using a pipeline that runs in R.\nDada2 (Callahan et al. 2016) is a bioinformatics pipeline which implements functions that can be used for the quality control, denoising, and analysis of amplicon sequencing data, particularly data generated from high-throughput sequencing technologies like Illumina’s MiSeq or HiSeq platforms. It is widely used in microbiome research and is becoming increasingly popular for the analysis of environmental DNA (eDNA) metabarcoding data sets.\nThe key modules (steps) include\n\nQuality Filtering: DADA2 begins by assessing the quality of each sequence read in the dataset. Low-quality reads, which may contain sequencing errors or noise, are filtered out. This step helps improve the accuracy of downstream analysis.\nSequence Dereplication: Identical sequences are collapsed into unique sequence variants (also known as amplicon sequence variants or ASVs). This step reduces redundancy in the dataset and accounts for potential PCR or sequencing errors.\nDenoising: DADA2 employs a statistical model to distinguish between true biological sequence variants and sequencing errors. By modeling the error profile of the sequencing data, it can identify and correct errors, resulting in more accurate sequence variants.\nChimera Removal: DADA2 identifies and removes chimera sequences, which are artificial sequences generated during PCR that can lead to incorrect biological interpretations.\nPhylogenetic Assignment: After denoising, DADA2 assigns taxonomy to the sequence variants by comparing them to reference databases, allowing researchers to identify the taxa present in the sample.\n\nIn contrast to other pipelines used to process metabarcoding data sets, DADA2 generates ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nExplain what the difference between an OTU and and ASV (include what each abbreviation stands for). You will want to revisit this question after having completed this tutorial and having a better understanding of the methods implemented in DADA2.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nEnough chit chat! Let’s go!"
  },
  {
    "objectID": "11_bioinformatics.html#demultiplexing",
    "href": "11_bioinformatics.html#demultiplexing",
    "title": "11  Bioinformatics",
    "section": "11.3 Demultiplexing",
    "text": "11.3 Demultiplexing\nGenerally, multiple samples are pooled and run on the same NGS sequencing lane. The amplified sequences (amplicons) of each sample are tagged with the same unique molecular barcode or index. This means that the first step is to take the entire output of a sequencing run and demultiplex it, i.e. each sequence read is assigned back to its respective sample based on the barcode/index combination.\n\n\nFASTQ files are a common file format used to store biological sequence data, particularly data generated by next-generation sequencing (NGS) technologies like Illumina sequencing platforms. In contrast to FASTA files which contain only sequence data, FASTQ files contain information about the sequences of DNA or RNA fragments, associated quality scores, and optional sequence identifiers.\nThis data set has already been demultiplexed and the individual fastq files are in your data folder."
  },
  {
    "objectID": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "href": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "title": "11  Bioinformatics",
    "section": "11.4 Quality check and filter sequences",
    "text": "11.4 Quality check and filter sequences\nIf you look in the data folder your project directory, you will see that you have a folder called seq that contains a folder for your raw sequences and also one that we will use to hold your sequences once we have done some quality filtering and trimming.\nBefore we get started we are going to create a series of vectors that contain our file paths and sample names that we can use throughout the analysis9. We are also going to extract the sample names and save them in a variable. We are able to do this because all of our filenames have a similar pattern.9 There are some key advantages to setting up vectors in this way. First, you only need to type in the information once, and then you can just call the vector which is a lot shorter and easier to type in. Second, you make the code more easily reusable for another analysis because you only need to change the file paths in one location and don’t have to find all the spots in your code where the file path needs to be changed. Finally, as you will see you are working with a large number of individual files, so when we create sample names based on the file names we don’t have to type them in all by hand which saves time and also minimizes the chances of typos.\n\n# designate file paths ----\n\n# create variable with initially trimmed reads\nraw &lt;- \"data/seq/raw\"\n\n# path for filtered reads\nfilt &lt;- \"data/seq/filt\"\n\n\n# create lists of matched forward & reverse fastq files ----\n\n# forward reads\nfnFs &lt;- sort(list.files(raw, pattern = \"_R1.fastq\", full.names = TRUE))\n\n# reverse reads\nfnRs &lt;- sort(list.files(raw, pattern = \"_R2.fastq\", full.names = TRUE))\n\n# create vector with sample names\nsample.names &lt;- substr(basename(fnFs), 1, (nchar(fnFs)-25))\n\n\n# create file names & paths for filtered reads ----\n\n# named vectors of forward reads \nfiltFs &lt;- file.path(filt, glue(\"{sample.names}_R1.fastq.gz\"))\nnames(filtFs) &lt;- sample.names\n\n# named vectors of reverse reads \nfiltRs &lt;- file.path(filt, glue(\"{sample.names}_R2.fastq.gz\"))\nnames(filtRs) &lt;- sample.names\n\nThere are 74 samples in the data set.\nNow that we have all of our file paths set up our next step is to take a look at the quality profile of the sequences. We could look at all of them, however, usually it is sufficient to spot check a few sequences to get an idea of what the quality looks like to give us an idea of what threshold values we should be using for quality filtering.\nDADA2 has a built in function plotQualityProfile() that pulls the information from the fastq files which contains the quality information for each nucleotide call. Quality is measured as PHRED scores, a score of 40 should be interpreted as an expected 1 in 10,000 error rate, 20 would be a 1 in 100 chance of a base call being incorrect.\n\n\nWe are wrapping the plotting function in ggplotly() which is built on the plotly package to generate interactive figures. It will take a second for your figure to pup up in the Viewer pane on the bottom left of Rstudio. If you hover over different points of the plot with your mouse you will see a little pop that lists the exact values.\nLet’s go ahead an plot the quality sequence for the first sample We can do this by passing the first file path in the fnFs vector using [1] to indicate the first element of the vector.\n\nggplotly(plotQualityProfile(fnFs[1]))\n\n\n\n\n\n\nFigure 11.1: Quality scores for the forward reads of the first sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is em quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nAs you can see there is a lot going on in this figure. The x-axis indicates the cycle number which is equivalent to the base position in the sequence (one base call is added per cycle) while the y-axis indicates the quality score. In the bottom left you can see the number of reads (sequences) in the file indicated. You can see grey shading which is a heatmap indicating the frequency of each score at each position. Darker colors indicate that a certain quality score is more common at that position across all the reads in the sample. The green line is the mean quality score, orange lines are the quartiles. The read lines indicates the proportion of reads that extend to that position.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nGo ahead and plot the reverse reads for the 12th sample in the data set. Then describe the quality patterns you are seeing for your forward and reverse reads. You can use the table below that summarizes how to interpret the quality scores.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplotly(plotQualityProfile(fnRs[12]))\n\n\n\n\n\n\nFigure 11.2: Quality scores for the reverse reads in 12th sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nReverse reads typically have lower quality and larger drop-off compared to forward reads. Trimming too conservatively can result in downstream issues when forward and reverse reads cannot be merged due to a lack of overlap.\n\n\n\n\n\n\n\n\n\n\nQuality score of base call\nConfidence of base call being correct\n\n\n\n\n10\n90\n\n\n20\n99\n\n\n30\n99.9\n\n\n40\n99.99"
  },
  {
    "objectID": "11_bioinformatics.html#trim-sequences",
    "href": "11_bioinformatics.html#trim-sequences",
    "title": "11  Bioinformatics",
    "section": "11.5 Trim sequences",
    "text": "11.5 Trim sequences\nWe will use the information from the quality scores to make decision on appropriate threshold values to use to trim our sequences.\nWe are going to use a series of filters to remove low quality reads from the samples. Specifically, we will remove any basecalls that were too ambiguous to call,10 and any remaining PhiX reads still in the data set11. Next, the first and last xx bases are trimmed for each read as these are usually low quality. Additionally, reads are truncated at first instance of a quality score &lt; 6. Finally, reads &lt; 35 bp after trimming are removed.10 N are unknown nucleotides. If the signal for a base is too ambiguous to make a call, the Illumina platform will call it N. DADA2 assumes there are no NNN in the data set so we have to remove them.11 PhiX is a bacteriophage. It’s DNA is spiked into libraries being sequenced to improve the quality sequencing run by increasing the sequence diversity\nWe are assuming matching order of forward and reverse reads for each sample.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the filterAndTrim() function and annotate each line of the code to indicate what each argument does, include both what the argument controls in general and then specifically what this means for this example.\n\n\n\n# filter reads\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, #\n                     truncLen = c(280, 280),     #\n                     trimLeft = c(18, 2),        #\n                     truncQ = 6,                 #\n                     maxEE = c(2,2),             #\n                     minLen = 35,                #\n                     rm.phix = TRUE,             #\n                     matchIDs = FALSE,           #\n                     compress = TRUE)            #\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nHere are descriptions of the key arguments that set thresholds for quality filters\n\ntruncQ: sets a minimum Q score. At the first instance of a quality score less than or equal to truncQ, the sequence is truncated.\ntruncLen: sets the length at which the sequences will be truncated. Sequences shorter than the length are eliminated.\ntrimLeft: sets the length that will be removed on the 5’ side of the reads. This allows you to remove the primers if it has not been done beforehand12.\nmaxEE: sets the maximum number of “expected errors” allowed in a read. This filter is based on the Q index. The more the number is increased, the less strict we are.\n\n12 Primers used are ITS3_KYO2: GATGAAGAACGYAGYRAA = 18bp ITS4: TCCTCCGCTTATTGATATGC = 20b\n\n\nLet’s take a look at what that function has done. We assigned the output to a variable called out.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse the functions class() and head() to get an idea of what type of object we have created and then briefly describe what information is contained in that object.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWait - that function has created a matrix array (a type of object you could describe as a multidimensional vector or a poor man’s data frame) that tells us how many reads per sample went into a the filters and then who many came out.\nBut where did our filtered reads go? Remember, we gave the function the file paths for both the input files (data/seq/raw/*.fastq.gz) and where to write the filtered files and what to name them (data/seq/fil/*.fastq.gz). If you use the file navigation pane you should see that it now contains those files.\nThe object we created that is now in our environment holds the record of what occurred during filtering. It is very common that bioinformatics pipelines generate output files at various steps that let the user track what is happening that can be used to ensure everything is going as expected and also allow them to pick up on unusual patterns that might be indicative of the fact that your threshold values might not be appropriate, that there is something odd going on with the data, or that perhaps commands where not properly formulated and therefore didn’t take place the way the researcher expected them to.\n\n\n\nLet’s take a look to see how many reads where removed from the data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThis code chunk converts the matrix into a dataframe. Use your coding skills to create a new column called perc_lost that contains the percent of reads that were lost in this filtering step. Then calculate the mean and standard deviation of the percent reads lost and the mean and standard deviation of the number of reads that remain per sample.\nThen use kable() to display your function in the console/your rendered html report.\nNote the inline code below that is pulling the key pieces of information directly from your data frame. This can be really helpful because e.g. if you are rerunning code on an updated data set you would not have to dig through your output to make sure that you updated all the results, instead R can do that for you. You can indicate that it is inline code using backticks and then indicating the engine using r. You will see that notation through out this document.\nThe function pull() allows you to extract (pull) a single value from a column.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n# summarize proportion of reads lost during trimming\ncheck &lt;- as.data.frame(out) %&gt;%\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nmean_loss\nstd_loss\nmean_reads\nstd_reads\n\n\n\n\n49.05541\n8.624355\n509.4459\n86.24355\n\n\n\nTable 11.1: Mean and standard deviation of the proportion of reads lost due to quality filtering and the mean and standard deviation of the number of remaining reads per sample post filtering.\n\n\n\n\n\nAfter quality trimming samples contain approx. 509% (+/- 86) reads. Quality trimming removed 49.1% (+/- 8.6%) reads from each sample.\nWe will track how many reads we “lose” at each step to understand how different steps affect the number of reads that remain in the sample.\nNext to knowing how many low quality reads where removed we will want to take a look at the quality of the remaining reads after we have trimmed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPlot the quality scores for the forward and reverse of one random sample and compare your results to the raw data. Make sure to add a figure caption using the code chunk options!\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what the plot for your forward reads should look like.\n\n\n\n\n\nFigure 11.3: Quality scores of forward reads for 16 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nRemember to plot the reverse reads for the same sample.\n\n\n\n\n\nFigure 11.4: Quality scores of reverse reads for 12 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position."
  },
  {
    "objectID": "11_bioinformatics.html#generate-error-model",
    "href": "11_bioinformatics.html#generate-error-model",
    "title": "11  Bioinformatics",
    "section": "11.6 Generate error model",
    "text": "11.6 Generate error model\nNext, we need to be able to distinguish between error due to for example PCR or sequencing error and actual biological differences among sequences. We can train DADA2 to be able to do this using a subset of our data as a training set using a machine learning approach to establish a parametric error model13 to estimate error rates.13 Note that the error rate is specific to a sequencing run\nError rates are generally expected to drop with increased quality. By default 100 Million bases are used to generate the error model, this number can be increased for a better estimate.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the code below to generate the error models. This will take a few seconds. While you wait, pull up the description of what this function does in the help pane and use comments to indicate what the function as a whole does and what the different arguments control.\n\n# forward reads\nerrF &lt;- learnErrors(filtFs,               # \n                    nbases = 1e8,         # \n                    multithread = FALSE)  # \n\n9877138 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n# reverse reads\nerrR &lt;- learnErrors(filtRs, \n                    nbases = 1e8,\n                    multithread = FALSE)\n\n10480322 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n\n\n\nMachine learning estimates and observed values should show close overlap to indicate the quality (fit) of the model. The observed error rates should be consistent with the expected learned error rates for the 16 possible base transitions. If they do not, the number of bases used can be increased to allow the ML algorithm to train on a larger subset of the data.\nWe can use the function dada2::plotErrors() to compare the observed and estimated error plots as an indication of how good our error models are. We will plot both the forward and reverse error model and assign those to objects so we can use the patchwork package to plot them next to each other in a single plot14.14 the package patchwork allows us to combine multiple plots in one file. You can read up on the basic layout options using +, \\ and | here and more a sneak peak of more complex layouts here\n\n# Visualize estimated error\np1 &lt;- plotErrors(errF, nominalQ = TRUE)\n\np2 &lt;- plotErrors(errR, nominalQ = TRUE)\n\np1 / p2\n\n\n\n\nFigure 11.5: Error rates for each possible transition for forward (top) and reverse reads (bottom). Observed (grey points) and estimated (black line) error rates for each consensus quality score. Expected error rates for nominal definition of the quality score are in red.\n\n\n\n\n\n\nThe plot will pop up in the Plot pane. You can change the size of the pane to resize and make it larger, it can also be helpful to use the zoom button to create a popout window for an even better look.\nNote also that you can use the code chunk option fig-height: to control the size in your rendered html output file.\nAs you can see this creates a series of plots - one for each possible transition e.g. the error frequency for an A being mistakenly called a C (A2C) etc. The black points are the observed error rates for each consensus quality scores. The black line is the estimated error rate after the model has converged. The red line indicates the expected error rates under the nominal definition of the Q-value for Illumina technology.\n\n\n\n\n\n\n Consider this\n\n\n\nUse the figures to briefly describe how well our error models capture the observed data and how this compares to the expected error rates.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "href": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "title": "11  Bioinformatics",
    "section": "12.1 Dereplicate, infer, and merge ASVs",
    "text": "12.1 Dereplicate, infer, and merge ASVs\nNow we are ready to identify the ASVs present in our data set.\nThe first step to do this is dereplication, which means that we are collapsing all identical reads into a set of unique sequences. Dereplicating or denoising data is an important step in amplicon processing workflows, instead of keeping all identical sequences only one is kept for processing and the number of sequences represented is stored a long with it. A new consensus quality score profile is calculated for each unique sequence based on the average quality score of each base for all sequences that are replicates of that given amplicon. These quality profiles Dereplication makes downstream processing a lot more efficient and less memory intense by eliminating redundant comparisons.\n\n\nThe function lapply() is similar to a for loop, where it applies the same function to every file in the vector containing the filtered samples.\n\nderepFs &lt;- lapply(filtFs,      \n                  derepFastq,      \n                  verbose = FALSE)  \n\nderepRs &lt;- lapply(filtRs, \n                  derepFastq, \n                  verbose = FALSE)\n\nIn the next step, sequence variants are inferred using the dereplicated data and the inferred error rates. Using the consensus quality profiles significantly increases DADA2’s accuracy.\n\n\nAs we’ve mentioned previously, many metabarcoding pipelines use OTUs instead of ASVs. For OTU’s reads that are at least 97% similar are clustered. There are two arguments for doing this, the first is many species have more than one haplotype for the same locus and so if you set the threshold for clustering at 100% you are oversplitting i.e. multiple unique sequences can “belong” to the same species. The second is that sequences from the same species might end up with different sequences not because of true biological differences but because of errors in the process of PCR amplifying sequences or during sequencing. However, ASVs are not the same thing as OTUs with a 100% threshold for clustering, instead, DADA2 uses the information from the quality profiles and the error models to distinguish true biological differences that separate unique amplicons vs amplicons that are only different due to errors (i.e. base differences are artifacts).\n\ndadaFs &lt;- dada(derepFs,            \n               err = errF,         \n               selfConsist = FALSE,\n               multithread = TRUE) \n\ndadaRs &lt;- dada(derepRs,\n               err = errR,\n               selfConsist = FALSE,\n               multithread = TRUE)\n\n\n\nDifferent sequence platforms and sequencing kits are limited by how long the reads can be. For example, this data set was sequenced on a 2x300bp Miseq platform. This means that each amplicon was sequenced 300 bp in the 5’ to 3’ direction and then 300bp in the 3’ to 5’ direction. This means that we can sequence inserts longer than 300 bp and merging the two strands (forward and reverse reads) increase the confidence in the reliability of the sequence in the overlapped region.\nOur last step is that we need to merge our forward and reverse reads to reconstruct the full target amplicon. Forward and reverse-complement of corresponding reverse sequences are aligned and merging requires an overlap of 12 sequences and there are no mismatches permitted in the overlapping region. Paired reads that do not exactly overlap are removed to reduce spurious output.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the mergePairs() function in the help pane and comment the code below to indicate what the function as a whole and each argument does.\n\nmergeASVs &lt;- mergePairs(dadaFs, filtFs, \n                        dadaRs, filtRs,\n                        minOverlap = 12,  \n                        maxMismatch = 0)\n\n\n\nWe can take a look at the results. The function returns a list16. Let’s take a look at the first element.16 Remember, we can think of a list as an object that is like a shelf where each shelf holds one element in this case each sample is an element\n\nhead(mergeASVs[1])\n\n$S1\n                                                                                                                                                                                                                                                                                                                                                                                                            sequence\n1             ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCCTTGGTATTCCGAGGGGCACACCCGTTTGAGTGTCGTGAATACTCTCAACCTTCTTGGTTTCTTTGACCACGAAGGCTTGGACTTTGGAGGTTTTTCTTGCTGGCCTCTTTAGAAGCCAGCTCCTCCTAAATGAATGGGTGGGGTCCGCTTTGCTGATCCTCGACGTGATAAGCATCTCTTCTACGTCTCAGTGTCAGCTCGGAACCCCGCTTTCCAACCGTCTTTGGACAAAGACAATGTTCGAGTTGCGACTCGACCTTACAAACCTTGACCTCAAATCGGGTGAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n2                                                                                             ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATCCCGAGGGGCATGCCTGTTCGAGCGTCATTTCACCACTCAAGCCTGGCTTGGTGTTGGGCGACGTCCCCTTTTGGGGACGCGTCTCGAAACGCTCGGCGGCGTGGCACCGGCTTTAAGCGTAGCAGAATCTTTCGCTTTGAAAGTCGGGGCCCCGTCTGCCGGAAGACCTACTCGCAAGGTTGACCTCGGATCAGGCAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n3                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTATAACCACTCAAGCCCCGGCTTGGTCTTGGGGTTCGCGGTCCGCGGCCCTTAAACTCAGTGGCGGTGCCGTCTGGCTCTAAGCGCAGTAATTCTCTCGCTATAGTGTCTAGGTGGTTGCTTGCCATAATCCCCCAATTTTTTACGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n4                                          ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n5          ATGCGATACGTAATGTGAATTGCAGAATTCCGTGAATCATTGAATCTTTGAACGCATCTTGCGCCTCTTGGTATTCCGAGGGGCATGCCTGTTTGAGTGTCATTAGAACTATCAAAAAAATAGATGATTTCAATCGTTAATTTTTTTGGAATTGGAGGTGGTGCTGGTCTTTTTCCATTAATGGCCCAAGCTCCTCCGAAATGCATTAGCGAATGCAGTGCACTTTTTCTCCTTGCTTTTTCTGGGCATTGATAGTTTACTCTCATGCCCTAAGCTGGTAGGGAGGAAGTCACAGAATGCTTCCCGCTCCTGAATGTAATACAAAACTTGACGATCAAACCCCTCAAATCAGGCAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n7                                                                                 ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATGATCAACCATCAAGCCTGGCTTGTCGTTGGACCCTGTTGTCTCTGGGCGACAGGTCCGAAAGATAATGACGGTGTCATGGCAACCCCGAATGCAACGAGCTTTTTTATAGGCACGCATTTAGTGGTTGGCAAGGCCCCCTCGTGCGTTATTATTTTCTTACGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n8                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTGCAACCCTCAAGCATTGCTTGGTATTGGGCTCCGCTGCTCACCCAGCGGGCCTTAAAATCAGTGGCGGTGCCGTCGAGGCCCTGAGCGTAGTAAATATCCTCGCTATAGGGACTCGGTGGACGCTGGCCATTAACCCCCAACTTTCTAAGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n9                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCCTCTGGTATTCCAGGGGGTATGCCTGTTCGAGCGTCATTACAACCCTCAAGCACTGCTTGGTATTGGATGTCAACCATTGGTGGTGCATCTCAAAAGTATTGGCAGTAGCATTTAGCTTCTAGTGTAGTAAATTTCTCGCTTTGGAGTCAAGTGTCTAATTGCTAGATAGAACCCCTAATTTATCAAAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n10                                                                                                   ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCGCTCGCGGCCGGCCCTAAAGACAGTGGCGGCAGCGTCTGGCTCCAAGCGTAGTACAATCCTCGCTCTGGTGCTAGGCGGTGGCCTGCCAGAACCCCCCTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n11                                                                                               ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCTTCTGGTATTCTGGGAGGTATGCCTGTTCGAGCGTCATTGCAACCATCAAGCCTAGGCTTGGTATTGGATGCCACCGCTTGGTGCATTTCAAAATTAGTGGCGGTGCCATTCAGCTTCAAGCGTAGTAAATTTCTCGCTCCTGGAGTTTGTATGTTGTCTGCTAGAACCCCCTAATTTATCAAGGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n12                                                                                                                       GCGCGATAGGTATTGTGAATTGCAGAATTGTGAATCATCGAATTTTTGAACGCACATTGCACCCATTGGTATTCCGATGGGTATACTTGTTTGAGCGTCATTTCATTCTCCTTTTGGGTTTTGGCATGAATATTTCTTGCTGAATTATAATGGTGTGGCTACCAGACTACAACGTGATAGATATTTCGTTGGATGTGACTGGGATTGCTCACCTTAAAAACATTGTATAGACCTCAAATCAAGCAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n13 ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCGCTCTTTGGTATTCCGAAGAGCATGCTTGTTTGAGTATCAGTAAACACCTCAACCTCCTCTTGTTTTTTCAAAAGGAGGGTGGACTTGAGCTATCCCAACAACCTTCACCGGTAGGCGGGCGGCTTGAAATGCAGGTGCAGCTGGACTTTTATCTGAGCTAAAAGCATATCTATTTAGTCCTCGTCAAACAGGATTATTACTATTGCTGCAGCTAACATAAAGGATAATTGTCCTCATTGCTGACTGATGCAGGATTTTACGACACTTTATGTGTTGTTCAACTCGATCTCAAATCAAGTAAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n14                                                                             ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCAACCATCAAGCCTGGCTTGTCGTTGGACCTCTTTGCCAATGAAATATGTGGCAGGTCCGAAAGATAATGACGGCGTCGTGTTTGACCCTAGATGCAACGAGCTTTTTATAGCACGCATTGATGTGGTCGGGCGACCCAGTCTTTAACCATTATTTTCTAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n15                                                                                 ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCTCCTGGTATTCCGGGAGGCATGCCTGTTCGAGCGTCATTAAAGACCACTCAAGCGATTTTGCTTGGTATTGGAAGAAGAGTGCCTCTGGCCCTCCCTTCCGAAATCCAATGGCGGAAAGTCTCACGTGCCCCGGCGTAGTAAGTTTATCTTTCGCTTGGACCCTGAGGCGTTCTCGCCCTCAAATCCCCAATACTATAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n16                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACCCTCTGGTCCATTCCAGAGGGTATGCCTGTTTGAGTGTCATTAATGTATCAAACCACCAAGCTTGCTTGGTTGGTCTTGGATGTTGAGGGTTGCTGGGGTTATAATGATCAGCTCCCTTTAAATGCATTAGCTTGGAATGTATAAGCCATTTTAGCTTAGGCTGATATGAATACAGCGTATTAAATGCTTTTGCTAAAGTGTAGCTTGTCTGGGCTTATAACTGTCTCTAGCTGAGACTGTCTTTTGACATTGTTAAATCATGATCATGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n17                                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCACCCTTTGGTATTCCGAAGGGTATGCCTGTTTGAGTGTCATTAAATTCTCAACTTCAAATTGAATTTTGAAGCTTGGACTTTGGAGGTTTGCTGGTGTCACTATCGGCTCCTCTTAAATTCATTAGCGGAACTGTAAGGACCGGCTTTGGTTTGATAGCTAACATTATCTATGCCGTTGCTGTGACCTTTGTGTTTGGCTTCTAATGGTCATTTTGTTGACTGTCTCTGCTTTGAGGCATACACTTTTAAGCTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n18                                                                                                    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCGGTGGTATTCCGCCGGGCATGCCTATTCGAGCGTCATTACAACCCTCACGCCCCGCGTGGTCTTGGGCCGAGCCCCCCGGGCTGGCCTCAAAAGCAGTGGCGGTGCCTCTGGGTCCTGAGCGTAGTAACACTTCCGCTACAGGGCTCCCGAGCGTGCTGGCCGAACCCCAACCCTTCAGGTTGACCTCGGATTAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n19                                                                                  ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCACAGTATCAAGCTTGGCTTGTCGTTGGGCCCTTTGTCACCTGGTGACAGGTCCCAAAGAGAATGACTGGTGTCGTAAAGACTCTAAATGCAACGAGCTTATAACAGCACGCATCTAGTAGTAATATGGCCCGGTTCTCACCTCTTTATTTCTCAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n21                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTACAACCCTCAAGCAATGCTTGGTGTTGGGCCGCGCCGCTAACCCGGCGGGCCCTAAAACCAGTGGCGGTGCCGTCGGGCTCTGAGCGTAGTAATTCTTCTCGCTATAGAGCCCCGGCGGATGCTAGCCAGCAACCCCCAATTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n22     ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCTCTTTGGTATTCCGAAGAGCATGCCTGTTTGAGTGTCATGAAAATATCAACCTTGACTTGGGTTTAGTGCTCTTGTCTTGGCTTGGATTTGGCTGTTTGCCGCTCGAAAGAGTCGGCTCAGCTTAAAAGTATTAGCTGGATCTGTCTTTGAGACTTGGTTTGACTTGGCGTAATAAGTTATTTCGCTGAGGACAATCTTCGGATTGGCCGAGTTTCTGGGACGTTTGTCCGCTTTCTAATACAAGTTCTAGCTTGCTAGACATGACTTTTTTATTATCTGGCCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n23                                                                                                    ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCCCGTGGCCGGCCCCAAAGTCAGTGGCGGTGCCGTCCGGCTCTAAGCGTAGTACATCTCTCGCTCTAGGGTCCCGCGGTGGCCTGCCAGAACCCCAACTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n24                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTAACCACTCAAGCCTAGCTTGGTATTGGGGCACGCGGTCTCGCGGCCCTTAAAATCAGTGGCGGCGCCGGTGGGCTCTAAGCGTAGTACATACTCCCGCTATAGAGTTCCCTCGGTGGCTCGCCAGAACCCCTAATTTTTACAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n25                                           ATGCGATACGTAATGTGAATTGCAGATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACTCTTTGGTATTCCGAAGAGTATGCCTGTTTCAGTATCATGAAAAACCTCACAAATTCAATTTTGGCTTTGTGGACTTGAGCATTTTGCGGCTTTGTTGCTGCTGGCTTAAAATATATTTCTTGGATAGCATATTATGGCTTTCGAAACTCGGCTTAATAGTTTTGGCTTTTGGTCAAATCTTTAGCTCTTTTCAAAGTCTTCAAGTTATTCAAAAGTTTTATACGAACACTTTCTCAATTTTGATCTGAAATCAGGTAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n26                                                                                               CTGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGGGGGGCATGCCTGTTCGAGCGTCACTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCTGCCGGCGACGGCAGGCCTTAAAACCAGTGGCGGCGCCGCTGGGCCCTGAGCGTAGTAATACTCCTCGCTACTGGGCCCCAGCGGATGCCTGCCAGCAAACCCAACTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n   abundance forward reverse nmatch nmismatch nindel prefer accept\n1        172       1       1    150         0      0      2   TRUE\n2         47       2       4    230         0      0      1   TRUE\n3         43       3       3    234         0      0      2   TRUE\n4         34       5       5    179         0      0      2   TRUE\n5         32       6       2    147         0      0      2   TRUE\n7         21       4       6    218         0      0      1   TRUE\n8         17       8       9    229         0      0      2   TRUE\n9         17       9       7    229         0      0      2   TRUE\n10        13       7      10    237         0      0      1   TRUE\n11        12      20      21    233         0      0      1   TRUE\n12        12      10       8    257         0      0      2   TRUE\n13        10      11      12    139         0      0      1   TRUE\n14         9      19      20    215         0      0      1   TRUE\n15         9      15      13    219         0      0      2   TRUE\n16         6      17      22    154         0      0      1   TRUE\n17         6      16      23    170         0      0      1   TRUE\n18         6      13      16    238         0      0      1   TRUE\n19         5      23      27    220         0      0      1   TRUE\n21         4      26      25    230         0      0      2   TRUE\n22         4      14      11    143         0      0      1   TRUE\n23         4      22      26    238         0      0      1   TRUE\n24         3      21      24    235         0      0      1   TRUE\n25         3      18      15    181         0      0      2   TRUE\n26         2      25      30    233         0      0      1   TRUE\n\n\nThis contains a dataframe where for each sequence (ASV) in that sample we get information on what the sequence looks like, the number of reads corresponding to this forward/reverse combination (abundance) etc.\nFor example we can query the largest and smallest overlap like this.\n\n# Largest overlap \nmax(mergeASVs[[1]]$nmatch) \n\n[1] 257\n\n# Smallest overlap\nmin(mergeASVs[[1]]$nmatch) \n\n[1] 139"
  },
  {
    "objectID": "11_bioinformatics.html#create-a-sequence-table",
    "href": "11_bioinformatics.html#create-a-sequence-table",
    "title": "11  Bioinformatics",
    "section": "12.2 Create a sequence table",
    "text": "12.2 Create a sequence table\nNow we have a fully denoised set of sequences that can be used to generate a sequence table for further analysis that contains the merged sequence, abundance and indices of forward and reverse sequence variants that were merged.\n\nseqtab &lt;- makeSequenceTable(mergeASVs)\n\nLet’s take a quick look at what this data set looks like.\n\ndim(seqtab)\n\n[1]  74 701\n\nseqtab[,1]\n\n S1 S10 S11 S12 S13 S14 S15 S16 S17 S18  S2 S20 S21 S22 S23 S24 S25 S26 S27 S28 \n  0 218   0   0 124   0  48   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS29  S3 S30 S31 S33 S34 S35 S36 S37 S39  S4 S41 S42 S43 S44 S45 S46 S47 S48 S49 \n  0   0   0   0   0   0  19   0 148   0   0   0   0  70   0   0   0   0 113  32 \n S5 S50 S51 S52 S53 S54 S55 S56 S58 S59  S6 S60 S61 S62 S63 S64 S65 S66 S67 S68 \n  0   0   0  46   0 117   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS69  S7 S72 S73 S74 S75 S76 S77 S78 S79  S8 S80 S81  S9 \n  0   0   0   0   0 234   0   0 109   0  43   0   0  40 \n\n\nWe can see that it has 74 rows (each sample is a row) and 701 columns (each ASV is a row). If we print just the first column (ASV) you can see that the way a sequence table is organized is that for each sample the number of times an ASV is observed is recorded."
  },
  {
    "objectID": "11_bioinformatics.html#remove-chimeras",
    "href": "11_bioinformatics.html#remove-chimeras",
    "title": "11  Bioinformatics",
    "section": "12.3 Remove chimeras",
    "text": "12.3 Remove chimeras\nThe DADA2 algorithm accounts for indel errors and substitutions when inferring ASVs but before taxonomic assignment we also need to check for chimeras17. DADA2 identifies sequences that are likely chimeras by aligning each sequence with sets of sequences that were recovered in higher abundance and then determining if any lower-abundant sequences can be made by mixing left and right sequences from two of the more abundant ones.17 Chimeras are non-biological sequences were the left- and right segment of the merged sequence are from two or more parent sequences.\nWe can use the function removeBimeraDenovo() to identify and remove Chimeras and then creating and updated sequence table.\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab,\n                                    method = \"consensus\", \n                                    multithread = FALSE, \n                                    verbose = TRUE)\n\nAfter removing chimeras, the data set consists of 700 unique sequences across 74 samples.\nEven though chimeric sequences can frequently make up a large part of sequence variants and therefore initially make the data set seem more variable than it is, overall once you account for abundance, they should only be a very small component of the merged sequence reads. Here 0.1 % of merged sequence variants are chimeras, though once you account for abundance of these variants, overall 99.9% of merged sequences are not chimeric.\nLet’s take a look at the distribution of our non-chimeric ASV lengths.\n\n\n\n\n\nFigure 12.1: Distribution of sequence length for merged ASVs. The red dotted line indicates sequences that are 100-105bp long. We are targeting a 106 bp amplicon and are using 2x150 bp sequencing. Primer sites are approx 25bp each."
  },
  {
    "objectID": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "href": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "title": "11  Bioinformatics",
    "section": "12.4 Summary of read filtering & processing for QC",
    "text": "12.4 Summary of read filtering & processing for QC\nAt this point, we will want to take a look at how many reads where lost at each step to determine if those patterns look as expected. We can pull that information from various output files by counting the reads and putting them all in a dataframe.\n\n# custom function to get read numbers\ngetN &lt;- function(x) sum(getUniques(x))\n\n# create data table with number of reads per sample at each step\ntrack &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergeASVs, getN), rowSums(seqtab.nochim)) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"sample\") %&gt;%\n  rename(trimmed = reads.out,\n         denoisedF = V3,\n         denoisedR = V4,\n         merged = V5,\n         rm_chimera = V6) %&gt;%\n  pivot_longer(names_to = \"filter\", values_to = \"reads\", cols = 2:7) %&gt;%\n  mutate(filter = ordered(filter, levels = c(\"reads.in\", \"trimmed\", \"denoisedF\", \"denoisedR\", \"merged\", \"rm_chimera\")),\n         k_reads = reads/1000)\n\n# plot distribution\nggplot(track, aes(x = filter, y = k_reads)) +\n  geom_boxplot(fill = \"darkorange\") +\n  labs(x = \"filter/processing step\", y = \"thousand reads per sample\") +\n  theme_standard\n\n\n\n\nFigure 12.2: Comparison of change in the number of reads per sample at each filtering & processing stage. Red dotted line indicates targeted 150k reads per sample.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at the boxplot and describe how the different processing steps have impacted the number of reads remaining in the data set.\nRemember that this is a modified data set where each sample has been subsampled to contain 1000 reads - normally that initial box describing the distribution of samples would be wider as different samples would contain different numbers of reads. Generally, researchers target &gt; 75,000 - 150,000 reads per sample to make sure that all taxa present can be recovered in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOutside of the initial filtering there should not be any steps at which a substantial amount of reads are lost, the majority of reads should merge. If this is not the case, then trimming is likely to conservative and should be revisited. Similarly, only a small proportion should be chimeric. If primers are not completely removed, ambiguous nucleotides can interfere during chimera ID and that step of quality filtering should be revisited."
  },
  {
    "objectID": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "href": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "title": "11  Bioinformatics",
    "section": "12.5 Taxonomic classification for ASVs present in each sample.",
    "text": "12.5 Taxonomic classification for ASVs present in each sample.\nWe are now in the final stretch. Now that we have our ASVs and our sequence table the only thing that we still need to do is figure out which species (or other taxonomic groups) our ASVs correspond to.\nThis brings us to an important limitation of metabarcoding studies which is that our results are only ever as good as our reference database to which we can match our ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nOur next step is to match the sequences to a list of sequences that have taxonomic information. In our case study this would be a sequenced fungi. Describe your expectations of the results - do you expect every ASV in the data set to find a match in the reference? What other issues could result in results being ambiguous?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nDADA2 has a built in function of a naive Bayesian classification menthod (assignTaxonomy()) that takes the sequences to be classified as the input along with a fasta file that contains the reference sequences18.18 fasta files consist of a header line that starts with &gt; and can contain any information about the sequence and then the sequence itself is in the next line. A multifasta file can contain multiple sequences, the program using the file determines the start of a new sequence by the fact that it “sees” the &gt; of the next header file.\nDepending on the project, scientists will pull sequences available from public databases such as genbank or if you are working with species that do not have a lot of sequences available you would need to create your own database by sampling individuals representing the different species/taxonomic groups you expect to encounter and sequencing them.\nWe are using the UNITE database of references which is designed to gather available ITS sequences to identify Eukaryotes.\nRunning this function is going to take several minutes - depending on the speed of your computer. Note that this code has the chunk option eval set as false so that it won’t run. Instead you can see that using the function save() I have writen the object out to your results folder. Then we can load it into your R environment in the next code chunk using load().\n\ntaxotab &lt;- assignTaxonomy(seqtab.nochim,\n                          refFasta = \"data/sh_general_release_dynamic_25.07.2023.fasta\",\n                          minBoot = 50, \n                          multithread = FALSE)\n\nsave(taxotab, \n     file = \"results/taxotab.rds\")\n\nWe can take a look at our results… there is a lot of information in this table so let’s just pull the first 5 ASV’s taxonomy without the ASV sequence for a better viewing experience.\n\n# load object into environment\nload(\"results/taxotab.rds\")\n\n# look at first 5 ASVs' taxonomy without the ASV sequence\nwrite.table(taxotab[1:5,], row.names = FALSE)\n\n\"Kingdom\" \"Phylum\" \"Class\" \"Order\" \"Family\" \"Genus\" \"Species\"\n\"k__Plantae\" NA NA NA NA NA NA\n\"k__Fungi\" \"p__Basidiomycota\" \"c__Agaricomycetes\" \"o__Agaricales\" \"f__Tricholomataceae\" \"g__Mycena\" NA\n\"k__Fungi\" \"p__Mucoromycota\" \"c__Umbelopsidomycetes\" \"o__Umbelopsidales\" \"f__Umbelopsidaceae\" \"g__Umbelopsis\" \"s__dimorpha\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Sordariomycetes\" \"o__Xylariales\" \"f__Amphisphaeriaceae\" \"g__Polyscytalum\" \"s__algarvense\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Dothideomycetes\" \"o__Mytilinidales\" \"f__Gloniaceae\" \"g__Cenococcum\" \"s__geophilum\"\n\n\nWe can also pull the unique species identified.\n\nunique(unname(taxotab[,7])) \n\n  [1] NA                    \"s__dimorpha\"         \"s__algarvense\"      \n  [4] \"s__geophilum\"        \"s__flavescens\"       \"s__terricola\"       \n  [7] \"s__elongatum\"        \"s__punicea\"          \"s__sylvestris\"      \n [10] \"s__humilis\"          \"s__subvinosa\"        \"s__opacum\"          \n [13] \"s__abramsii\"         \"s__ericae\"           \"s__terminalis\"      \n [16] \"s__variata\"          \"s__rexiana\"          \"s__zollingeri\"      \n [19] \"s__deciduus\"         \"s__album\"            \"s__chlamydosporicum\"\n [22] \"s__saponaceum\"       \"s__rufescens\"        \"s__populi\"          \n [25] \"s__podzolica\"        \"s__reidii\"           \"s__asperellum\"      \n [28] \"s__microspora\"       \"s__simile\"           \"s__acicola\"         \n [31] \"s__subsulphurea\"     \"s__camphoratus\"      \"s__pseudozygospora\" \n [34] \"s__heterochroma\"     \"s__brunneoviolacea\"  \"s__verrucosa\"       \n [37] \"s__auratus\"          \"s__mutabilis\"        \"s__chlorophana\"     \n [40] \"s__fuckelii\"         \"s__miniata\"          \"s__lignicola\"       \n [43] \"s__pilicola\"         \"s__phyllophila\"      \"s__australis\"       \n [46] \"s__citrina\"          \"s__fragilis\"         \"s__conica\"          \n [49] \"s__lubrica\"          \"s__pygmaeum\"         \"s__isabellina\"      \n [52] \"s__var._bulbopilosa\" \"s__finlandica\"       \"s__echinulatum\"     \n [55] \"s__lacmus\"           \"s__trabinellum\"      \"s__reginae\"         \n [58] \"s__spadicea\"         \"s__myriocarpa\"       \"s__physaroides\"     \n [61] \"s__calyptrata\"       \"s__nigrella\"         \"s__carneum\"         \n [64] \"s__vagans\"           \"s__metachroides\"     \"s__fumosa\"          \n [67] \"s__cantharellus\"     \"s__laetior\"          \"s__fusiformis\"      \n [70] \"s__spirale\"          \"s__pullulans\"        \"s__crocea\"          \n [73] \"s__sublilacina\"      \"s__acerinum\"         \"s__macrocystis\"     \n [76] \"s__vrijmoediae\"      \"s__changbaiensis\"    \"s__cygneicollum\"    \n [79] \"s__hymenocystis\"     \"s__dioscoreae\"       \"s__alnicola\"        \n [82] \"s__difforme\"         \"s__bicolor\"          \"s__spurius\"         \n [85] \"s__griseoviride\"     \"s__rebaudengoi\"      \"s__rufum\"           \n [88] \"s__globulifera\"      \"s__skinneri\"         \"s__sindonia\"        \n [91] \"s__verhagenii\"       \"s__maius\"            \"s__anomalovelatus\"  \n [94] \"s__diversispora\"     \"s__fellea\"           \"s__splendens\"       \n [97] \"s__coccinea\"         \"s__nitrata\"          \"s__risigallina\"     \n[100] \"s__juniperi\"         \"s__columbetta\"       \"s__rhododendri\"     \n[103] \"s__cinereus\"         \"s__fusispora\"        \"s__scaurus\"         \n[106] \"s__soppittii\"        \"s__grovesii\"         \"s__atropurpureum\"   \n[109] \"s__renispora\"        \"s__pura\"             \"s__foliicola\"       \n[112] \"s__phaeococcinea\"    \"s__rosea\"            \"s__stuposa\"         \n[115] \"s__minima\"           \"s__atrovirens\"       \"s__canadensis\"      \n[118] \"s__silvestris\"       \"s__sepiacea\"         \"s__pyriforme\"       \n[121] \"s__bulbillosa\"       \"s__glutinosum\"       \"s__cylichnium\"      \n[124] \"s__aeria\"            \"s__veluwensis\"       \"s__epicalamia\"      \n[127] \"s__hyalina\"          \"s__cylindrica\"       \"s__miyabei\"         \n[130] \"s__terrestris\"       \"s__rimosissimus\"     \"s__acuta\"           \n[133] \"s__myxotrichoides\"   \"s__physodes\"         \"s__alpina\"          \n[136] \"s__fallax\"           \"s__fumosibrunneus\"   \"s__albicastaneus\"   \n[139] \"s__mors-panacis\"     \"s__glacialis\"        \"s__acerina\"         \n[142] \"s__flavidum\"         \"s__ocularis\"         \"s__exigua\"          \n[145] \"s__piceae\"           \"s__verzuoliana\"      \"s__alliacea\"        \n[148] \"s__entomopaga\"       \"s__hyalocuspica\"     \"s__umbrosum\"        \n[151] \"s__bombacina\"        \"s__boeremae\"         \"s__fortinii\"        \n[154] \"s__miyagiana\"       \n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to additionally pull the unique genera, families, and orders contained in the data set and describe our results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#from-the-beginning",
    "href": "11_bioinformatics.html#from-the-beginning",
    "title": "11  Bioinformatics",
    "section": "12.6 From the beginning …",
    "text": "12.6 From the beginning …\n\n\n\n\n\n\n Consider this\n\n\n\nNow that we have been through all the steps of processing next generation sequencing data set in a metabarcoding processing pipeline go back over the entire process, make sure you have understand what the key steps are and then outline that process below. Be sure to list the key steps in an organized manner and describe what occurs at each step in 2-3 sentences.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#acknowledgements",
    "href": "11_bioinformatics.html#acknowledgements",
    "title": "11  Bioinformatics",
    "section": "12.7 Acknowledgements",
    "text": "12.7 Acknowledgements\nThis chapter borrows heavily from the original DADA2 tutorial as well as Alexis Carteron & Simon Morvan’s tutorial based on the subset sequences from (Carteron et al. 2021).\n\n\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7."
  },
  {
    "objectID": "15_misinformation.html#on-liars-and-bullshitters",
    "href": "15_misinformation.html#on-liars-and-bullshitters",
    "title": "15  Climate change: Misinformation",
    "section": "15.1 On Liars and Bullshitters",
    "text": "15.1 On Liars and Bullshitters\n\n\n\n\n\n\n\nThere are three kinds of lies, lies, damned lies, and statistics\n~ probably not Mark Twain.\n\n\nThe bullshitter is neither on the side of the true or the side of the false. His eye is not on the facts at all. He does not reject the authority of the truth, as the liar does, and oppose himself to it. He pays no attention to it at all. By virtue of this, bullshit is a greater enemy of the truth than lies are.\n~ Harry G. Frankfurt\n\n\n\n\nIn his book ‘On Bullshit’, Harry G. Frankfurt distinguishes between a liar who knows the truth and deliberately tries to convince somebody of something that is untrue and a bullshitter who either does not know the truth or does not care - their focus is solely on persuading the listener. The focus of this distinction is the blatant disregard for truth and a carelessness in how data and facts are used.\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief definition of misinformation and disinformation. Then compare these definitions with the framework of a liar vs a bullshitter and argue whether or not you agree with Harry Frankfurt that “bullshit is the greater enemy of the truth than lies are”.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nIn 2015, the National Review posted what they claimed to be “the only climate change chart you need to see”.\n\n\n\nTwitter post from the National Review depicting the “only climate change chart you need to see”\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe the figure and state what you think the message is that the National Review was trying to convey by posting this chart. Argue whether your would classify this as misinformation or disinformation1.\n\n\n1 Remember, when you ‘argue’ you should always state how/why it fulfills or illustrates a definition, statement, or set of criteria.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nClimate skepticism comes in different flavors. Early misinformation campaigns were directed at outright denying that global warming is occurring and/or that increased greenhouse gas emissions are the cause, i.e. that we are observing natural climate variability and there is therefore no need for policy to regulate emissions or to change individual behavior.\nArguably, a new flavor of climate skeptics has emerged in the last years that spend less time arguing about whether or not anthropogenic climate change is happening and focus on arguing that the reaction is overblown. Bjorn Lomborg and Michael Shellenberger are very outspoken representatives for this position and focus a lot of their writing on why “climate alarmists” are not only wrong but potentially harmful.\n\n\n\n\n\n\n Consider this\n\n\n\nSharpen your bullshit detector by looking up either Lomborg or Shellenberger and sketch out their position, main arguments on why climate change is being overblown, and suggestions for solutions/strategies (or why proposed solutions like cutting emissions are wrong and we should be focusing on other strategies). As part of your description, figure out what their background is, i.e. what credentials do they have to position themselves as “climate experts”.\nBriefly discuss this brand of climate skeptics. Points for discussion could include e.g. how their brand differs from what we typically think of as climate deniers, whether you think they have some valid points/arguments, what you think makes them compelling/effective, why you think it might be easier/more difficult to refute their claims (if you disagree), would you place them in categories of misinformation, disinformation, or bullshitter, or do you think it is a valid position to hold etc.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "15_misinformation.html#on-the-power-of-a-really-good-bad-visualization",
    "href": "15_misinformation.html#on-the-power-of-a-really-good-bad-visualization",
    "title": "15  Climate change: Misinformation",
    "section": "15.2 On the power of a really good, bad visualization",
    "text": "15.2 On the power of a really good, bad visualization\n::: {.callout-tip icon=false appearance=“simple”}\n\nAs I indicated before, having something that sounds scientific to say when making assertions to laymen is not the same as being correct\n~ Christopher Ashley Ford\n\n\n\n\n\n\n\n Consider this\n\n\n\n“Science-y charts” are a commonly employed tool by climate skeptics. Briefly discuss why you think visualizations are especially compelling.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe will do a deep dive into techniques specifically used by climate deniers & skeptics, with a focuse on the use of visualizations to round out our climate change module.\n\n\n\n\n\n\n Consider this\n\n\n\nCreate a list of at least five typical statements climate deniers/skeptics typically make and rank them by “level of denial”. Additionally refer to the Interview with Michael Mann on The Six D-words of climate change to create a framework of the types of arguments frequently encountered today that have replace “classic climate denialism”.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nReflect on posts you may have seen on social media (or other outlets) where climate deniers/skeptics have made use of visualizations. Briefly, contrast major categories they fall into (scientific(looking) figures, infographics, memes, photographs, bumper stickers/embroidered pillow-style slogans, …) in terms of how/why they are effective, which are more common than others, whether certain groups tend to use different types of visualizations, whether they more commonly are spreading misinformtion or disinformation, … and other things that pop out to you.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "15_misinformation.html#fa-clipboard-question-consider-this-5",
    "href": "15_misinformation.html#fa-clipboard-question-consider-this-5",
    "title": "15  Climate change: Misinformation",
    "section": "15.3  Consider this",
    "text": "15.3  Consider this\nReflect on posts you may have seen on social media (or other outlets) where climate deniers/skeptics have made use of visualizations. Briefly, contrast major categories they fall into (scientific(looking) figures, infographics, memes, photographs, bumper stickers/embroidered pillow-style slogans, …) in terms of how/why they are effective, which are more common than others, whether certain groups tend to use different types of visualizations, whether they more commonly are spreading misinformtion or disinformation, … and other things that pop out to you.\n:::\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "15_misinformation.html#on-becoming-bullsht-resistant",
    "href": "15_misinformation.html#on-becoming-bullsht-resistant",
    "title": "15  Climate change: Misinformation",
    "section": "15.3 On becoming bullsh*t resistant",
    "text": "15.3 On becoming bullsh*t resistant\n\n\n\n\n\n\n\nA lie can travel halfway around the world while the truth is still putting on its shoes\n~ still not Mark Twain - maybe Churchill?\n\nFalsehood will fly, as it were, on the wings of the wind, and carry its tales to every corner of the earth; whilst truth lags behind; her steps, though sure, are slow and solemn, and she has neither vigor nor activity enough to pursue and overtake her enemy\n~ Thomas Francklin, Sermons on Various Subjects, 1787\n\n\n\n\nOne of the goals of this course is to equip you with a framework to be able to categorize and assess new information you encounter, or as Neil deGrasse Tyson puts it “To be scientifically literate is to empower yourself to know when someone else is full of bullshit”. The key term here is to empower yourself, i.e. develop a set of skills so that you can pro-actively and habitually assess information - in short, be bullshit resistant.\n\n\n\n\n\n\n Consider this\n\n\n\nThink back to a time that you spotted some bullshit, briefly describe the misinformation you came across and outline the strategies you used to distinguish fact from fiction2.\n\n\n2 the mis from information?\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nHere is a non-exhaustive list of strategies that you should tuck away your tool belt for frequent use.\n\n15.3.1 Use Common Sense\n\n\n\n\n\n\n\nRemember to use your own brain\n~ definitely my Dad. so. many. times.\n\n\n\n\nThe used cars salesman principle is simple but powerful, when evaluating a source of information sk yourself four simple questions:\n\nWho is telling me this?\nHow do they know it?\nWhat are they trying to sell me?\nWhat do they have to gain?\n\n\n\n15.3.2 If it’s to good or to bad to be true - it probably is (unless it’s 2020)\nThis principle is a close relative of Occam’s Razor, also known as the Law of Parsimony.\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief definition of Occam’s Razor/Law of parsimony. Assess how the Law of Parsimony and the principle that claims that sound to good/bad to be true probably are can be used to identify misinformation. You may use an example if you’d like.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n15.3.3 Entertain multiple hypothesis\n\n\n\n\n\n\n\n*It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories,Instead of theories to suit facts..\n~ Sherlock Holmes\n\n\n\n\nThis principle encompasses both confirmation bias and the dreaded echo chamber - another way of stating it is difficult to not fall in love with your hypothesis and continue to entertain independent evidence.\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly define the concepts of confirmation bias and the echo chamber. List some techniques you can use to counteract them.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n15.3.4 Garbage in/garbage out\nWith ever larger and more available data sets and increasingly complex analysis it is important to not only consider the output and interpretation of an analysis and the methods used (which are becoming increasingly difficult to understand as a layperson) is only ever as good as the data set used in the first place. Consider the source of the data, sample size, how it was generated, and whether it is a legitimate comparison3.3 Assessing large-scale data sets and how we analyze them is as you may have noticed a central theme of this course. Consider our discussion of responsible conduct in science and the practices driving the reproducibility crisis.\n\n\n15.3.5 Causation & Correlation\nThere are two common logical fallacies that can be used to take a relationship of two variables to infer causation - cum hoc ergo propter hoc4 and post hoc ergo proper hoc5.4 with this, therefore because of this”, i.e. correlation implies causation5 after this, therefore because of this”, i.e. because something occurred first it must be causing the later\nTaking this to ad absurdum6 we have fun examples involving chocolate and Nobel laureates along with storks and babies. Both illustrate the fact that it is important to be careful when inferring causation, consider whether proxies are meaningful, whether there could be a common cause rather than correlation, and if there is a way to design manipulative experiments to determine causation. With increasingly large data sets it is easy to find a “signal” in a noisy data set and spin it into a story7.6 How many more fancy Latin phrases do I have up my sleeve? We shall probably never truly know… Is this an exempli gratia of using fancy language to obfuscate information? Possibly? Is using pretentious language and jargon also a strategy for spreading misinformation? Also difficult to tell.7 We will touch on this topic through the course in connection of determining whether your analysis is descriptive, inferential, or causal/mechanistic.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nWant to engage in some ambiguous visualization? Go to Spurious Correlations, create your own misleading but highly correlated relationship, download it, and post it in the #graphicdetail channel on slack as this week’s Graphic Detail. You get one Spurious Correlation for the semester … use it wisely!\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n15.3.6 When in doubt, go to the source\nIn one of their exhibits, the Pacific Science Center emphasizes four principles for a lay audience to consider when engaging with “science”:\n\nScience is meant to move slowly\nChange is a natural part of science\nExperts have different specialties\nNot all sources are trustworthy\n\nThese are important concepts to keep in mind when watching science unfold in real time and out in the open via social media, pre-prints (before peer-review) and in media outlets. It is important to slow down and give science an chance to breath and mature through discussion and testing.\n\n\nSee list of Media Outlet’s ranked by quality of their science reporting; there are several similar assessments you can use as a point of orientiation as well.\n“New study finds …” makes for a great headline, though unfortunately occasionally both deliberately and unintentionally by the time the “new study” is translated into a press release and that press release is written up in the media misinformation or disinformation may have sneaked in. This may include\n\ncherry-picking\nover-generalizing (see # In Mice examples and an article describing the intent behind the hashtag).\nfalse interpretation\n\n\n\n15.3.7 Beware of misleading visualizations\nWe’ll go into detail on this one in the next section."
  },
  {
    "objectID": "15_misinformation.html#on-general-stratgies-for-unmasking-manipulative-visualizations",
    "href": "15_misinformation.html#on-general-stratgies-for-unmasking-manipulative-visualizations",
    "title": "15  Climate change: Misinformation",
    "section": "15.4 On general stratgies for unmasking manipulative visualizations",
    "text": "15.4 On general stratgies for unmasking manipulative visualizations\nManipulative visualization either present false data or misrepresent data to tell a specific story, i.e. they are either lying with false data or lying with truthful data.\nA good way to assess any visualization is to break it down into content, structure, and presentation8.8 There is some overlap between these categories, you can also think about how these related to the grammar of graphics in having a data set (content), mapping aesthetics and geometry both of which connect to the structure and presentation of the graph.\n\n15.4.1 Content\nThe focus here is on the content itself, i.e. the data set, including how the data was generated, and what parts of the data is being presented.\n\n\n\n\n\n\n Consider this\n\n\n\nCreate a list of questions you can ask to identify manipulative visualizations based on content.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n15.4.2 Structure\nStructure refers to how data points are encoded, i.e. what data points represent, what bin sizes are used, how axes are presented etc.99 Think about all the components of the grammar of graphics we have discussed learning how to use ggplot.\n\n\n\n\n\n\n Consider this\n\n\n\nCreate a list of questions you can ask to identify manipulative visualizations based on structure.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n15.4.3 Presentation\nThere is some overlap between Structure and Presentation, but the focus here is more on the visualization as a whole, i.e. this includes not only “scientific-style” figures but other forms of visualization as well.\n\n\n\n\n\n\n\n*It is alright to decorate construction but never construct decoration\n~ Augustus Welby Northmore Pugin\n\n\n\n\nThe presentation for a graph can easily be used for intentional manipulation and misrepresentation of the data.\nAs we have seen from examples of how visualizations are used in climate change misinformation campaigns, the choice of a scientific(ish) figure is frequently intentional - it lends gravitas to claims. This does not always mean that the visualization meets the standards for a scientific analysis.\nSimilarly, we have seen that photographs are powerful and can easily be used to be manipulative. Keep in mind, that just because you agree with the statement being supported by a picture that doesn’t mean that picture is being used intentionally to sway you to a point.\nInfographics have experienced a rapid growth in popularity in the last ten years10. They are a peculiar mix of data visualization, entertainment, art, and communication. They also vary in the quality of the underlying data set, frequently lack context but feature catchy fun facts and large print key “results”. When done well, they effectively communicate information - unfortunately they are also frequently used for misinformation.10 Here’s an infographic the rise of infographics for you to refer to.\n\n\n\n\n\n\n Consider this\n\n\n\nHere is an example of an infographic shared by the Heartland Institute. The Heartland Institute is a conservative/libertarian public policy think tank that rejects scientific consensus on climate change and publishes a range of materials supporting climate change denial.\nThis is the their description\n\nThis image contains 10,000 dots, each representing atmospheric gases. Nitrogen: 7,800 dots Oxygen: 2,100 dots Argon: 93 dots Natural Carbon Dioxide: 3 dots Human Carbon Dioxide Emissions: 1 dot Leftists claim 1 dot is going to destroy the Earth. Don’t let them take away your freedoms with cries of doom and gloom!\n\n\n\n\nInfographic posted by the Heartland Institute.\n\n\nDescribe what the central message is you think they are trying to convey with this infographic and argue whether this falls in the category of misinformation or disinformation. Discuss whether you think this figure is compelling in it’s design and what could make it effective in persuading viewers of their point of view.\nStep it up: Find an infographic and discuss the quality of the visualization and underlying data as one of your weekly Graphic Detail posts.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nTwo types of potentially misleading figures are Ducks and Glass slippers. Frequently, they fall more into the category of Frankfurtian BS as typically the focus is on how the data is presented using specific gimmicks at the cost of carelessness towards accurately representing the data at\n\n\nDucks are named after Big_Duck a building shaped like a duck built to sell duck and duck eggs.\n\nDucks are figures where the topic of the figure is reflected in the design itself, e.g. the global carbon footprint being shaped like a footprint.\nGlass slippesr (mis)use a very specific format meant to visualize a very specific thing (e.g. a periodic table) in an incorrect context that detracts more from the content than adds to it (and sometimes is quite confusing!)11.\n\n11 If you are familiar with the original version of Cinderella, the Prince and his footman take the glass slipper to all the houses in the kingdom to find his princess. The evil stepsisters chop off their toes and heel in desperation to make it fit. Why this is not in the Disney version remains a mystery. Nevertheless, Glass slipper describes desperately trying to shoehorn data into a visualization type that doesn’t quite fit.\n\n\n\n\n\n Consider this\n\n\n\nFor each of the following images, briefly describe what the visualization depicts and argue whether you think it is a Duck or a glass slipper, briefly argue whether you think it is an effective visualization or if the presentation obscures the information being conveyed or could easily mislead the viewer.\nFrom the cover of Time Magazine.\n\n\n\nCover of Time Magazine from July 2020.\n\n\nFrom The Economist\n\n\n\nEconomist figure that may have gotten out of hand.\n\n\nOldie, but goldie - sponsored by the FAU Center for Alcohol & Other Drug Prevention\n\n\n\nYour grades & Beer.\n\n\nNexthink’s Periodic Table of IT Ops Tools12\n\n\n\nNexthink Twitter post.\n\n\nStep it up: Find a visualization that is a “Duck” or a “Glass slipper” and discuss the quality of the visualization and underlying data as one of your weekly Graphic Detail posts.\n\n\n12 For this one “no clue what they are trying to convey” is an option as an answer, but if you are able to figure it out, please do share. Definitely, answer the other parts of the question.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nWrite a list of what you think are good practice for visualizing your data sets. Include at least three Do’s and three Don’ts. Document it here and then also share it in our slack channel in the #assignments channel.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "15_misinformation.html#acknowledgements",
    "href": "15_misinformation.html#acknowledgements",
    "title": "15  Climate change: Misinformation",
    "section": "15.5 Acknowledgements",
    "text": "15.5 Acknowledgements\nA large component of the general content of this set of activities is inspired by Carl Bergstrom and Jevin West’s course and book on Calling Bullshit: The Art of Skepticism in a Data-Driven World and directly draws on some of their content. Their course is online, including videos, it’s entertaining and informative. Put the book on your Christmas wish list."
  },
  {
    "objectID": "z_references.html",
    "href": "z_references.html",
    "title": "References",
    "section": "",
    "text": "Callahan, Benjamin J., Paul J. McMurdie, Michael J. Rosen, Andrew W.\nHan, Amy Jo A. Johnson, and Susan P. Holmes. 2016.\n“DADA2: High-resolution\nSample Inference from Illumina Amplicon Data.”\nNature Methods 13 (7, 7): 581–83. https://doi.org/10.1038/nmeth.3869.\n\n\nCarlson, John K., and Ivy Baremore. 2005. “Growth Dynamics of the\nSpinner Shark (Carcharhinus Brevipinna) Off the\nUnited States Southeast and Gulf of\nMexico Coasts: A Comparison of Methods.” Fishery\nBulletin 103 (2). https://aquadocs.org/handle/1834/26223.\n\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and\nEtienne Lalibert’e. 2021. “Temperate Forests\nDominated by Arbuscular or Ectomycorrhizal\nFungi Are Characterized by Strong Shifts from\nSaprotrophic to Mycorrhizal Fungi with\nIncreasing Soil Depth.” Microbial Ecology\n82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7.\n\n\nHeupel, Michelle R., John K. Carlson, and Colin A. Simpfendorfer. 2007.\n“Shark Nursery Areas: Concepts, Definition, Characterization and\nAssumptions.” Marine Ecology Progress Series 337 (May):\n287–97. https://doi.org/10.3354/meps337287.\n\n\nHeupel, Michelle R., Shiori Kanno, Ana P. B. Martins, Colin A.\nSimpfendorfer, Michelle R. Heupel, Shiori Kanno, Ana P. B. Martins, and\nColin A. Simpfendorfer. 2018. “Advances in Understanding the Roles\nand Benefits of Nursery Areas for Elasmobranch Populations.”\nMarine and Freshwater Research 70 (7): 897–907. https://doi.org/10.1071/MF18081.\n\n\nNeer, J. A., B. A. Thompson, and John K. Carlson. 2005. “Age and\nGrowth of Carcharhinus Leucas in the Northern\nGulf of Mexico: Incorporating Variability in\nSize at Birth - Neer - 2005 - Journal of\nFish Biology - Wiley Online Library.”\nJournal of Fish Biology 67 (2): 370–83. https://onlinelibrary.wiley.com/doi/full/10.1111/j.0022-1112.2005.00743.x.\n\n\nPlumlee, Jeffrey D., Kaylan M. Dance, Philip Matich, John A. Mohan,\nTravis M. Richards, Thomas C. TinHan, Mark R. Fisher, and R. J. David\nWells. 2018. “Community Structure of Elasmobranchs in Estuaries\nAlong the Northwest Gulf of Mexico.”\nEstuarine, Coastal and Shelf Science 204 (May): 103–13. https://doi.org/10.1016/j.ecss.2018.02.023.\n\n\nSwift, Dominic G., and David S. Portnoy. 2021. “Identification and\nDelineation of Essential Habitat for\nElasmobranchs in Estuaries on the Texas\nCoast.” Estuaries and Coasts 44 (3): 788–800. https://doi.org/10.1007/s12237-020-00797-y."
  },
  {
    "objectID": "01_intro-data-science.html#learning-objectives",
    "href": "01_intro-data-science.html#learning-objectives",
    "title": "1  What even is Data Science?",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives\nAfter completing this lab you should be able to\n\ndefine what data is and what the structural/functional relationships to information, knowledge, insight and wisdom are.\ndescribe what data science is and the major field/skill sets that comprise it.\ndescribe the individual components that comprise the data science life cycle."
  },
  {
    "objectID": "01_intro-data-science.html#defining-data-and-science",
    "href": "01_intro-data-science.html#defining-data-and-science",
    "title": "1  What even is Data Science?",
    "section": "1.2 Defining “data” and “science”",
    "text": "1.2 Defining “data” and “science”\nData are qualitative and quantitative observations that are measured and collected.\n\n\n\n\n\n\n Consider this\n\n\n\nData fall in two distinct categories, categorical and numerical data. Briefly compare and contrast these two categories by describing the data types you would expect to find in each.\n\n\nThe ‘data - information - knowledge - wisdom pyramid’ gives us a framework to consider how data can be used to inform decision making and impact the world around us. Not until data is organized and processed thus adding context can we glean information from the signal. Additional meaning is transferred as we synthesize and further contextualize information resulting in knowledge.\nThese three categories look backwards - we describe the “what” and ask about the “why” to reveal patterns and relationships. At this point we start looking forward to determine what action(s) should be taken, we know seek to reveal principles and directions that can be applied.\nWe integrate knowledge across disciplines to gain insight and wisdom to further understanding of problems and derive actionable solutions. This culminates in the decision-making process resulting in change.\n\n\n\n\n\n\n Consider this\n\n\n\nThe science council defines science as the pursuit and application of knowledge and understanding of the natural and social world following a systematic methodology based on evidence. Compare and contrast this definition to the DIKW framework and make an argument that all science is data science."
  },
  {
    "objectID": "01_intro-data-science.html#what-even-is-data-science",
    "href": "01_intro-data-science.html#what-even-is-data-science",
    "title": "1  What even is Data Science?",
    "section": "1.3 What even is data science?",
    "text": "1.3 What even is data science?\nData science is a fuzzy term and no single definition exists. Most definitions emphasize that it is a interdisciplinary field and that it has arisen in response to the increasingly large data sets that are produced.\nA common way of defining data science is to describe it as being the intersection of domain knowledge, statistics/mathematics, and computer science - though different definitions will ascribe more importance to certain different components.\nOne distinction to the typical scientific process as you may have learned it to be is that a large component of data science is hypothesis generation through exploratory analysis rather than hypothesis confirmation.\nThe data science process generally starts by posing an interesting question and ends with visualizing and communicating the results. The key steps to the end results are obtaining the data, processing and exploring the data, and modeling the data to understand the data set and derive conclusions."
  },
  {
    "objectID": "01_intro-data-science.html#the-data-science-process",
    "href": "01_intro-data-science.html#the-data-science-process",
    "title": "1  What even is Data Science?",
    "section": "1.4 The data science process",
    "text": "1.4 The data science process\n\n1.4.1 Ask an interesting question\nLet’s start by asking an interesting question:\n\nWill Sasquatch by impacted by climate change?\n\nSpecifically, climate change could result in a shift in habitat availability, i.e. should we expect a species extinction because of habitat loss or a range expansion or range shift?\nTo do this we need to generate a species distribution model (SDM) for the Sasquatch, a large, hairy, bipedal ape-like creature found (or is it?) throughout North America1, i.e. first we need to understand where Sasquatch are currently distributed to then assess how that might change in the future.1 I suppose, technically, since we were venturing into cryptozoology here it is not a species, but rather a cryptid distribution model\n\n\n1.4.2 Get occurrence data\nThe first thing we need for any SDM is a data set documenting species occurrence, i.e. geo-coded observations of a given species in the wild.\nFour our purposes, we turn to the Bigfoot Field Researchers Organization (BFRO), founded in 1995 as the “only scientific research organization exploring the Bigfoot/Sasquatch mystery”. You can turn to their website for answers on important FAQs, including ‘Do Bigfoot Sasquatch bury their dead?’, ‘Where is the physical evidence?’, ‘Wasn’t this all shown to be a fake?, and ’Why do you want scientists to recognize the Sasquatch as a species? Isn’t it better to just leave them alone?’. Their main focus though is on compiling reports of sightings and investigating them. In other words, they have a database full of geo-coded reported sightings2.2 And even better, it has already been downloaded and wrangled and is accessible (with sightings through 2018) right here.\nLet’s read in the data and then we can take a look at the information we can glean from this data set by looking at the column names.\n\n# read data\noccurrence &lt;- read_delim(\"data/bfro_reports_geocoded.txt\", delim = \"\\t\")\n\n\n\n1.4.3 Tidy, Transform & Explore the data\nLet’s start by taking a look at our data set to determine how we need to wrangle to get the information we need process it so we can generate our species distribution model.\nBecause we need to be able to identify the exact locations Sasquatch occur, we are going to remove any observations that do not have latitude and longitude information.\n\n# filter NAs\noccurrence &lt;- occurrence %&gt;%\n  filter(!is.na(longitude),\n         !is.na(latitude))\n\nNext, let’s consider is what geographic extent of the observations is by looking at the distributions on a map.\n\n# get minimum and maximum lat/longs\nmax.lat &lt;- ceiling(max(occurrence$latitude))\nmin.lat &lt;- floor(min(occurrence$latitude))\nmax.lon &lt;- ceiling(max(occurrence$longitude))\nmin.lon &lt;- floor(min(occurrence$longitude))\n\n# create an extent object of the range of observations\ngeo_range &lt;- extent(x = c(min.lon, max.lon, min.lat, max.lat))\n\n# get base map\ndata(wrld_simpl)\n\n# plot the base map\nplot(wrld_simpl, \n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE, \n     col = \"grey75\")\n\n# add individual occurrences\npoints(x = occurrence$longitude, \n       y = occurrence$latitude, \n       col = \"darkorange\", \n       pch = 20, \n       cex = 0.75)\n\n# draw box around figure\nbox()\n\n\n\n\nFigure 1.1: Map of Sasquatch sighting in the United States based on the BFRO database (1950 - 2021).\n\n\n\n\nHere is an example of where domain knowledge comes in - while we are tidying and exploring the data set we need to assess whether there are artifacts our outlier data points that should be removed.\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief description of the spatial distribution of the occurrence of Sasquatch in the United States. Note areas where sightings appear to be random, clustered or more dispersed, determine if you think any points should be removed.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss some possible explanations for the patterns you have observed to determine whether you think this data set is a reasonable representation of the ecological niche of the Sasquatch and can be used to create a species distribution model.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGood catch - those sightings in the middle of the ocean are probably errors in the geo-coding. Further, we don’t know how reliable the data for Alaska is as this data set is based on opportunistic sightings - at least for the most part, if you poke around the BFRO website you will find that people do plan expeditions.\n\n\n\nThe data for for observations in the lower 48 seems much more reliable, so let’s restrict our data to those observations.\n\n# load and filter data\noccurrence &lt;- read_delim(\"data/bfro_reports_geocoded.txt\", delim = \"\\t\") %&gt;%\n  filter(!is.na(longitude),\n         !is.na(latitude),\n         longitude &gt; -130,\n         latitude &lt; 55)\n\n# get minimum and maximum lat/longs\nmax.lat &lt;- ceiling(max(occurrence$latitude))\nmin.lat &lt;- floor(min(occurrence$latitude))\nmax.lon &lt;- ceiling(max(occurrence$longitude))\nmin.lon &lt;- floor(min(occurrence$longitude))\n\n# create an extent object\ngeo_range &lt;- extent(x = c(min.lon, max.lon, min.lat, max.lat))\n\n# get base map\ndata(wrld_simpl)\n\n# plot the base map\nplot(wrld_simpl, \n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE, \n     col = \"grey75\")\n\n# add individual occurrences\npoints(x = occurrence$longitude, \n       y = occurrence$latitude, \n       xlim = c(min.lon, max.lon),\n       ylim = c(min.lat, max.lat),\n       col = \"darkorange\", \n       pch = 20, \n       cex = 0.75)\n\n# draw box around figure\nbox()\n\n\n\n\nFigure 1.2: Map of Sasquatch sighting in the lower 48 states based on the BFRO database as (1950 - 2021).\n\n\n\n\nThis should work - on to the next step!\n\n\n1.4.4 Get more data\nOccurrence data along will not allow us to build a species distribution model; what we need is environmental data to go with the locations, i.e. we need a set of predictor variables.\nCombining multiple data sets is typical for data science projects, frequently the quality of a project hinges on the availability of hight quality data sets that can provide information to describe or preodict behavior if the data set you are exploring.\nClimate is complex and multidimensional, though at its core climate is determined by long-term patterns in mean and variability of temperature and precipitation.\nWe are going to use the bioclim data set from CliMond. Bioclim variables are commonly used for species distribution modeling as they are based on long-term trends (as opposed to e.g. the exact conditions when the species was observed).\n\n\n\n\n\n\n Consider this\n\n\n\nLook up the descriptions of the bioclim variables and give a brief description of the four abiotic parameters that are included and how they are being parameterized. Argue which you think are most important to describe a species distribution/ecological niche and whether you think overall this data set will help us understand Sasquatch species distribution.\n\n\nThe data set we are using includes a core set of 19 variables that describe temperature and precipitation, along with an additional 16 variables that describe solar radiation and soil moisture. This information is encoded in the raster files of the historical (contemporary) bioclim data sets at a resolution of 10’ (minutes) into the data folder. The “historical” data set consists of data from 1961 - 1990 centered on 1975.\nA raster file is an image file consisting of pixels with data associated with it. In this case, our “pixels” are 10’ x 10’ and depending on the layer the value associated with each pixel is the value for that bioclim value at that geographic location.\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_1975H', full.names=TRUE )\n\n# import and convert to raster stack\npredictors &lt;- stack(files)\n\nWe have now created an object that at its core consists of a list where each element is a layer (bioclim variable raster).\n\n\n1.4.5 Tidy, transform & explore the data (again)\nLet’s plot the first bioclim variable (Bio01, annual mean temperature).\n\nplot(predictors@layers[[1]])\n\n\n\n\nFigure 1.3: Global distribution of annual mean temperature (1961 - 1990, centered on 1975).\n\n\n\n\nWe see the pattern we would intuitively expect, with temperatures decreasing as you move poleward and being warmest around the poles.\nLet’s extract the values for each bioclim variable at our occurrence points (observations).\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  dplyr::select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\nLet’s take a quick look at the first few rows and columns of the matrix we just created.\n\nhead(presence[,1:3])\n\n     CM10_1975H_Bio01_V1.2 CM10_1975H_Bio02_V1.2 CM10_1975H_Bio03_V1.2\n[1,]              8.387707              15.84337             0.3534581\n[2,]             10.156170               9.49416             0.2941423\n[3,]             15.522450              13.50635             0.3474373\n[4,]             11.259700              12.40612             0.3456673\n[5,]             10.189860              11.02385             0.3073747\n[6,]              8.724987              10.20157             0.2807353\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly state what these values represent.\n\n\n\n\n1.4.6 Create species distribution model\nOur next step is to fit a bioclim model to the data set we just generated.\n\n# fit bioclim model\nmodel.fit &lt;- bioclim(presence)\n\nThe bioclim model is a classic climate-envelope-model3.3 You may remember reading about this in your reading assignments.\nBriefly, the algorithm computes the similarity of locations by comparing the value of each environmental variables being used (our bioclim data set) to a distribution of that values at all locations with known presence4. The closer that value is to the median (50th percentile), the more suitable that location is assumed to be. Suitability scores are between 0 and 1, with 1 indicating a “perfect suitability”.)4 Also called the training sites; these are our occurrence points.\nIn general, there is no distinction between the tails of the distribution (i.e. the 90th and 10th percentile are equivalent), though in some implementations you can specify those to be treated as distinct. As a result e.g. low levels of precipitation could be limiting but high levels would not be.\nFinally, we will use our suitability scores and the bioclim raster data set to generate a predictive map of the Sasquatch species distribution. This means that the algorithm will assign a suitability score to each pixel based on the model and create a new raster layer.\n\n# generate raster with predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\nLet’s plot our species distribution map.\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.4: Species distribution model for Sasquatch. Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief description of the habitat suitability map, including the maximum probabilities. Describe wether this result fits your expectations based on the distribution map of reports we looked at earlier. According to our map, if you were planning a Sasquatch research trip, where would you be headed?\n\n\nOur highest habitat suitability values (probability of occurring) seem pretty low. One reason for this is that we used presence-only data.\n\n\n\n\n\n\n Consider this\n\n\n\nThe alternative to presence-only models is to have presence-absence data. Discuss how this would improve the models. Argue why you think presence-only data sets are easier to generate.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nA work-around of not having absence data is to generate pseudo-absence data. This is done by generating random points within the geographic range and using those as proxies for absence data. Briefly argue the merits and limitations of such an approach.\n\n\n\n\n1.4.7 Model future climate change\nProjections of future climate change are heavily dependent on human activity and the resulting greenhouse gas emissions. Therefore the IPCC’s Assessment Reports contain scenario families that represent projected climate conditions based on emission scenarios resulting from future technological and economic development as defined by each scenario.\nLet’s look at how the species distribution map might change in response to a shift in environmental parameters.\nTo do this we will use bioclim raster files for 2100 generated using the A1B and A2 scenarios.\nThe A1 climate scenarios assume a more integrated world characterized by rapid economic growth, a global population that peaks at 9 billion (2050) and the gradually declines, rapid spread of new/efficient technologies, and a convergent world characterized by extensive worldwide social and cultural interactions. Scenario A1B further assumes a balanced emphasis on fossil and non-fossil fuels.\nBy contrast, A2 scenarios assume a more divided world consisting of independently operating and self-reliant nations and regionally-oriented economic development. The population is assumed to continuously grow. Finally, this scenario is characterized by high emissions.\nLet’s start with the A1 climate scenario to create our species distribution model.\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_2100_A1B', full.names=TRUE )\n\n# import and convert to raster stack\npredictors_A1 &lt;- stack(files)\n\nNow let’s fit our model and create predictive map.\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors_A1, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\n# fit the bioclim model\nmodel.fit &lt;- bioclim(presence)\n\n# create raster layer of predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\nFinally, let’s plot our species distribution map.\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.5: Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A1B). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief description of the habitat suitability map, including the maximum probabilities. Describe whether this fit your expectations based on the change in bioclim variables. Describe the shift in habitat suitability relative to our current model.\n\n\nHow about our A2 climate scenario?\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_2100_A2', full.names=TRUE )\n\n# import and convert to raster stack\npredictors_A2 &lt;- stack(files)\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors_A2, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\n# fit the bioclim model\nmodel.fit &lt;- bioclim(presence)\n\n# create raster layer of predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.6: Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A2). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly compare this model to the previous two. Comment on whether you expected to see greater or smaller difference to the other future climate prediction based on the scenarios that they are based on.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether you think bioclim variables are good models to predict a species’ respone to climate change. In your discussion consider how future bioclim data sets are generated, as well as, whether abiotic conditions along will determine range changes."
  },
  {
    "objectID": "01_intro-data-science.html#visualize-and-communicate-your-results",
    "href": "01_intro-data-science.html#visualize-and-communicate-your-results",
    "title": "1  What even is Data Science?",
    "section": "1.5 Visualize and Communicate your results",
    "text": "1.5 Visualize and Communicate your results\nPrinting our maps side by side for better comparison would be a good way to visualize and communicate our results. We would probably include a discussion of our our approach (we should probably validate our model too which we haven’t done here) and make recommendations based on our findings."
  },
  {
    "objectID": "01_intro-data-science.html#acknowledgments",
    "href": "01_intro-data-science.html#acknowledgments",
    "title": "1  What even is Data Science?",
    "section": "1.6 Acknowledgments",
    "text": "1.6 Acknowledgments\nSince I’m not the first person to create a lab/tutorial on species distribution modeling, I drew inspiration from various educators, R and data enthusiasts to shape this tutorial, most notably:\nAnna L. Carter. November 2017, posting date. Painting turtles: an introduction to species distribution modeling in R. Teaching Issues and Experiments in Ecology, Vol. 13: Practice #1 [online]. http://tiee.esa.org/vol/v13/issues/data_sets/carter/abstract.html\nWendy L. Clement, Kathleen L. Prudic, and Jeffrey C. Oliver. 16 August 2018, posting date. Exploring how climate will impact plant-insect distributions and interactions using open data and informatics. Teaching Issues and Experiments in Ecology, Vol. 14: Experiment #1 [online]. http://tiee.esa.org/vol/v14/experiments/clement/abstract.html\nhttps://jcoliver.github.io/learn-r/011-species-distribution-models.html"
  },
  {
    "objectID": "02_install-R.html#learning-objectives",
    "href": "02_install-R.html#learning-objectives",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nAfter completing this activity you should\n\nbe able to download and install R and Rstudio on your laptop\nbe able to install Rtools & devtools to be able to compile R packages from source (Windows).\nunderstand the main use for each of the four main panes in the Rstudio GUI.\nunderstand what a package is in R and how to install them."
  },
  {
    "objectID": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "href": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.2 Install & Set up R and Rstudio on your computer",
    "text": "2.2 Install & Set up R and Rstudio on your computer\nIf you have already installed R and Rstudio make sure your R version is up to date. Whenever you open Rstudio the version will be printed in the console (bottom left pane). In addition, you can always check what version is installed by typing sessionInfo() into your console. You should be using version 4.0.0 or later. You do not need to uninstall old version of R. If you do have to update, you will need to re-install packages (see below) for R4.0.0\n\n2.2.1 Windows\nInstall R\n\nDownload most recent version of R for Windows here.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall Rtools\n\nDownload Rtools here.\nRun the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\nInstall Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Windows XP/Vista/7/8/10.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nFinish setting up Rtools\n\nOpen Rstudio to make sure you aren’t getting any error messages.\nPut Rtools in your path by typing writeLines('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', con = \"~/.Renviron\") in the console window.\nInstall the devtools package by typing install.packages(\"devtools\") in the console.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\n\n2.2.2 Mac OS X\nDownload & install R\n\nGo to (CRAN)[http://cran.r-project.org/], select Download R for (Mac) OS X.\nDownload the .pkg file for your OS X version.\nRun the downloaded file to install R.\n\nDownload & install XQuartz (needed to run some R packages)\n\nDownload XQuartz\nRun the downloaded file to install\n\nDownload & install Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Mac OS X.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard."
  },
  {
    "objectID": "02_install-R.html#get-to-know-rstudio",
    "href": "02_install-R.html#get-to-know-rstudio",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.3 Get to know Rstudio",
    "text": "2.3 Get to know Rstudio\nRstudio is an Integrated Development Environment (IDE) that you can use to write code, navigate files, inspect objects, etc. The advantage of using an IDE is that you have access to shortcuts, visual cues, troubleshooting, navigation, and autocomplete help.\n\n2.3.1 GUI Layout\nGUI stands for graphic user interface and refers to a type of user interface that allows users to interact with software applications and electronic devices through visual elements such as icons, buttons, windows, and menus, rather than using text-based command-line interfaces.\nYou have probably mostly interacted with computer programs through a GUI, where you interact with the system by manipulating graphical elements using a pointing device like a mouse, touch screen, or stylus. GUIs provide a more intuitive and user-friendly way for individuals to interact with computers and software because you can “see” what the effect of what you are doing is having. Graphical User Interfaces are a major departure from earlier text-based interfaces like command-line interfaces. They have contributed significantly to the widespread adoption of computers and software by making them more accessible to a broader range of users. GUIs are used in various types of software, from operating systems to applications like web browsers, image editors, word processors, and more.\nNot too long ago, if you had wanted to learn R or another programming language you would have been working directly on a console instead of an IDE like Rstudio which has made coding a lot more accessible to beginners because you can more easily use scripts, interactively run code and visualize data.\n\n\n\n\n\n\nNote\n\n\n\nUse this link to access an Rstudio IDE Cheatsheet pointing out the key features using annotated impages of the different panes. You can also download a pdf version and keep a printout handy as you get used to the GUI.\n\n\nOpen Rstudio and identify the four panes in the interface (default layout).\n\nEditor (top left): edit scripts/other documents, code can be sent directly to the console.\nR console (bottom left): Run code either by directly typing the code or sending it from the editor pane.\nEnvironment/history (top right): Contains variables/objects as you create them & full history of functions/commands that have been run.\nfiles/plots/packages/help/viewer (bottom right): Different tabs in this pane wil let you explore files on your computer, view plots, loaded packages, and read manual pages for various functions.\n\nThe panes can be customized (Rstudio -&gt; Preferences -&gt; Pane Layout) and you can move/re-size them using your mouse.\n\n\n\n\n\n\nNote\n\n\n\nWe are going to switch to have the Console in our top right and the Environment in the bottom left which makes it easier to see your code output and your script/quarto document at the same time.\n\n\n\n\n2.3.2 Interacting with R in Rstudio\nThink of R as a language that allows you to give your computer precise instructions (code) to follow.\n\nCommands are the instructions we are giving the computer, usually as a series of functions.\nExecuting code or a program means you are telling the computer to run it.\n\nThere are three main ways to interact with R - directly using console, script files (*.R), or code chunks embedded in R markdown (*.Rmd) or quarto files (*.qmd). We will generally be working with the later.\nThe console is where you execute code and see the results of those commands. You can type your code directly into the console and hit Enter to execute it. You can review those commands in the history pane (or by saving the history) but if you close the session and don’t save the history to file those commands will be forgotten.\nBy contrast, writing your code in the script editor either as a standard script or as a code chunk in an quarto document allows you to have a reproducible workflow (future you and other collaborators will thank you).\nExecuting an entire script, a code chunk, or individual functions from a script will run them in the console.\n\nCtrl + Enter will execute commands directly from the script editor. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute.\nIf you are using a quarto file you can execute an entire code chunk by pressing the green arrow in the top right corner.\n\nIf the console is ready for you to execute commands you should see a &gt; prompt. If you e.g. forget a ) you will see a + prompt - R is telling you that it is expecting further code. When this happens and you don’t know what you are missing (usually it is an unmatched quotation or parenthesis), make sure your cursor is in the console and hit the Esc key.\n\nWe will run through these options, but you can always check back here while you are getting used to R.\n\n\n\n2.3.3 Customize Rstudio\nThere are several options to customize Rstudio including setting a theme, and other formatting preferences. You can access this using Tools &gt; Global Options. I recommend using a dark theme (it’s a lot easier on the eyes) and keeping the panes in the same positions outlined above because it will make troubleshooting a lot easier1.1 “You should see xx in the top left” is a lot more helpful if your top left looks like my top left!"
  },
  {
    "objectID": "02_install-R.html#installing-and-using-packages-in-r",
    "href": "02_install-R.html#installing-and-using-packages-in-r",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.4 Installing and using packages in R",
    "text": "2.4 Installing and using packages in R\n\n2.4.1 Install a package\nThink of R packages or libraries as tool kit comprising a set of functions (tools) to perform specific tasks. R comes with a set of packages already installed that gives you base R functions; you can view these and determine which have been loaded in the Packages tab in the bottom right pane. For other tasks we will need additional packages. 22 Most R packages are found in the CRAN repository and on Bioconducter, developmental packages are available on github.\nA central group of packages for data wrangling and processing form the tidyverse, described as “… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” - We are going to heavily rely on core functions from the tidyverse to wrangle, summarize, and analyze data.\nWhen you install packages they will be downloaded and installed onto your computer. Determine what your default path is using .libPaths() and change if necessary.\nThe easiest way to install packages directly in the console is to use the install.packages() function.\nUse the R console to install some libraries to get us started (we will install other libraries as needed for other labs).\n\n\nUsing # in an R script allows you to insert comments that are ignored by R when executing your code. Use comments to document your code, future you will thank you! Before submitting any of your skills tests or homework assignments you should always go through and make sure each piece of code has a descriptive comment. You do not need to add a comment for multi-line code that you are stringing together using a pipe %&gt;% but you should have one descriptive comment above the set of commands you are giving R and then make sure that you add any comments that you need to remember how the function works or which parameters might be useful to tweak/set differently if you were to reuse that code.\n\n# install central packages in the tidyverse\ninstall.packages(\"tidyverse\")\n\n# install additional packages\ninstall.packages(\"plyr\", \"ggthemes\", \"patchwork\", \"glue\")\n\nLet’s check if you were able to successfully install those packages by ensureing you can load them. Any time you start a new R session (e.g. by closing Rstudio and restarting it), you will need to load your libraries beyond the base libraries that are automatically loaded using the library() function in order to be able to use the functions specific to that package3.3 Troubleshooting tip: if you get an error along the lines of function() cannot be found the first thing you will want to do is check if your libraries are loaded!\n\n# load library\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIf you don’t see any error messages in the console along the lines of there is no package called ... you are all set. If you look in the packages tab in the lower right panel you should also see that packages such as dplyr and tidyr (two of the central tidyverse packages) now have a little check box next to them.\n\n\n2.4.2 Updating R packages\nYou should generally make sure to keep your R packages up to date as new versions include important bugfixes and additional improvements. The easiest way to update packages is to use the Update button in the Packages tab in the bottom right panel. Over the course of the semester you should not have to do this, but when you install new packages you might get message that some of your packages need to be updated which you can then either choose to do at that point or ignore.\n\n\n\n\n\n\nWarning\n\n\n\nBe aware that updating packages might break some code you have previously written. For most of what we will be doing this should not be the case. If you used R for a previous course, make sure to update you packages at the beginning of this course and we should be set for the semester."
  },
  {
    "objectID": "03_Rbasics.html#learning-objectives",
    "href": "03_Rbasics.html#learning-objectives",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\nAfter completing this tutorial you should be able to\n\nname, create, and assign values to objects.\nsave a series of commands/code as an R script.\nuse comments to describe your code/scripts.\ncall functions and modify the default options using their arguments.\nunderstand what a vector is and distinguish between the main data types.\nto inspect, subset, and extract their content from a vector.\nunderstand how data.frames and vectors relate."
  },
  {
    "objectID": "03_Rbasics.html#r-is-all-about-objects",
    "href": "03_Rbasics.html#r-is-all-about-objects",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.2 R is all about Objects",
    "text": "3.2 R is all about Objects\nYou can think of the R console as a super-powerful calculator. You can get output from R by simply typing math directly into the console.\n\n13 + 29\n\n[1] 42\n\n\nor\n\n546 / 13\n\n[1] 42\n\n\nWell that’s fun - but not super helpful in our context.\nIn the R programming language, an object is a fundamental concept used to store, manipulate, and represent data. Everything in R is treated as an object, whether it’s a number (numeric), a text string (character), a dataset (data.frame), or even more complex data structures.\nObjects in R can be created, modified, and used to perform various operations. Objects are assigned names that you can then use to reference them in your code. When you create an object, you’re essentially creating a container that holds a value or data.\nCreating an object is straightforward. First, we give it a name, then we use the assignment operator to assign it a value.\nThe assignment operator (&lt;-) assigns the value on the right of the &lt;- to the object on the left1.1 Start building good habits starting now in terms of your coding style. For example, your code is a lot more readable if you use white space to your advantage. For example, make sure you have a space before and after your &lt;-\n\nfork_length_mm &lt;- 344\n\nType that into the console and execute the command using Enter. If you look at your Global Environment (bottom left panel) you should now see forklength and the value you assigned it.\nNotice, how when you assigned a value to your new object nothing was printed in the console compared to when you were typing in math.\nTo print the value of an object you can type the name of the object into the console.\n\n# print value\nfork_length_mm\n\n[1] 344\n\n\nNow that fork_length_mm is in your environment we can use it to compute instead of the value itself.\nFor example, we might need to convert our fork length from millimeters (mm) to centimeters (cm).\n\nfork_length_mm / 10 \n\n[1] 34.4\n\n\nWe can change the value of an object any time by assigning it a new one. Changing the value of one object does not change the values of other objects.\n\nfork_length_mm &lt;- 567\n\n\n\n\n\n\n\n Give it a whirl!\n\n\n\nCreate a new object with the fork length in centimeters. Then change then change the value of our fork length in millimeters object to 50. What do you think the value of fork_length_mm will be now?\n\n\nSome initial thoughts on naming things22 You will soon discover that coding is 90% naming things.\nTheoretically, we can name objects anything we want - but before that gets out of hand let’s think about some guidelines for naming objects.\n\nMake them simple, specific, and not too long (otherwise you will end up with a lot of typing to do and difficulties remembering which object is which).\nObject names cannot start with a number.\nR is case sensitive, fork_length is not the same as Fork_Length.\nAvoid using dots (.) in names. Typically dots are used in function names and also have special meaning (methods) in R.\nSome names are already taken by fundamental functions (e.g. if, else, for) and cannot be used as names for objects; in general avoid using names that have already been used by other function names.\nRule of thumb: nouns for object names, verbs for function names.\n\nUsing a consistent style for naming your objects is part of adopting a consistent styling of your code; this includes things like spacing, how you name objects, and upper/lower case. Clean, consistent code will make following your code a lot easier for yourself and others3.3 Remember, future you is your most important collaborator.\n\n\n\n\n\n\nNote\n\n\n\nOne of the criteria for your homework assignments and skills tests is your code style. Next to imitating the style of coding presented in this manual, you can refer to r4ds (2e) Ch 5 for some initial pointers, you can also access a short style guide here and a more detailed, tidyverse specific style guide here."
  },
  {
    "objectID": "03_Rbasics.html#saving-your-work",
    "href": "03_Rbasics.html#saving-your-work",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.3 Saving your work",
    "text": "3.3 Saving your work\nSo far, we have inputed all of our code directly into the console. If you scroll up in the console you will find that all the commands and results from your current R session are still in the console. Using Cmd/Ctrl + L will clear the entire console.\nUh-oh - what if we need to go back over the code we just cleared?\nWell, for one if you check the History tab in the top right panel you will see that all your commands have been recorded. If you highlight one of them and either click on To Console or hit Enter it will send it directly to the console.\nUsually your history will be saved automatically when you close R/end an R session (unless you have changed the settings) and it will be restored when you open R again. You can use the broom icon to clear your entire history.\nUh-oh - now what do we do?\nIn general, you should only be typing code directly into the console for quick queries or troubleshooting but since usually we want to be able to revisit and share our work you will want to be able to save your work in an R script (*.R) or include it in a quarto document (*.qmd) as a code chunk. For this course we will mostly be operating with quarto files (more on that in the next chapter).\nYou can open a new R script using Ctrl + Shift + C or using File &gt; New File &gt; R Script. This will open an R script in a new tab in the top left pane.\nSave your R script using Cmd/Ctrl + S or File &gt; Save As - this will open a dialogue box for you to save your R script with the file extension .R.\nCtrl + Enter will execute commands directly from the script editor by sending them through to the console. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute. You can also run all the code in a script by clicking on the Run button.\nCreate a new R script to keep track of the rest of the things we will learn today."
  },
  {
    "objectID": "03_Rbasics.html#using-comments",
    "href": "03_Rbasics.html#using-comments",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.4 Using comments",
    "text": "3.4 Using comments\nYou can add comments to your R scripts using #. Essentially, once you type an # in a line anything to the right of it will be ignored.\nThis is really helpful as it will allow you to comment your script, i.e. you can leave notes and explanations as to what your code is doing for future you and for other collaborators. This is especially helpful if you come back to some of your code after a period of time, if you are sharing your code with others, and when you are debugging code. You will find that as you become more experienced your comments will become shorter and more concise and you might even be tempted to leave them out completely - don’t4!4 To help you build a habit of good commenting practice, commenting your code is a requirement for your homework assignment and skills tests.\nFor example you might find a comment like this more helpful at the moment:\n\n# assign value to new object total length\nfork_length &lt;- 436\n\nBut soon you’ll find this just as helpful:\n\n# total length fish\nfork_length &lt;- 436\n\n\n\n\n\n\n\n Consider this.\n\n\n\nPredict what value of the object total_length will be after executing this command.\n\n\n\nFL &lt;- 436  # total length fish\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can comment/uncomment multiple lines at once by highlighting the lines you want to comment (or uncomment) and hitting Ctrl + Shift + C. This can be useful if you are playing around with code and don’t want to delete something but don’t want it to be run either."
  },
  {
    "objectID": "03_Rbasics.html#functions",
    "href": "03_Rbasics.html#functions",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.5 Functions",
    "text": "3.5 Functions\nWhen we installed R packages earlier we mentioned that they are sets of predefined functions. These are essentially mini-scripts that automate using specific sets of commands. So instead of having to run multiple lines of code (this can be 10s - 100s of lines code) you call the function instead.\nEach function usually requires multiple inputs (arguments) and once executed return a value (though this is not always the case).\nFor example the function round() can be used to round a number5.5 This is an excellent example of naming things well!\n\nfork_length_cm &lt;- round(34.8821)\n\nIf we print the value of our object we see the following value is returned.\n\nfork_length_cm\n\n[1] 35\n\n\nFor this function the input (argument) is a number and the returned value is also a number. This is not always the case, arguments can be numbers, objects, file paths …\nMany functions have set of arguments that alter the way a function operates - these are called options. Generally, they have a default value which are used unless specified otherwise by the user.\nYou can determine the arguments as function by calling the function args().\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nOr you can call up the help page using ?round or by typing it into the search box in the help tab in the lower right panel.\nFor example, our round() function has an argument called digits, we can use this to specify the number of significant digits we want our rounded value to have.\n\nround(34.8821, digits = 2)\n\n[1] 34.88\n\n\nIf you provide the arguments in the exact same order as they are defined you do not have to specify them.\n\nround(34.8821, 2)\n\n[1] 34.88\n\n\nHowever, if you specify the arguments, you can switch their order.\n\nround(digits = 2, x = 34.8821)\n\n[1] 34.88\n\n\n\n\n\n\n\n\nProtip\n\n\n\nGood code style is to put the non-optional arguments (frequently the object, file path or value you are using) first and then specify the names of all the optional arguments you are specifying. This provides clarity and makes it easier for yourself and others to follow your code.\n\n\nOccasionally you might even want to use comments to further specify what each argument is doing or why you are choosing a specific option.\n\nround(34.8821,     # number to round\n      digits = 2)  # specify number of significant digits\n\n[1] 34.88"
  },
  {
    "objectID": "03_Rbasics.html#vectors-data-types-i",
    "href": "03_Rbasics.html#vectors-data-types-i",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.6 Vectors (data types I)",
    "text": "3.6 Vectors (data types I)\nNow that we’ve figured out what objects and functions are let’s get to know the two data types we will be spending the most time with this semester - vectors and data frames (data.frame)6.6 Other data types include lists (list), factors (factor) matrices (matrix), and arrays (array); we’ll introduce those later on.\nThe most simple data type in R is the (atomic) vector which is a linear vector of a single type. There are six main types -\n\ncharacter: strings or words.\nnumeric or double: numbers.\ninteger: integer numbers (usually indicated as 2L to distinguish from numeric).\nlogical: TRUE or FALSE (i.e. boolean data type).\ncomplex: complex numbers with real and imaginary parts (we’ll leave it at that).\nraw: bitstreams (we won’t use those either).\n\nYou can check the data type of any object using class().\n\nclass(fork_length)\n\n[1] \"numeric\"\n\n\nCurrently, our fork_length object consists of a single value. The function c() (concatenate) will allow us to assign a series of values to an object.\n\nfork_length &lt;- c(454, 234, 948, 201)\n\nfork_length\n\n[1] 454 234 948 201\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nPredict what data type you expect this vector to be.\n\n\nWe call the same function to create a character vector.\n\nsharks &lt;- c(\"bullshark\", \"blacktip\", \"scallopedhammerhead\")\n\nclass(sharks)\n\n[1] \"character\"\n\n\nThe quotes around \"bullshark\" etc. are essential because they indicate that this is a character.\n\n\n\n\n\n\nProtip\n\n\n\nIf we do not use quotes, R will assume that we are trying to call an object and you will get an error code along the lines of “! object 'bullshark' not found”.\n\n\nYou can use c() to combine an existing object with additional elements (assuming they are the same data type).\n\nspecies &lt;- c(sharks, \"gafftop\")\n\nspecies\n\n[1] \"bullshark\"           \"blacktip\"            \"scallopedhammerhead\"\n[4] \"gafftop\"            \n\n\nNext to class() there are other helpful functions to inspect the content of a vector. For example length() will tell you how many elements are in a particular vector.\n\nlength(fork_length)\n\n[1] 4\n\n\nThe function str() will give you an overview of the structure of any object and its elements.\n\nstr(fork_length)\n\n num [1:4] 454 234 948 201\n\n\nRecall, that an atomic vector is a linear vector of a single type. Let’s explore what that means by taking a look at what happens if we create atomic vectors where we mix the data types.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe what happens when data types are mixed in a single atomic vector based on the messages generated by the code chunk below to figure out what the rules are in terms of which data type is convered to match the others when they are mixed.\n\n\n\nnumeric_character &lt;- c(1, 2, 3, \"a\")\nnumeric_logical &lt;- c(1, 2, 3, TRUE)\ncharacter_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\nwtf &lt;- c(1, 2, 3, \"4\")\n\nWe already discovered that we can combine vectors - but can we extract certain components from vectors? Indeed, there are a variety of ways that we can subset vectors.\nThe most simple way is using square brackets to indicate which element (or elements) we can’t extract. In R, indices start at 1.77 This is not the case for all programming languages, e.g. Perl, Python, or C++ start with 0.\n\n# extract second element\nspecies[2]\n\n[1] \"blacktip\"\n\n# extract fourth and second element\nspecies[c(4, 2)]\n\n[1] \"gafftop\"  \"blacktip\"\n\n\nYou can also repeat indices to create a new object with additional elements.\n\nspecies_longer &lt;- species[c(2, 2, 4, 3, 4, 4, 1, 1)]\n\nspecies_longer\n\n[1] \"blacktip\"            \"blacktip\"            \"gafftop\"            \n[4] \"scallopedhammerhead\" \"gafftop\"             \"gafftop\"            \n[7] \"bullshark\"           \"bullshark\"          \n\n\nMore frequently, we will want to extract certain elements based on a specific condition (conditional subsetting).\nThis is done using a logical vector, here TRUE select the element with the same index and FALSE will not.\n\nfork_length &lt;- c(454, 234, 948, 201)\n\n# use logical vector to subset\nfork_length[c(TRUE, FALSE, TRUE, FALSE)]\n\n[1] 454 948\n\n\nThis seems like a very impractical option. However, normally we would not create the logical vector by hand as we have done here, rather it will be the output of a function or logical test. For example, we might want to identify fish with a fork length &gt; 300mm.\n\n# identify fish with fork length &gt; threshold\nfork_length &gt; 300\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nThis creates an output the same length as the vector we looked at (fork_length) consisting of TRUE/FALSE statements for each element by comparing the each element of the vector to the condition (&gt;300) and determining if the condition is met (the statement is true) or not.\nInstead of first creating a vector of TRUE/FALSE statements can use this condition to subset our vector directly.\n\n#  identify true/false of fish with fork length &gt; threshold\nfork_length[c(fork_length &gt; 300)]\n\n[1] 454 948\n\n\nThere are a series of boolean expressions8 we can use for subsetting vectors.8 Boolean expressions are logical statements that are either true or false; most of them you are probably already familiar with because math\n\n&gt; and &lt; (greater/less than)\n=&gt; and =&lt; (equal to or greater/less than)\n== (equal to) and != (is not equal to)\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can combine to boolean expressions using &, (both conditions must be met) and | (at least one condition must be met).\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset the fork_length vector to\n\ncontain only values equal to 234\ncontain all values but 948\ncontain all values larger than 230 but smaller than 900\ncontain all values smaller than 250 or larger that 900\n\n\n\nR is set apart from other programming languages because it was designed to analyze data9 it has straightforward ways to deal with missing data (NA or na values) because those are quite common in real world data sets.9 some people will argue that it is a ‘statistical language’ rather than a true programming language … don’t listen to them, they are just jealous of your R skillz!\nLet’s create a vector with a missing value.\n\ntotal_length &lt;- c(560, NA, 1021, 250)\n\nLet’s say we want to calculate the mean value.\n\nmean(total_length)\n\n[1] NA\n\n\nMost functions will return NA when doing operations on objects with missing values. As such, many functions include an argument to omit missing values.\n\nmean(total_length, na.rm = TRUE)\n\n[1] 610.3333\n\n\nOther functions that are helpful to deal with missing values are is.na(), na.omit(), and complete.cases().\n\n\n\n\n\n\n Give this a whirl.\n\n\n\nSubset the fork_length vector to\nRun each of these functions on our total_length vector and describe what they do.”"
  },
  {
    "objectID": "03_Rbasics.html#data-frames-data-types-ii",
    "href": "03_Rbasics.html#data-frames-data-types-ii",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.7 Data frames (data types II)",
    "text": "3.7 Data frames (data types II)\nRecall that atomic vectors are linear vectors of a simple type, essentially they are one dimensional. Frequently we will be using data frames (data.frame) which you can think of as consisting of several vectors of the same length where each vector becomes a column and the elements are the rows.\nLet’s create a new object that is a dataframe with three columns containing information on species, fork length, and total length.\n\n# combine vectors into data frame\ncatch &lt;- data.frame(species, fork_length, total_length)\n\nYou should now see a new object in your Global Environment and you will now also see that there are two categories of objects Data and Values. You will see that the data.frame is described as having 4 obs (observations, those are your rows) of 3 variables (those are your columns). If you click on the little blue arrow it will give you additional information on each column - note that because each column is essentially a vector, each one must consist of a single data type which is also indicated.\nCalling the str() will give you the same information.\n\nstr(catch)\n\n'data.frame':   4 obs. of  3 variables:\n $ species     : chr  \"bullshark\" \"blacktip\" \"scallopedhammerhead\" \"gafftop\"\n $ fork_length : num  454 234 948 201\n $ total_length: num  560 NA 1021 250\n\n\nYou can further inspect the data.frame by clicking on the little white box on the right which will open a tab in the top left panel next to your R script. You can also always view a data.frame by calling the View() function.\n\nView(catch)\n\nThis can be a helpful way to explore your data.frame, for example, clicking on the headers will sort the data frame by that column. Usually we won’t build or data.frames by hand, rather we will read them in from e.g. a tab-delimited text file - but more on that later."
  },
  {
    "objectID": "04_Rproj.html#learning-objectives",
    "href": "04_Rproj.html#learning-objectives",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nAfter completing this tutorial you should\n\nbe able to set up a well structured research compendium1.\nunderstand what a working directory is and how to set up an R project.\nunderstand the value of using Rmarkdown and quarto documents to document your work.\nunderstand the core components of the markdown format.\nbe able to use basic markdown syntax to format a document (headers, bold/italics).\nbe able to add a code chunk to a .qmd file and add options.\nunderstand how to modify code chunk options to determine what is/is not rendered in the knitted document.\nknow how to render a document to produce an *.html file from an *.qmd2.\n\n1 We will at a later point in this semester revisit and determine if this truly is a well-structured folder structure2 the recent implementation of quarto has made exporting to other formats such as pdf a lot more straightforwardThe goal of open science and reproducible research is to make scientific methods, data, and results more transparent, available, and reproducible. Quarto documents written in markdown are a useful tool to be able to generate reports documenting your data, methods (code used to process data), and results. Recently quarto has been implemented as a authoring framework for data science that extends past previous use of Rmarkdown focused on R and makes it easy to include python and other coding languages.\nSimilar to R markdown (*.Rmd) files, quarto documents will let you document your workflow, share how you processed/analyzed your data and the resulting output along with any visualizations. Quarto unifies and extends the functionality of several packages that were devoloped around using R markdown unto a consistent system that supports several coding languages beyond R including python and Julia. This is a text based file format that consists of standard text, code chunks, and the resulting output using a very simple syntax (henc markdown as opposed to markup languages like html or LateX which have much more complicated syntax). When you render your document, the code is executed and the resulting output with be included in the rendered document (common formats are html or pdf). Advantages to a workflow centered around using quarto and markdown to document your work include:\n\nthe simple syntax makes it easy to learn the basics (but some of the more advance options will let you create some sophisticated reports)3.\nresulting files have a simple, standardized formatting that looks professional and is easy to read and understand (code, documentation, figures all in one place).\nfuture you will be thankful when you don’t have to remember your assumptions, individual steps, and modifications.\neasy modification to extend/refine analysis or re-run with updated data.\n\n3 Rstudio now also has a visual editor so you can really get away with knowing very little markdown, though the basics can’t hurt and knowing a few tricks like inline code will let you do some pretty cool stuff with your documents\n\n\n\n\n\nNote\n\n\n\nUse this link to access a quarto cheatsheet for a quick overview on publishing and sharing with quarto. You can also download a pdf summarizing the core quarto functionalities to keep handy as you get used to setting up quarto documents.\nSimilarly, Use this link to access a rmarkdown cheatsheet. We won’t be using Rmarkdown documents but the syntax of writing in markdown is the same. You can also download and print a pdf for easy access."
  },
  {
    "objectID": "04_Rproj.html#project-organization-101",
    "href": "04_Rproj.html#project-organization-101",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.2 Project organization 101",
    "text": "4.2 Project organization 101\nA key component to doing data analysis is organizing your working directory which will contain your data, scripts, quarto-documents, results, figures, and other output. Keeping this well organized will help you establish a reproducible workflow, collaborate well, and share your results.\n\n4.2.1 Organizing your files and directories\nFor each project/lab we will set up a project directory4 with the following set of sub-directories:4 We’ll use this term interchangeably with working directory and research compendium\n\ndata\nresults\nscr\nscratch\n\nYou will want to set up a folder (directory) locally5 on your computer called bi349 that you will use throughout this semester for all the project directories, a good place is your Documents directory or in a pinch your desktop. You will frequently download an entire research compendium with data and quarto documents “preloaded”. Macs will automatically unzip those folders on a Windows computer you will need to do this by hand (right click &gt; extract all), then move that folder into your bi329 directory. Make sure you are running your Rprojects out of the correct folder - this is one of the most common issues we run into when things aren’t working as they should.5 Local means that it is physically on your computer hard drive. If you have an automatic integration with a cloud storage service like OneDrive past experience has shown that you can run into difficulties, so yes, cloud backup is good but make sure that you are running your projects locally off your computer\n\n\n4.2.2 A note on Naming things\nNaming conventions for files, sub-directories etc. should conform to the following key principles that form the holy trinity of file naming6:6 see Jenny Bryan’s excellent summary of these principles\n\nHuman readable: keep it short but self-explanatory.\nMachine readable: don’t use special characters or spaces.\nSortable: standardize components of the file names to make it possible to sort files and find what you are looking for.\n\nApplying these principles includes conventions include sticking to all lowercase7, consistent use of _ and - instead of spaces, writing your dates as year-month-day, using leading zeros (e.g. 001, 002, … etc instead of 1, 2, ... 10, 11, 12... which will sort as 1, 11, 12, ... 2, 21, ... etc once you go into double digits).7 alternatives include uppercase or CamelCase, but since R is case sensitive this leads to mroe typos\n\n\n4.2.3 Set up your project directory using Rprojects\nIf you have not already, create a directory called bi328 locally on your computer. Make sure you know where it is, you will be adding to this directory throughout the semester.\nCreate a project directory 8 zz_skills9) within your bi328 folder, and within that directory create sub-directories data, results, scr, and scratch. Throughout the semester you will add quarto documents to this directory as you complete your weekly skills tests.8 Yes, a directory is essentially a folder, however when using the term directory we are considering the relationship between a folder and it’s full path.9 addin the zz before the folder name means that it will sort to the bottom of this directory, this is an example of sortable. Naming it skills is descriptive in terms that you know it refers to your skills tests (human readable) and using the _ makes it machine-readable, as you are avoiding issues with white spaces\nNow, we are going to create an R project within this directory.\n\nin the top right hand corner of Rstudio click on the project icon\nselect New Project and Create in existing directory\nfollow the prompts to navigate to your zz_skills directory to create a new Rproject.\n\nThis should create a new R project and open it (the R project name should be in the top right corner next to the icon).\nIf you look in the bottom left hand pane in the Files tab, the bread crumbs should lead to your project folder which has now become your working directory, i.e. all paths are relative to this location. 10 If you navigate away from your working directory (project directory) you can quickly get back to your project directory by clicking on the project icon in the Files pane or by clicking the cog icon (More) and selecting Go to Working Directory.10 If you weren’t working with an R project, you can set your working directory by navigating to your new working director and selecting More &gt; Set as working directory."
  },
  {
    "objectID": "04_Rproj.html#structure-of-an-quarto-document",
    "href": "04_Rproj.html#structure-of-an-quarto-document",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.3 Structure of an quarto document",
    "text": "4.3 Structure of an quarto document\nFor each skills test you will either be creating a quarto document your solutions or you may be asked to download a quarto document to work in. Let’s make sure you know how to create a new document and what the different component of that document are.\nCreate a new .qmd file using File -&gt; New File -&gt; Quarto Document and save that file in your project directory as Lastname_first-quarto-document.qmd.\nAn qmd-file consists of three components:\n\nHeader: written in YAML format the header contains all the information on how to render the .qmd file.\nMarkdown Sections: written in Rmarkdown syntax.\nCode chunks: Chunks of R code (or other code such as bash, python, …). These can be run interactively while generating your document and will be rendered when knitting the document."
  },
  {
    "objectID": "04_Rproj.html#yaml-header",
    "href": "04_Rproj.html#yaml-header",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.4 YAML header",
    "text": "4.4 YAML header\nThe header is written in YAML syntax, it begins and ends with ---. It will include a few default parameters. You will find that there is a wide range of parameters that you can use to customize the look of your document but for now we will add these four.\n\n---\ntitle: \"title\"\nauthor: \"name\"\ndate: \"Date\"\nformat: html\n----\n\nCustomize your .qmd by changing the title and add your name in the author line11. Changing the date to `r Sys.Date()` will automatically include the current date when you render the document instead of having to update that yourself.11 You can always do this when you start a new file, for a lot of case studies this semester you will download quarto documents where you will want to change those"
  },
  {
    "objectID": "04_Rproj.html#markdown-sections",
    "href": "04_Rproj.html#markdown-sections",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.5 Markdown sections",
    "text": "4.5 Markdown sections\nYour markdown sections can contain any text you want using the markdown syntax; once you render the .qmd the resulting (html) file will appear as text.\nMost of your text (without syntax) will appear as paragraph text but you can add additional syntax to format it in different ways.\nHere are the basics that are fairly consistent across a range of markdown flavors:\nText formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\nHeadings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n    \n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n    \nLinks and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](quarto.png){fig-alt=\"Quarto logo and the word quarto spelled in small case letters\"}\n\nTables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\n\nNote\n\n\n\nCurrent Rstudio versions do have a visual editor that is WYSIWYG12 and will allow you to format your document similar to the way you would in Google Docs or Microsoft Word. However, it is helpful to know the basics because de-bugging can be more straightforward using the Source Editor, and if you are comfortable with the syntax it is also a lot faster to format.\n\n\n12 What you see is what you get"
  },
  {
    "objectID": "04_Rproj.html#code-chunks",
    "href": "04_Rproj.html#code-chunks",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.6 Code chunks",
    "text": "4.6 Code chunks\nCode chunks contain your R code and start and end with three back ticks; {r} determines that the code chunk should be interpreted as R code.\n\n\n\nYou can type it in manually but it is a lot quicker to add a code chunk using the shortcut Ctrl + Alt + I on a Windows computer or Command + Option + I for you Mac users.\n\n\n\n\n\n\nNote\n\n\n\nYou can also insert a code chunk using Code -&gt; Insert Chunk in the tool bar, the insert button in the tab bar (little green box with C and +) or if you are in the visual editor, your can use insert button in the editor bar.\nIn short, there is no excuse not to be adding code chunks and writing code!\n\n\nYou can run (execute) entire code chunks entire chunk by clicking the Run button in the tab bar or the little green arrow in the top right corner of an R chunk.\nIt is a lot faster to use shortcuts. You can execute and entire code chunk using Cmd/Ctrl + Shift + Enter. Or if you only want to execute a certain piece of code, using Cmd/Ctrl + Enter while your cursor is placed within that code, or highlight the code you want to execute and then hit Cmd/Ctrl + Enter.\nRemember to use # to comment your code, any lines following a # will not be run by R, you can use them to describe what your code is doing. Use comments liberally to document your code, future you will thank you!\n\n\n\n\n\n\nNote\n\n\n\nBefore submitting any of your skills tests or homework assignments you should always go through and make sure each piece of code has a descriptive comment. You do not need to add a comment for multi-line code that you are stringing together using a pipe %&gt;% but you should have one descriptive comment above the set of commands you are giving R and then make sure that you add any comments that you need to remember how the function works or which parameters might be useful to tweak/set differently if you were to reuse that code.\n\n\nOptionally you can add a label to your code chunks that can be used to navigate directly to code chunks using the drop-down menu in the bottom left of the script editor.\n\n```{r}\n#| label: code-label-1\n\n1 + 1\n```\n\n[1] 2\n\n\nIf you do this for figures or tables you can start your labels with fig- or tbl- which will allow them to be automatically numbered and you can link to them later in your document. Labels cannot be repeated (i.e. they all must be unique) and cannot have spaces, best practice here would be using dashes (-) to separate words.\nYou can add options to each code chunk to customize how/if a chunk is executed and appears in the rendered output. These options are added to within the curly brackets. For example, eval: false: results in code chunk not being evaluated or run though it will still be rendered in the knitted document13. You can apply multiple options to the same chunk.13 This can be useful for you if e.g. for one of your skill tests you cannot solve one of the challenges and the document will not render because your code won’t run, this way you can show you attempt but also run the document\n\n```{r}\n#| label: code-label-2\n#| eval: false\n\n1 + 1\n```\n\nYou do have options to add figure and table captions, you can also e.g. control figure width and height. See section in r4ds (2e) Chapter 29 for a list of commonly used code options and you can find additional options here."
  },
  {
    "objectID": "04_Rproj.html#render-your-document",
    "href": "04_Rproj.html#render-your-document",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.7 Render your document",
    "text": "4.7 Render your document\nknitr is an R package used to render quarto documents to another format (usually html or pdf). In Rstudio the most straightforward way of knitting a document is using the render button in the editor toolbar. This will open a new tab in your console titled Background Jobs that will show the knitting process; any errors that occur with show up here along with a line number so you can determine where the error is occurring in your .qmd file to troubleshoot the issue. The output will automatically be saved in your working directory."
  },
  {
    "objectID": "04_Rproj.html#some-advanced-options",
    "href": "04_Rproj.html#some-advanced-options",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.8 Some advanced options",
    "text": "4.8 Some advanced options\nYou can stylize your rendered document by modifying the YAML header to include a table of contents like this14:14 the option of toc-depth determines how many levels are included in the table of contents, e.g. here headers at level 1 and 2 will be included\n---\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n---\nIf you really want to jazz things up, you can change the theme15.15 you can choose from various options here\n---\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 2\n---\nYou may be noticing some similarities between this lab manual and the documents you are producing in terms of layout … for exactly the reasons you are suspecting!"
  },
  {
    "objectID": "05_data-management.html#wait-spreadsheets",
    "href": "05_data-management.html#wait-spreadsheets",
    "title": "5  Data management using spreadsheets",
    "section": "5.1 Wait… spreadsheets?",
    "text": "5.1 Wait… spreadsheets?\nThe foundation of any research project is good data organization. This not only includes your actual data points (observations) but also things like keeping track of specimen and other samples along with all the meta-data2. Additionally, you should keep good records of how the data was produced (your methods). Thinking through ahead of time what measurements are important, i.e. what data you will record and how you will store your data is really important to make sure you are keeping track of the entire process. Good data management and clean data sets will make sharing and analyzing data a lot more straightforward.2 Metadata is data about your data. It helps describe, categorize, organize, and provide context to the main content or primary data it is associated with. It’s commonly used to provide additional information that helps users, systems, or processes understand and interpret the main data.\nWhile we are going to use R to wrangle, manipulate, analyze, and visualize data in R for a more efficient and reproducible approach compared to what can be using spreadsheets, spreadsheets are the better tool for data entry, data management/organization, and simple quality control (QC) and quality assurance (QA). Thinking ahead to how you want to organize and format your data in spreadsheets will prevent a lot of extra work and headaches down the line, especially when we plan ahead as to how we should organize it to make it more efficient to use with command-line computational tools such as R.\nSo, before we dive deep into manipulating data with R, we’ll take a small step back and think through a few fundamental rules for managing data in spreadsheets before learning how to do these and more advanced data wrangling and manipulation using R.\nWhile you can do some statistics and plotting using spreadsheet programs we will not be learning how to do this in this class. Data analysis in spreadsheets requires a lot of manual work and generally any time you want to change one parameter or if you have to update your spreadsheet with new entries or you need to apply the same analysis to another data set you end up having to redo everything by hand. The more things you do by hand, the more likely you are to make a mistake. Even if you do apply some sophisticated coding in spreadsheets and/or use it for analysis or plotting3 it is very difficult to track the exact steps or document them in a way that makes it fully reproducible for another person.3 It can be helpful to know the fundamentals for simple plots in spreadsheets for quick and dirty plotting to get a quick look at your data to get an idea of what it looks like and spot potential mistakes during data entry without having to export data and fire up R or a another command-line program."
  },
  {
    "objectID": "05_data-management.html#best-practices-for-formatting-data-in-spreadsheets",
    "href": "05_data-management.html#best-practices-for-formatting-data-in-spreadsheets",
    "title": "5  Data management using spreadsheets",
    "section": "5.2 Best practices for formatting data in spreadsheets",
    "text": "5.2 Best practices for formatting data in spreadsheets\n\n5.2.1 Format your data set for the tool you will use to analyze it with\nOur brains don’t work the same way as computers. Your spreadsheet is not a lab notebook and while a layout where there are notes in the margin, context of the experiment, or a specific layout of data might be something that you can interpret, it will more difficult for another person to forllow your through process and understand your records/notes. Another person might have the opportunity to ask follow up questions and get the clarifications they need, but a computer cannot.\nOccasionally, you might use a spreadsheet instead of a lab notebook where it is a way of keeping track of an experiment and various people interacting with samples, completing different steps etc. However, if you are using a spreadsheet for data entry and management, then you want to ensure that you have set up your spreadsheet in a way where a computer is going to be able to correctly interpret it as intended. This means that we need to think through how we want to set up spreadsheets. There are generally a few different ways you can set things up and some of them will limit how easy it is for you and/or a future collaborator to work with it down the line4.4 the optimum software/interface for data input and layout/formatting may differ depending on your intended analysis, so keep in mind how you want to be able to analyze your data and whether that will require specific formats. In general, try to pick a format that will give you the advantage of being able to easily convert it between different formats - which is something we will learn to do with R specifically in the tidyverse which centers on a specific concept of what makes data tidy.\n\n\n5.2.2 Never touch your raw data\nRaw data is the original, unaltered data that is collected or generated before any manipulation, transformation, or analysis takes place. It’s the most fundamental form of data, directly obtained from observations, measurements, or data sources. Raw data is often in its most unstructured and basic state, and it serves as the foundation for subsequent data processing and analysis.\nIn the biological and environmental sciences typical sources include direct observations made in experiments, field studies, or natural phenomena or measurments from sensors or other instruments measuring physical and chemical parameters such as temperature, GPS coordinate, pH etc.\n\n\n\n\n\n\nBe mindful\n\n\n\nNever touch your raw data. Always keep a copy of your raw data that you never modify directly.\n\n\nFor any kind of data related work it is important that you preserve the original, unaltered version of your data when conducting data analysis. Avoid making changes directly to the original data files or data set. Instead, You should work with copies of the data or use a structured workflow that ensures the integrity and reproducibility of your analysis.\nKeeping your raw data as a separate file that is never altered is important for\n\nData Integrity: Modifying the raw data directly could lead to unintended changes or loss of information. By keeping the raw data untouched, you ensure that you have a reliable source of truth to refer back to if needed.\nReproducibility: If you or others need to replicate your analysis in the future, having access to the exact original data is crucial. Changes made to the raw data could make it difficult or impossible to reproduce your results accurately.\nError Prevention: Working with copies of the raw data minimizes the risk of accidental changes or mistakes that could affect your analysis. If errors occur, you can always go back to the untouched raw data.\nData Auditing: In some cases, you might need to show the authenticity and accuracy of your data. Keeping the original data untouched allows you to demonstrate the reliability of your work.\nMultiple Analyses: If you’re working on different analyses, projects, or collaborations using the same data, maintaining the integrity of the raw data enables consistency across these efforts.\n\nBest practices of maintaining the integrity of your raw data include * making copies: Always work with copies of the original data files or datasets. This ensures that any changes you make are isolated from the raw data. * implementing a structured workflow: Develop a structured workflow that includes data cleaning, transformation, and analysis steps. Document each step thoroughly to ensure transparency and repeatability. * using version vontrol: Use version control systems like Git to track changes to your code and analysis scripts. This allows you to see how your analysis evolves over time. * creating backups: Regularly back up your data, including both the raw data and any processed versions, to prevent data loss. * creating documentation: Maintain clear and detailed documentation about the steps you’ve taken, the rationale behind your decisions, and any changes you’ve made to the data.\n\n\n5.2.3 Keep track of your formatting steps\nBy working with copies and following a structured workflow, you can ensure the accuracy, reproducibility, and integrity of your data analysis work. While you shouldn’t modify the raw data directly, it’s also important to apply necessary data preprocessing steps (like cleaning and transforming) as part of your analysis process. This means that you should do two things\n\nAny time you need to process or analyze your data make a copy instead of operating directly in your raw data5 and then create a new file with your cleaned or analyzed data.\nKeep track of the exact steps you took to clean or analyze your data6; this is just as important as keeping a detailed record of the steps you took in an experiment. Good practice would be to keep a plain text file or similar in the same folder as your data set where you record any steps you take.\n\n5 In our next lesson you are going to see that this is a key advantage of command line programs like R where you can read a raw data set into the program and then apply specific data wrangling, manipulation and analysis steps without altering the raw data.6 The second advantage of command-line programs like R is that because you are using a series of commands to wrangle and analyze your data your are automatically creating a very detailed, reproducible record of your your workflow\n\n5.2.4 Put variables in columns and observations in rows\nObservations and variables are two fundamental concepts that describe different aspects of data.\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast what an observation is compared to a variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAn observation is a single data point or unit within a data set, while a variable is a characteristic that is being measured or observed for each of those data points.\nTogether, observations and variables make up the structure of a data set, where each observation has values for the various variables being measured.\n\n\n\nHere is the key rule for structuring data in spreadsheet:\nEvery variable gets its own column, every observation gets its own rule, do not combine multiple pieces of information in one cell7.7 This is what we will refer to as tidy data. We will explore this concept more in depth down the line and will play “Is it tidy?” regularly this semester.\n\n\n5.2.5 Export data as text-based formats\nWhile it is a lot easier to enter and look at data in a spreadsheet you should always export your raw and cleaned data set as a text-based format such as CSV, TSV, JSON, XML, etc.).\nThis offers several advantages compared to proprietary binary formats8:8 an example would be Excel’s .xlsx\n\nInteroperability: Text-based formats are widely supported by various software and programming languages. This means that data can be easily shared and integrated into different applications and systems, regardless of the software being used. Especially if program have proprietary formats having a format that is platform independent is really important.\nSimplicity: Text-based formats have a simple, human-readable structure. This makes it easier to understand the data’s content, and it allows manual inspection and editing using basic text editors.\nVersion Control: Text-based formats work well with version control systems (e.g., Git). Since changes can be easily tracked in plain text, it’s easier to collaborate, review, and manage changes made to the data. & Data Integrity: Text-based formats are less prone to corruption and data loss. Proprietary binary formats can sometimes become corrupted, making data recovery difficult.\nPlatform Independence: Text-based formats are platform-independent. They can be used on different operating systems without compatibility issues\nReduced File Size: Text-based formats generally have smaller file sizes compared to their binary counterparts. This can be advantageous for sharing and storage, especially when dealing with large datasets.\nAutomation and Scripting: Text-based formats are well-suited for automation and scripting tasks. Many programming languages have libraries and tools to read and write data from these formats easily.\nData Analysis: Text-based formats can be directly used in data analysis workflows. Data scientists and analysts often use tools like Python, R, and SQL to work with text-based data formats.\nData Accessibility: When sharing data with others, especially outside your organization, text-based formats offer a universal way to provide data that can be imported into various tools without compatibility issues."
  },
  {
    "objectID": "05_data-management.html#common-spreadsheet-formatting-issues",
    "href": "05_data-management.html#common-spreadsheet-formatting-issues",
    "title": "5  Data management using spreadsheets",
    "section": "5.3 Common spreadsheet formatting issues",
    "text": "5.3 Common spreadsheet formatting issues\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe common formatting mistakes formatting data in spreadsheets, explain why it might be tempting to format data in this way and why it might cause downstream issues for data analysis.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere are the key points you will want to discuss:\n\nmultiple tables in one tab\ndata spread across multiple tabs\nnot filling in zeros\nusing problematic null values for missing data\nusing formatting to convey information\nusing formatting to make the data sheet look pretty\nplacing comments or units in cells\nentering more than one piece of information per cell\nusing problematic field/column names\nusing special characters in data\nincluding metadata in the data table\ndate formatting"
  },
  {
    "objectID": "05_data-management.html#dates-are-data-but-like-the-worst-kind",
    "href": "05_data-management.html#dates-are-data-but-like-the-worst-kind",
    "title": "5  Data management using spreadsheets",
    "section": "5.4 Dates are data (but like, the worst kind)",
    "text": "5.4 Dates are data (but like, the worst kind)\nProbably the most intuitive way to store dates in a spreadsheet would be to create a column called date and then just store your dates in there.\n\n\n\n\n\n\n Consider this\n\n\n\nQuick. Off the top of your head come up with 10 ways that you could format a date.\n\n\nSo that’s the first problem - what should a date even look like? Problem two is that while to you the human this would be the most natural way to do this, the spreadsheet might be displaying it in a way that makes sense to you but is actually storing it in a very different format. Additionally, different spreadsheet programs (Microsoft Excel, Google Sheets, LibreOffice, OpenOffice) might be storing and handling dates slightly different from each other. In this case the date functions valid for one might be only somewhat-ish compatible with each other. Additionally, spreadsheet programs generally are trying to automatically recognize dates so e.g. gene/protein names like MAR1, OCT4 would be interpreted to dates and getting the original identifier back might be tricky.\nAdditionally spreadsheets sometimes try to be especially helpful by autocompleting information.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOpen the exercise-dates Google Sheets document in the scratch folder of the 01_SharkNurseries folder. Label cell A1 as location and cell B1 as date_sample-1. In cell A2-16 type in A, B, … . Then in cell B2 type a date as just month/day. Hit enter, then click back on the cell and look at the value bar at the top. Describe what you observed and how this “helpful” behavior could lead to data entry problems.\n\n\nYou can switch between different data formats by customizing the format of the cell.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn your Google Sheets document type a few dates in cells B3 - B6. Next, highlight column A. and click on the 123 button in the tool bar. Select Custom Date and Time and describe what type of formatting options you have. Pick one, and see how it changes how the content of your cells is displayed.\n\n\nFor some of the more elaborate formats if you look at the value bar you will notice that even though the content of your cell has changed in terms of how it is formatted, that value might not necessarily match the cell content. How does the spreadsheet program so easily convert between all the different formats?\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s assume that you revisited every site 15 days after the initial visit. In your Google Sheets document type date_sample2 in cell C1. Now in cell C2 type B2 + 15. Describe what happens.\n\n\nWait? Since when can you just add an integer and a date? Aren’t those completely different data formats?\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHighlight column B. Now Click on the 123 button in the toolbar and select Automatic. Describe what you see. Speculate what this means about how spreadsheet programs actually store dates and what implications this could have if you export spreadsheets as text files.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGoogle Sheets is actually storing dates as integers from a default day of December 31, 1899. This can be useful, spreadsheet programs were initially developed for and most heavily used for accounting, so the option of being able to in a straightfoward way add days, months, or years to a given date is quite practical.\nHowever, if you export as a text file you can run into the issue that you text file now has a column with an integer where you expected a date.\nAs a side note it also becomes problematic if you are using dates before December 31, 1899 because it cannot parse them correctly.\n\n\n\nIt is a lot safer to store dates in spreadsheets not as date but rather in three columns as year, month, day. Another option is to use Julian Day or day of the year. Or you can store as a single string as YYYYMMDD you can do the same for time stamps as YYYYMMDDhhmmss. This has advantages in terms of sorting by assending and descending order and you do not have to worry about converting to text.\nIf you were to read this format into R it would initially think that it is an integer, however there are functions that we can use to tell R that it is actually a date and what format it is, and then we would be able to apply functions to extract year, month etc into other programs.\nIn sum, treat dates as multiple pieces of data to make them easier to handle downstream."
  },
  {
    "objectID": "05_data-management.html#quality-assurance-and-control",
    "href": "05_data-management.html#quality-assurance-and-control",
    "title": "5  Data management using spreadsheets",
    "section": "5.5 Quality assurance and control",
    "text": "5.5 Quality assurance and control\nYou will frequently hear people say something along the lines of “oh we still have to QC the data” or “we need to complete QA/QC before we can analyze the data. QA stands for quality assurance and QC stands for quality control and both processes are critical to ensure that data being used moving forward is accurate, reliable and valid.\nQuality assurance focuses on preventing errors and ensuring that the proceses used to generate and enter the data are effective and efficient and minimize error. It involves establishing guidelines, standards, and best practices to be followed during the processes. The goal is to identify and address potential issues before they can occur or at least as early as possible in the process.\nBy contrast, quality control focuses on identifying errors that may have occured during the process of generating and entering data by performing checks and tests at various stages of the process to verify that the final output meets the predefined quality standards.\nEnsuring that we have accurate and consistent data collection methods, checking for and removing or correcting data entry errors, and validating data against predefined criteria is a critical step in (data) science. It is important that you keep a good record of the steps you took, rules you apply to discern “good” vs “bad” errors, and which data was removed to ensure transparency and repeatability.\nOne important component of quality assurance is stopping from bad data being entered in the first place by creating a list of valid values which will then prohibit false values from being entered. For example, we might be working with different types of gear to catch sharks at each location, longlines, gillnets, and hook-and-line. It would be easy to accidentally mistype one of these gear types or forget whether we are entering everything lowercase or using capitalization.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn your Google Sheet in column D type gear. Then place your cursor in cell D2 and navigate to Data &gt; Data Validation which will bring up a dialog on the right sight of your screen. Click on Add rule.\nIn the Apply to range box it will currently say Sheet1!D2, you can extend this to include all cells from D2 to D6 by modifying this to Sheet1!D2:D6 (or by marking the area in the spreadsheet). Click on the Criteria Dropdown menu to see how many different types of options you have to set rules in terms of what is allowed to be entered into the cells to which you are applying this rule.\nWe are going to use a Dropdown. By default you will have two fields. Fill those out as longline and gillnet. Then click on add another item and add hook-and-line.\nCheck out the advance options which allows you to change whether you just get a warning or the input is rejected if your entry is invalid, you can also play with the display style to see how that affects the formatting and ease of use. Then click Done.\nWe have a short list of options so you you can easily see all three and select the correct one. For longer lists it is more helpful that you can start typing in you data and then select it.\n\n\nYou can see how this option is helpful for categorical data where typos are an issue. But you can also restrict dates to certain time periods or numbers to certain values.\nUsing these types of rules help minimize errors, however it is almost inevitable that something will sneak in eventually which is where quality control comes in.\n\n\n\n\n\n\nBe mindful\n\n\n\nRemember, before you implement any quality control measure you want to make sure that you make a copy of your data and save the original data as your raw, unaltered data set. You will want to make sure that the file name reflects that it is your raw data.\nCreate a separate file that you will then clean, make sure that your filename includes some sort of versioning and/or a date so you have a good record of when you processed a data set. Then you want to make sure that your data are all values and not formulas which refer to specific cells. Once you start moving cells around this can screw with your data.\nYou will also want to create a text-file (a typical filename would be README) that keeps track of all your files and manipulations so that future you or a collaborator can easily understand and replicate any steps that you take.\n\n\nThe goal of QC is to find erroneous data. This means that it is generally going to stick out from the rest of the values in a specific column9.9 Errors are not the same as outliers. Sometimes you know that e.g. certain values cannot be true, for example if all your sample locations where in the northern hemisphere then you cannot have latitudes from the southern hemisphere.\nWe can generally make the assumption that the vast majority of your data is correct. This means that if we sort the values in a column if there are a few errors they will stick out and in many cases they will sort at the very top or very bottom. For example, if your column is numeric anything that is a character will pop out or if you have null values or empty cells they will generally sort to the bottom of a column.\n\n\n\n\n\n\nBe mindful\n\n\n\nAny time you are going to sort data, make sure that you are sorting the entire dataset not just a single column or you will corrupt your data set and everything will end up scrambled.\nGenerally, if you don’t have any empty columns or too much missing data if you place the cursor in a cell with a value you can use the shortcut Ctrl/Cmd + A to select all.\nAlways double check that you have expanded your sort to the entire data set\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn the scratch folder, open up the catchdata_messy Google Sheet. Then sort each column and see which errors you can spot.\nMake the entire data set is highlighted. Then go select Data &gt; Sort range &gt; Advanced sorting options. Make sure you check the Data has header row option. Then use the sort by drop down menu to select the column you want to sort. Once you are ready, cleock sort.\nThen inspect your column to determine if there are unexpected values and describe your observations. Discuss what you will do with you findings - consider whether it is better (more ethical/responsible) to remove them or correct them.\n\n\nSimilarly, we can use conditional formatting which allows you to apply specific rules for automatically color coding to a column based on specific rules. This makes it easier for unusual entries or entries outside the possible boundaries to stand out.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn the catchdata_messy Google Sheet, highlight the STL column. The select Format &gt; Conditional formatting from the toolbar. This will pull up a dialog on the right hand of your window.\nSimilar to the Data Validation dialog, you can select the range you want to apply this rule to either by typing it in or selecting it directly in the spreadsheet.\nClick on the Format rules dropdown menus and look through the available options. Let’s say that we know that the sharks cannot be smaller than 50cm or larger than 2m. Set up rules for conditional highlighting that will allow you to quickly pull out invalid entries.\nClick Done once you are all set and evaluate your results.\n\n\nEspecially for smaller data sets being able to quickly scan for errors can be really helpful, however, down the line we will also learn how to use similar principles to identify errors using R.\n\n\n\n\n\n\n Consider this\n\n\n\nArgue the pros and cons of doing QA/QC directly in the spreadsheet compared to using a command-line program like R."
  },
  {
    "objectID": "05_data-management.html#exporting-data",
    "href": "05_data-management.html#exporting-data",
    "title": "5  Data management using spreadsheets",
    "section": "5.6 Exporting data",
    "text": "5.6 Exporting data\nGenerally, want to make sure that we are storing our data in a universally accessible, open, and static format rather than e.g. the default Excel file format (*.xls or *.xlsx).\n\nExcel files have a proprietary format and it is possible that in the future technology will change and you will no longer be able to access your files.\nother program may not be able to read Excel formatted files.\ndifferent version of Excel may handle data differently which can lead to inconsistencies.\nfrequently journals or grant agencies require you to deposit your data in a data repository that only accepts certain formats which may not include Excel.\n\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether you think Google Sheets has the same problems or if it is an acceptable format to avoid these issues.\n\n\nText-based formats such as comma-delimited (*.csv) or tab-delimited (*.txt or *.txv) files overcome these issues. In CSV files, columnes are separated by commas and in tab-delimited files by tabs10. The advantage of text files is that they can be opened in any plain text editors11 but you can also import them into spreadsheet programs or command-line programs like R.10 This will look like whitespace if you look at it in a text editor, but tabs, but using whitespace can cause issues when command-line files parse them, tabs are less ambiguous11 Your operating system will have a built in plain text editors such as notepad. However, you are regularly operating with textfiles it can be helpful to have a more powerful program like Notepad++ or Atom.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOpen the Excel spreadsheet longline_catchdata.xlsx in the data folder of the project directory you downloaded.\nSelect File &gt; Save as from the ribbon, then select Comma Separated Values (*.csv) from the list. Double check the file location and name then click Save.\nNow, repeat the same process to export as a tab-delimited file. You will have multiple options to export as text file, make sure that it says tab-delimited.\nOpen both files in a text editor and compare them. If you double click on a *.csv file your computer will typically open it in excel, you may need to right click and then select open with to open in a text editor.\n\n\nYou will find that I habitually use tab-delimited files because in Germany we use a , instead of a . for our decimals which means that data values can include commas and therefore exporting as *.csv files can cause a bit of chaos. However, as you pick a data set for your course project you will more likely end up with *.csv files.\n\n\n\n\n\n\nTip\n\n\n\nGoogle Sheets now make it a lot easier to export and download copies of spreadsheets in different formats including *.csv by selecting File &gt; Download from the main toolbar and the choosing comma-delimited or tab-delimited from the drop down menu."
  },
  {
    "objectID": "05_data-management.html#acknowledgments",
    "href": "05_data-management.html#acknowledgments",
    "title": "5  Data management using spreadsheets",
    "section": "5.7 Acknowledgments",
    "text": "5.7 Acknowledgments\nThis chapter is adapted from data carpentries “Data Organization in Spreadsheets for Ecologists lesson."
  },
  {
    "objectID": "06_data-frames.html#reading-data-into-r",
    "href": "06_data-frames.html#reading-data-into-r",
    "title": "6  Intro to dataframes",
    "section": "6.1 Reading data into R",
    "text": "6.1 Reading data into R\nlibrary(tidyverse) is actually loading a set of packages used for data science that share a common design philosophy, and “grammar”. One of the packages we loaded is called readr which contains functions for reading in and parsing files.\nAt the end of Chapter 5 when we were exploring the usefulness of spreadsheets for data entry and management. You would have export the excel file with the catch data as *.csv (comma delimited) and as a tab-delimited text file (*.txt or *.tsv if exporting from google sheets).\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse ?read_delim to pull up the help page for the function we will using and explore the arguments. How do you think we read in our csv file?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nread_delim has two required arguments, the path (data/longline_catchdata.csv) which tells R where your file is located and the delimiter in this case a comma (,) tells R how columns are separated from each other.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.csv\", delim = \",\")\n\nAs we access data sets that are not as “clean” as the one we have here, you will find that some of the other arguments apart from specifying the delimiter will come in handy - but don’t worry about those for now.\n\n\n\nExecute the code. If you look over in your environment pane you should now see the object catch. This is your dataframe. Click on it, you should see the command View(catch) in your console and a tab catch appear in your top left pane.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nBased on how you read in the csv file how would you read in the tab-delimited version?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to change delimiter to indicate that it is tab-delimited. In this case, we would specify it as \\t is “computer” for tab.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\nAnother delimiter you might encounter are white space (\" \") but technically it could be anything.\n\n\n\nWhen you loaded your data set you should have seem an message along the lines of parsed with column specification and information on the number of columns and their data type. What this means is that read_delim() looks through the first 1,000 rows for each column and guesses the data type - usually this works pretty well though occasionally we will have to either specify the data types manually using the col_types argument or convert the data type later on.\nLet’s use class() to figure out what type of object we are dealing with.\n\nclass(catch)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nYou can see that this object actually has multiple classes attached to it. The last one in the list is data.frame which is the standard format for (rectangular) tabular data.\nRecall from our tutorial on vectors that each column in a data.frame is an atomic vector, they must all have the same length (hence, “rectangular”) and each must contain the same data type (characters, integers, …).\nThe other three have the same basic properties as a data.frame along with some additional features. The tbl (pronounced tibble) was designed to be at the center of the tidyverse which means that when you use readr functions it will automatically be read in as a tibble and data.frame. If you do some exploring and/or troubleshooting on the web you will likely run into tibbles but for our intents of purposes we will use data.frame when talking about data in a rectangual, tabular shape."
  },
  {
    "objectID": "06_data-frames.html#inspecting-your-data.frame",
    "href": "06_data-frames.html#inspecting-your-data.frame",
    "title": "6  Intro to dataframes",
    "section": "6.2 Inspecting your data.frame",
    "text": "6.2 Inspecting your data.frame\nYou have of course already peaked at the data when you opened it in excel to export it in a text-based format. But not infrequently, you may access data from a public database or a collaborator might share a text-based formatted data set with you and the first thing that you are going to want to do is figure out what information is contained in the data set.\n\n\n\n\n\n\n Consider this\n\n\n\nYou know that this data set is the result from a long-lining survey and you’re now basically an expert in formatting data - what information do you expect to find in this data set? How would you expect it to be formatted if this is a ‘tidy data set’.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThere are several functions that you can use for a preliminary inspection of your data, including figuring out what dimensions it has and what information is contained in your data set.\nCall the following functions on your object and describe what each function does, what information you can learn about your data set from executing them, and when these could be helpful.\n\ndim(), nrow(), ncol()\nstr(), summary()\nhead(), tail()\ncolnames(), rownames()\nView()"
  },
  {
    "objectID": "06_data-frames.html#subsetting-your-dataframe",
    "href": "06_data-frames.html#subsetting-your-dataframe",
    "title": "6  Intro to dataframes",
    "section": "6.3 Subsetting your dataframe",
    "text": "6.3 Subsetting your dataframe\nSimilar to the way we were able to subset vectors, we can do the same things with our data.frames using rows and columns as our “coordinates” in the format data_frame[row_index, column_index].\n\n6.3.1 Using coordinates\nSo for example we can extract the first row and column from our catch object as\n\ncatch[1, 1]\n\n# A tibble: 1 × 1\n  Site       \n  &lt;chr&gt;      \n1 Aransas_Bay\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you extract the 5th column from the third row?\n\n\nYou can select multiple rows or columns by specifying them using a vector.\n\ncatch[c(1, 20, 40), c(2, 5)]\n\n# A tibble: 3 × 2\n  Species         PCL\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Bagre_marinus    NA\n2 Bagre_marinus    NA\n3 Bagre_marinus    NA\n\n\nYou can also select a set of adjacent rows (columns) using : as so\n\ncatch[500:505, 2:5]\n\n# A tibble: 6 × 4\n  Species       Sex   Observed_Stage   PCL\n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Bagre_marinus U     &lt;NA&gt;              NA\n2 Bagre_marinus U     &lt;NA&gt;              NA\n3 Bagre_marinus U     &lt;NA&gt;              NA\n4 Bagre_marinus U     &lt;NA&gt;              NA\n5 Bagre_marinus U     &lt;NA&gt;              NA\n6 Bagre_marinus U     &lt;NA&gt;              NA\n\n\nYou can exclude indices using -\n\ncatch[1:5, -1]\n\n# A tibble: 5 × 11\n  Species     Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day Month\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bagre_mari… U     &lt;NA&gt;              NA   287   353        10     1    28     7\n2 Bagre_mari… U     &lt;NA&gt;              NA   425   495        10     1    28     7\n3 Bagre_mari… U     &lt;NA&gt;              NA   416   502        15     1    28     7\n4 Bagre_mari… U     &lt;NA&gt;              NA   416   507        10     1    28     7\n5 Bagre_mari… U     &lt;NA&gt;              NA   418   510        15     1    28     7\n# ℹ 1 more variable: Year &lt;dbl&gt;\n\n\nYou can select all columns of a given row by leaving the column index blank; for example if we want to extract the first row.\n\ncatch[1, ]\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you extract the entire 5th column?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are quite a few ways to do this:\n\nyou can use indices by number (as we have done up until this point)\ninstead of index numbers you can use the column name\n\nHere are two options using indices:\n\ncatch[, 1]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\ncatch[1]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\n\nInstead of using indices you can also call their column names directly - both of these options will return a data.frame.\n\ncatch[\"Site\"]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\ncatch[, \"Site\"]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\n\n\n\n\n\n\n6.3.2 Extracting columns as vectors\nUsing [] will always return a subset of your dataframe as a data frame. Occassionally, we might want to extract the column as a vector. You can do this using square brackets [[]] or $.\n\ncatch[[\"Site\"]]\n\n   [1] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [4] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [7] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [10] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [13] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [16] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [19] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [22] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [25] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [28] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [31] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [34] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [37] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [40] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [43] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [46] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [49] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [52] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [55] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [58] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [61] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [64] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [67] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [70] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [73] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [76] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [79] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [82] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [85] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [88] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [91] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [94] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [97] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [100] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [103] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [106] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [109] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [112] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [115] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [118] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [121] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [124] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [127] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [130] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [133] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [136] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [139] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [142] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [145] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [148] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [151] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [154] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [157] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [160] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [163] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [166] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [169] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [172] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [175] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [178] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [181] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [184] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [187] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [190] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [193] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [196] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [199] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [202] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [205] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [208] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [211] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [214] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [217] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [220] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [223] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [226] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [229] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [232] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [235] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [238] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [241] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [244] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [247] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [250] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [253] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [256] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [259] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [262] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [265] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [268] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [271] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [274] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [277] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [280] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [283] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [286] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [289] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [292] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [295] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [298] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [301] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [304] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [307] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [310] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [313] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [316] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [319] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [322] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [325] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [328] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [331] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [334] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [337] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [340] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [343] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [346] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [349] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [352] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [355] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [358] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [361] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [364] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [367] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [370] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [373] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [376] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [379] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [382] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [385] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [388] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [391] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [394] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [397] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [400] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [403] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [406] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [409] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [412] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [415] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [418] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [421] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [424] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [427] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [430] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [433] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [436] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [439] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [442] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [445] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [448] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [451] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [454] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [457] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [460] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [463] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [466] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [469] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [472] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [475] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [478] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [481] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [484] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [487] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [490] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [493] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [496] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [499] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [502] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [505] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [508] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [511] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [514] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [517] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [520] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [523] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [526] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [529] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [532] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [535] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [538] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [541] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [544] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [547] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [550] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [553] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [556] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [559] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [562] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [565] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [568] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [571] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [574] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [577] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [580] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [583] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [586] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [589] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [592] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [595] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [598] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [601] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [604] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [607] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [610] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [613] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [616] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [619] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [622] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [625] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [628] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [631] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [634] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [637] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [640] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [643] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [646] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [649] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [652] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [655] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [658] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [661] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [664] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [667] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [670] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [673] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [676] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [679] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [682] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [685] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [688] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [691] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [694] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [697] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [700] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [703] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [706] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [709] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [712] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [715] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [718] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [721] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [724] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [727] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [730] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [733] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [736] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [739] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [742] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [745] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [748] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [751] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [754] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [757] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [760] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [763] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [766] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [769] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [772] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [775] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [778] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [781] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [784] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [787] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [790] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [793] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [796] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [799] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [802] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [805] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [808] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [811] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [814] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [817] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [820] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [823] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [826] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [829] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [832] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n [835] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [838] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [841] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [844] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [847] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [850] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [853] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [856] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [859] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [862] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [865] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [868] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [871] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [874] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [877] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [880] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [883] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [886] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [889] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [892] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [895] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [898] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [901] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [904] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [907] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [910] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [913] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [916] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [919] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [922] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [925] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [928] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [931] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [934] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [937] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [940] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [943] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [946] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [949] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [952] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [955] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [958] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [961] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [964] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [967] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [970] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [973] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [976] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [979] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [982] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [985] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [988] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [991] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [994] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [997] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1000] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1003] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1006] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1009] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1012] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1015] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1018] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1021] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1024] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1027] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1030] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1033] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1036] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1039] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1042] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1045] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1048] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1051] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1054] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1057] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1060] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1063] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1066] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1069] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1072] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1075] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1078] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1081] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1084] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1087] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1090] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1093] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1096] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1099] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1102] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1105] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1108] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1111] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1114] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1117] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1120] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1123] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1126] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1129] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1132] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1135] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1138] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1141] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1144] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1147] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1150] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1153] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1156] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1159] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1162] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1165] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1168] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1171] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1174] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1177] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1180] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1183] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1186] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1189] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1192] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1195] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1198] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1201] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1204] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1207] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1210] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1213] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1216] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1219] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1222] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1225] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1228] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1231] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1234] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1237] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1240] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1243] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1246] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1249] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1252] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1255] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1258] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1261] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1264] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1267] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1270] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1273] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1276] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1279] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1282] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1285] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1288] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1291] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1294] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1297] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1300] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1303] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1306] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1309] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1312] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1315] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1318] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1321] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1324] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1327] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1330] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1333] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1336] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1339] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1342] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1345] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1348] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1351] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1354] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1357] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[1360] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1363] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1366] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1369] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1372] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1375] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1378] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1381] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1384] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1387] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1390] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1393] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1396] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1399] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1402] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1405] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1408] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1411] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1414] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1417] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1420] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1423] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1426] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1429] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1432] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1435] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1438] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1441] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1444] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1447] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1450] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1453] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1456] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1459] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1462] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1465] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1468] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1471] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1474] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1477] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1480] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1483] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1486] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1489] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1492] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1495] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1498] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1501] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1504] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1507] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1510] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1513] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1516] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1519] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1522] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1525] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1528] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1531] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1534] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1537] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1540] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1543] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1546] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1549] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1552] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1555] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1558] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1561] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1564] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1567] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1570] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1573] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1576] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1579] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1582] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1585] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1588] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1591] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1594] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1597] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1600] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1603] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1606] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1609] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1612] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1615] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1618] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1621] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1624] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1627] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1630] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1633] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1636] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1639] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1642] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1645] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1648] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1651] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1654] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1657] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1660] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1663] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1666] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1669] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1672] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1675] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1678] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1681] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1684] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1687] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1690] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1693] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1696] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1699] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1702] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1705] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1708] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1711] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1714] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1717] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1720] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1723] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1726] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1729] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1732] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1735] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1738] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1741] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1744] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1747] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1750] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1753] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1756] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1759] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1762] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1765] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1768] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1771] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1774] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1777] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1780] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1783] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1786] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1789] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1792] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1795] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1798] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1801] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1804] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1807] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1810] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1813] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1816] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1819] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1822] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1825] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1828] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1831] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1834] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1837] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1840] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1843] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1846] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1849] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1852] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1855] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1858] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1861] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1864] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1867] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1870] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1873] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1876] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1879] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1882] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1885] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1888] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1891] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1894] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1897] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1900] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1903] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1906] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1909] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1912] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1915] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1918] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1921] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1924] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1927] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1930] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1933] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1936] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1939] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1942] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1945] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1948] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1951] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1954] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1957] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1960] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1963] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1966] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1969] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1972] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1975] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1978] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1981] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1984] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1987] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1990] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1993] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1996] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1999] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2002] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2005] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2008] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2011] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2014] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2017] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2020] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2023] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2026] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2029] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2032] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2035] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2038] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2041] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2044] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2047] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2050] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2053] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2056] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2059] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2062] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2065] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2068] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2071] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2074] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2077] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2080] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2083] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2086] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2089] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2092] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2095] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2098] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2101] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2104] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2107] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2110] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2113] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2116] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2119] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[2122] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2125] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2128] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2131] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2134] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2137] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2140] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2143] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2146] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2149] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2152] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[2155] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2158] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2161] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2164] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2167] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2170] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2173] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2176] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2179] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2182] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2185] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2188] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2191] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2194] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2197] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2200] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2203] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2206] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2209] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2212] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2215] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2218] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2221] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2224] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2227] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2230] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2233] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2236] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2239] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2242] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2245] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2248] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2251] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2254] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2257] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2260] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2263] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2266] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2269] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2272] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2275] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2278] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2281] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2284] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2287] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2290] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2293] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2296] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2299] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2302] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2305] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2308] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2311] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2314] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[2317] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2320] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2323] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n\ncatch$Site\n\n   [1] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [4] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [7] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [10] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [13] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [16] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [19] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [22] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [25] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [28] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [31] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [34] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [37] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [40] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [43] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [46] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [49] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [52] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [55] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [58] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [61] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [64] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [67] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [70] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [73] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [76] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [79] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [82] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [85] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [88] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [91] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [94] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [97] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [100] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [103] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [106] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [109] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [112] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [115] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [118] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [121] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [124] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [127] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [130] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [133] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [136] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [139] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [142] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [145] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [148] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [151] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [154] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [157] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [160] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [163] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [166] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [169] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [172] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [175] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [178] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [181] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [184] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [187] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [190] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [193] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [196] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [199] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [202] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [205] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [208] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [211] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [214] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [217] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [220] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [223] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [226] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [229] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [232] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [235] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [238] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [241] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [244] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [247] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [250] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [253] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [256] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [259] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [262] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [265] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [268] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [271] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [274] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [277] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [280] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [283] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [286] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [289] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [292] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [295] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [298] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [301] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [304] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [307] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [310] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [313] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [316] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [319] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [322] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [325] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [328] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [331] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [334] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [337] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [340] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [343] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [346] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [349] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [352] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [355] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [358] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [361] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [364] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [367] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [370] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [373] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [376] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [379] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [382] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [385] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [388] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [391] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [394] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [397] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [400] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [403] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [406] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [409] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [412] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [415] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [418] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [421] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [424] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [427] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [430] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [433] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [436] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [439] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [442] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [445] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [448] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [451] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [454] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [457] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [460] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [463] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [466] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [469] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [472] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [475] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [478] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [481] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [484] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [487] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [490] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [493] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [496] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [499] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [502] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [505] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [508] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [511] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [514] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [517] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [520] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [523] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [526] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [529] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [532] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [535] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [538] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [541] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [544] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [547] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [550] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [553] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [556] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [559] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [562] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [565] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [568] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [571] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [574] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [577] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [580] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [583] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [586] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [589] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [592] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [595] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [598] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [601] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [604] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [607] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [610] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [613] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [616] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [619] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [622] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [625] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [628] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [631] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [634] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [637] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [640] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [643] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [646] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [649] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [652] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [655] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [658] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [661] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [664] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [667] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [670] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [673] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [676] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [679] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [682] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [685] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [688] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [691] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [694] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [697] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [700] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [703] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [706] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [709] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [712] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [715] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [718] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [721] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [724] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [727] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [730] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [733] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [736] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [739] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [742] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [745] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [748] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [751] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [754] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [757] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [760] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [763] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [766] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [769] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [772] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [775] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [778] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [781] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [784] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [787] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [790] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [793] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [796] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [799] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [802] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [805] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [808] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [811] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [814] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [817] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [820] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [823] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [826] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [829] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [832] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n [835] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [838] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [841] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [844] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [847] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [850] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [853] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [856] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [859] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [862] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [865] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [868] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [871] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [874] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [877] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [880] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [883] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [886] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [889] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [892] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [895] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [898] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [901] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [904] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [907] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [910] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [913] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [916] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [919] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [922] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [925] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [928] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [931] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [934] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [937] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [940] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [943] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [946] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [949] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [952] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [955] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [958] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [961] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [964] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [967] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [970] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [973] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [976] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [979] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [982] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [985] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [988] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [991] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [994] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [997] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1000] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1003] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1006] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1009] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1012] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1015] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1018] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1021] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1024] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1027] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1030] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1033] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1036] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1039] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1042] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1045] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1048] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1051] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1054] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1057] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1060] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1063] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1066] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1069] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1072] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1075] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1078] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1081] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1084] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1087] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1090] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1093] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1096] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1099] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1102] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1105] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1108] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1111] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1114] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1117] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1120] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1123] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1126] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1129] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1132] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1135] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1138] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1141] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1144] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1147] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1150] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1153] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1156] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1159] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1162] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1165] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1168] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1171] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1174] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1177] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1180] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1183] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1186] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1189] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1192] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1195] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1198] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1201] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1204] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1207] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1210] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1213] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1216] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1219] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1222] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1225] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1228] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1231] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1234] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1237] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1240] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1243] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1246] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1249] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1252] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1255] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1258] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1261] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1264] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1267] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1270] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1273] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1276] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1279] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1282] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1285] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1288] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1291] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1294] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1297] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1300] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1303] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1306] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1309] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1312] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1315] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1318] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1321] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1324] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1327] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1330] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1333] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1336] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1339] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1342] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1345] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1348] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1351] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1354] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1357] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[1360] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1363] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1366] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1369] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1372] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1375] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1378] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1381] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1384] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1387] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1390] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1393] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1396] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1399] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1402] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1405] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1408] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1411] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1414] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1417] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1420] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1423] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1426] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1429] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1432] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1435] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1438] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1441] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1444] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1447] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1450] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1453] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1456] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1459] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1462] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1465] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1468] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1471] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1474] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1477] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1480] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1483] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1486] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1489] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1492] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1495] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1498] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1501] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1504] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1507] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1510] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1513] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1516] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1519] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1522] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1525] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1528] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1531] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1534] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1537] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1540] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1543] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1546] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1549] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1552] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1555] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1558] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1561] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1564] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1567] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1570] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1573] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1576] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1579] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1582] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1585] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1588] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1591] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1594] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1597] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1600] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1603] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1606] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1609] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1612] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1615] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1618] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1621] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1624] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1627] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1630] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1633] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1636] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1639] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1642] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1645] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1648] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1651] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1654] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1657] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1660] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1663] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1666] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1669] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1672] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1675] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1678] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1681] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1684] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1687] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1690] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1693] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1696] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1699] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1702] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1705] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1708] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1711] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1714] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1717] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1720] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1723] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1726] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1729] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1732] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1735] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1738] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1741] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1744] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1747] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1750] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1753] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1756] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1759] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1762] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1765] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1768] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1771] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1774] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1777] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1780] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1783] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1786] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1789] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1792] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1795] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1798] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1801] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1804] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1807] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1810] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1813] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1816] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1819] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1822] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1825] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1828] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1831] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1834] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1837] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1840] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1843] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1846] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1849] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1852] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1855] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1858] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1861] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1864] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1867] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1870] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1873] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1876] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1879] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1882] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1885] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1888] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1891] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1894] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1897] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1900] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1903] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1906] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1909] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1912] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1915] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1918] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1921] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1924] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1927] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1930] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1933] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1936] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1939] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1942] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1945] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1948] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1951] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1954] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1957] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1960] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1963] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1966] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1969] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1972] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1975] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1978] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1981] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1984] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1987] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1990] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1993] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1996] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1999] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2002] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2005] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2008] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2011] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2014] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2017] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2020] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2023] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2026] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2029] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2032] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2035] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2038] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2041] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2044] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2047] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2050] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2053] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2056] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2059] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2062] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2065] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2068] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2071] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2074] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2077] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2080] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2083] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2086] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2089] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2092] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2095] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2098] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2101] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2104] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2107] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2110] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2113] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2116] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2119] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[2122] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2125] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2128] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2131] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2134] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2137] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2140] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2143] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2146] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2149] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2152] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[2155] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2158] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2161] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2164] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2167] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2170] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2173] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2176] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2179] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2182] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2185] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2188] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2191] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2194] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2197] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2200] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2203] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2206] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2209] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2212] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2215] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2218] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2221] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2224] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2227] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2230] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2233] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2236] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2239] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2242] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2245] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2248] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2251] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2254] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2257] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2260] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2263] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2266] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2269] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2272] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2275] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2278] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2281] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2284] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2287] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2290] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2293] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2296] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2299] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2302] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2305] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2308] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2311] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2314] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[2317] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2320] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2323] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a vector of all the species in the data set called species.\n\n\nIf you call the vector by typing its name (species) in the console you will notice that it repeats the species names.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nExecute the following code chunk and describe what this function does:\n\nunique(species)\n\n\n\nUsing indices might seem a little bit clunky, e.g. you have to know which column and row is which by position but it has its practical applications and is computationally very fast. For most of our data wrangling we will be using functions from the tidyverse packages dplyr and tidyr which work a little bit more intuitively."
  },
  {
    "objectID": "06_data-frames.html#write-data.frame-to-file",
    "href": "06_data-frames.html#write-data.frame-to-file",
    "title": "6  Intro to dataframes",
    "section": "6.4 Write data.frame to file",
    "text": "6.4 Write data.frame to file\nFrequently, we will process raw data sets and then need to write intermediate or final results to file, for example to share them with collaborators. Here, the readr packages comes in handy.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a subset of our data set consisting of the first 100 rows and containing information on the species, sex and fork length. Then use the help page for the function write_delim() to figure out how to write out a tab-delimited file into your data folder."
  },
  {
    "objectID": "07_data-transformation-i.html#data-wrangling",
    "href": "07_data-transformation-i.html#data-wrangling",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.1 Data Wrangling",
    "text": "7.1 Data Wrangling\nNow that we have a data frame to play with, let’s explore some data wrangling options using dplyr. The d stands for data and the plyr stands for plyers - this package is designed to manipulate data frames3. We are going to focus on the central actions (verbs) that will allow you to manipulate the data frame.3 This is also known as data wrangling or data munging, manipulating is not a negative thing in this case it means you can clean up and format the data in appropriate way to fit the questions you are asking and allow to to transform the information in a helpful way so that you can apply analysis and modeling as needed in the next step\nThe main advantages to using a command line program like R/code compared to a spreadsheet program such as Excel or Google sheets are:\n\nYou aren’t manipulating the raw data set - if you make a mistake or accidentally overwrite something you haven’t made any permanent damage.\nYou can manipulate data sets too large to easily handle in a spreadsheet\nIf you update your data set or have a second identically formatted data set you just have to re-run the code.\n\nBe sure to record all the steps (code chunks) in your quarto document - both the examples given here and the applications you will be asked to make. You can copy and paste, but you will find that writing out the code will help you get more used to syntax, how auto complete etc. works. Be sure to annotate/comment your code as reminders while we go through new functions in class, and that you take the time to go over your comments before submitting your knotted *.html document.\nThese are central concepts that you will use and reuse throughout the semester so you will likely want to refer back to this document. A good way to create a “cheatsheet” would be to for example for each function write a short description of what it does in general before each code chunk, then make your comment in the code specific to your example. Similarly use normal text to refer to the question numbers in this manual as you work through the problem sets."
  },
  {
    "objectID": "07_data-transformation-i.html#selecting-and-organizing-columns",
    "href": "07_data-transformation-i.html#selecting-and-organizing-columns",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.2 Selecting and organizing columns",
    "text": "7.2 Selecting and organizing columns\nLet’s start by loading our data set.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\nBe mindful\n\n\n\nWe will make heavy use of the magrittr pipe %&gt;% this smester which allows you to link commands together; think of it as “and now do this”. R for Data Science (2e) implements the native R pipe |&gt;. For our intents and purposes they are identical.\nBecause we are mostly interested in what the individual functions (verbs) do we will not always assign a new object, but just having it print to the console/below the code chunk we will be able to immediately assess the affect. By piping our function to head() it will print just the first 6 lines.\n\n\nThe function select() is used to select a subset of columns from a data set.\nFor example, you can select just the Site and Species columns4.4 Remember, the function head() allows you to just print the first few lines of the dataframe to the console, otherwise you can end up with several thousand lines!\n\ncatch %&gt;%\n  select(Site, Species) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Site        Species      \n  &lt;chr&gt;       &lt;chr&gt;        \n1 Aransas_Bay Bagre_marinus\n2 Aransas_Bay Bagre_marinus\n3 Aransas_Bay Bagre_marinus\n4 Aransas_Bay Bagre_marinus\n5 Aransas_Bay Bagre_marinus\n6 Aransas_Bay Bagre_marinus\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select just Day, Month, and Year columns?\n\n\nYou can also specify individual columns to eliminate by name. For example, the PCL column doesn’t contain any information (all NAs).\n\ncatch %&gt;%\n  select(-PCL) %&gt;%\n  head()\n\n# A tibble: 6 × 11\n  Site      Species Sex   Observed_Stage    FL   STL Hook_Size   Set   Day Month\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;             287   353        10     1    28     7\n2 Aransas_… Bagre_… U     &lt;NA&gt;             425   495        10     1    28     7\n3 Aransas_… Bagre_… U     &lt;NA&gt;             416   502        15     1    28     7\n4 Aransas_… Bagre_… U     &lt;NA&gt;             416   507        10     1    28     7\n5 Aransas_… Bagre_… U     &lt;NA&gt;             418   510        15     1    28     7\n6 Aransas_… Bagre_… U     &lt;NA&gt;             434   515        10     1    28     7\n# ℹ 1 more variable: Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you eliminate hook size from the data set?\n\n\nYou can also eliminate multiple columns by name, for example you would remove Day, Month and Year like this:\n\ncatch %&gt;%\n  select(-Day, -Month, -Year) %&gt;%\n  head()\n\n# A tibble: 6 × 9\n  Site        Species     Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   287   353        10     1\n2 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   425   495        10     1\n3 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   416   502        15     1\n4 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   416   507        10     1\n5 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   418   510        15     1\n6 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   434   515        10     1\n\n\nIf you want to re-arrange columns in your data frame, you would also use select().\n\ncatch %&gt;%\n  select(FL, Sex, Day) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n     FL Sex     Day\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1   287 U        28\n2   425 U        28\n3   416 U        28\n4   416 U        28\n5   418 U        28\n6   434 U        28\n\n\n\n\n\n\n\n\nProtip\n\n\n\nIf you wanted to move a set of columns to the front, but not not want to have to type in all the other column names you can use everything().\n\ncatch %&gt;%\n  select(Day, Month, Year, everything()) %&gt;%\n  head()\n\n# A tibble: 6 × 12\n    Day Month  Year Site        Species   Sex   Observed_Stage   PCL    FL   STL\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   287   353\n2    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   425   495\n3    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   416   502\n4    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   416   507\n5    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   418   510\n6    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   434   515\n# ℹ 2 more variables: Hook_Size &lt;dbl&gt;, Set &lt;dbl&gt;\n\n\n\n\nThere you go, creating subsets of columns: Simple as that."
  },
  {
    "objectID": "07_data-transformation-i.html#separating-uniting-columns",
    "href": "07_data-transformation-i.html#separating-uniting-columns",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.3 Separating & uniting columns",
    "text": "7.3 Separating & uniting columns\nOccasionally you will find that you want to combine the contents of two columns into a single column (e.g. first name, last name) or at other times you may want to separate the contents of a column over multiple columns (e.g. dates).\nFor example, you may have noticed that the Species is entered as genus_species - what if you wanted to have two separate columns with that information?\nThe function separate() will split the contents from one column across two or more columns. To do this you need to specify the new column names (into = c(\"column1\", \"column2\")), and what pattern should be used to determine where the content should be split (sep = \"pattern\").\n\ncatch %&gt;%\n  separate(Species, into = c(\"species\", \"genus\"), sep = \"_\", remove = FALSE) %&gt;%\n  head()\n\n# A tibble: 6 × 14\n  Site    Species species genus Sex   Observed_Stage   PCL    FL   STL Hook_Size\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   287   353        10\n2 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   425   495        10\n3 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   416   502        15\n4 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   416   507        10\n5 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   418   510        15\n6 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   434   515        10\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn some cases, there might not be a distinct pattern that you can use to identify where to split the column content. In this case it may be more helpful to use the position (e.g. “split at”third character from the left”) Look up the separate() function in the help tab and determine how you could split the Year column so you get two new columns by splitting off the last two digits (i.e. 2021 would be 20 and 21). Then eliminate the column containing the first two digits.\n\n\nIn other cases you might have information in two columns that you want to combine into a single column. This can be accomplished using the function unite().\nFor example, if we wanted to create a column called date that had the day, month, and year of each sampling trip separated by an _.\n\ncatch %&gt;%\n  unite(Date, Day, Month, Year, sep = \"_\", remove = FALSE) %&gt;%\n  head()\n\n# A tibble: 6 × 13\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set Date \n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1 28_7…\n2 Aransas_… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1 28_7…\n3 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1 28_7…\n4 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1 28_7…\n5 Aransas_… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1 28_7…\n6 Aransas_… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1 28_7…\n# ℹ 3 more variables: Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a data set with the following columns in this sequence and print the first few rows to the console.\n\nSet_ID (combining day, month, year, and set)\nGenus\nSpecies\nFL\nSTL"
  },
  {
    "objectID": "07_data-transformation-i.html#sorting-dataframes-by-a-specific-column-content",
    "href": "07_data-transformation-i.html#sorting-dataframes-by-a-specific-column-content",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.4 Sorting dataframes by a specific column content",
    "text": "7.4 Sorting dataframes by a specific column content\nUntil you want to visualize a table how the rows are arranged is not really important. However, for example, when generating reports you might want values to be listed in a specific way. This can be done using the function arrange().\nFor example, if we wanted to sort our dataframe based on the Observed_Stage column we could do the following:\n\ncatch %&gt;%\n  arrange(Observed_Stage)\n\n# A tibble: 2,325 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Sphyrn… M     MAT              622   668   850        10     1     8\n 2 Redfish… Sphyrn… M     MAT              656   710   869        10     1     8\n 3 Redfish… Sphyrn… F     MAT              708   770   979        15     2     8\n 4 Corpus_… Sphyrn… M     MAT              695   757   954        10     2    12\n 5 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 6 Corpus_… Sphyrn… M     MAT              621   689   856        10     2    27\n 7 Redfish… Sphyrn… F     MAT              781   853  1020        10     4    29\n 8 Redfish… Sphyrn… M     MAT              721   783   980        10     3    11\n 9 Redfish… Carcha… U     UND               NA    NA    NA        15     2    16\n10 Corpus_… Sphyrn… U     UND               NA    NA    NA        10     1    27\n# ℹ 2,315 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would sort your table by Sex?\n\n\nBy default, characters are sorted alphabetically, numeric columns from smallest to largest value. If you want to order your values from largest to smallest, you can specify that using desc()\n\ncatch %&gt;%\n  arrange(desc(FL))\n\n# A tibble: 2,325 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n 2 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 3 Redfish… Carcha… M     &lt;NA&gt;             792   882  1092        15     2    16\n 4 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 5 Redfish… Sphyrn… F     MAT              781   853  1020        10     4    29\n 6 Corpus_… Sciaen… U     &lt;NA&gt;              NA   841   950        10     3    25\n 7 Redfish… Carcha… M     &lt;NA&gt;             740   840  1010        15     3    29\n 8 Redfish… Carcha… M     &lt;NA&gt;             740   820  1020        10     4     1\n 9 Aransas… Carcha… M     &lt;NA&gt;             720   812   912        15     4    22\n10 Redfish… Sphyrn… M     MAT              721   783   980        10     3    11\n# ℹ 2,315 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would sort your site column from Z to A?"
  },
  {
    "objectID": "07_data-transformation-i.html#filtering-subsetting-rows",
    "href": "07_data-transformation-i.html#filtering-subsetting-rows",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.5 Filtering (subsetting) rows",
    "text": "7.5 Filtering (subsetting) rows\nFrequently, we are less interested in being able to sort columns by content, rather, we want to extract a subset of rows based on specific content.\nThe function filter() is used to subset a data frame by row based on regular expressions and the boolean operators we previously encounter to describe the content of sets of rows.\nFor example, we might a data.frame with only Gafftop sail catfish (Bagre marinus)5.5 Remember for exact matches we use == not =\n\ncatch %&gt;%\n  filter(Species == \"Bagre_marinus\")\n\n# A tibble: 1,511 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 1,501 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select only rows containing Scalloped Hammerheads (Sphyrna lewini)?”\n\n\nIf we want all rows but Gafftop sailfish you can use a ! to say “not that” instead of having to list all the species that you do want to keep6.6 This is frequently called “blacklisting”, while creating a list of content that you do want to keep would be referred to as “whitelisting”.\n\ncatch %&gt;%\n  filter(!Species == \"Bagre_marinus\")\n\n# A tibble: 814 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Rhizop… M     &lt;NA&gt;             351   378   433        15     1    29\n 2 Redfish… Sphyrn… F     &lt;NA&gt;             470   430   600        10     3    29\n 3 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   670        15     3    29\n 4 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   340        10     3    29\n 5 Redfish… Hypanu… M     &lt;NA&gt;              NA    NA   810        10     4    29\n 6 Corpus_… Carcha… F     &lt;NA&gt;             609   670   820        15     1    30\n 7 Corpus_… Sphyrn… M     &lt;NA&gt;             495   485   615        10     2    24\n 8 Corpus_… Sphyrn… F     &lt;NA&gt;             550   370   720        10     2    24\n 9 Corpus_… Sphyrn… M     &lt;NA&gt;             470   505   645        10     3    24\n10 Corpus_… Sphyrn… F     &lt;NA&gt;             540   565   720        10     3    24\n# ℹ 804 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a dataframe without Scalloped Hammerheads (Sphyrna lewini) entries?\n\n\nSometimes you might want to select rows that match one of a set of values7. In this case we would use %in% to indicate “keep any of these”.7 Recall, the function c() (concatenate) creates a vector\n\ncatch %&gt;%\n  filter(Species %in% c(\"Sciades_felis\", \"Bagre_marinus\", \"Synodus_foetens\"))\n\n# A tibble: 2,166 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 2,156 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nAgain, if you wanted everything but rows containing those values you would preface it with a !.\n\ncatch %&gt;%\n  filter(!Species %in% c(\"Sciades_felis\", \"Bagre_marinus\", \"Synodus_foetens\"))\n\n# A tibble: 159 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Rhizop… M     &lt;NA&gt;             351   378   433        15     1    29\n 2 Redfish… Sphyrn… F     &lt;NA&gt;             470   430   600        10     3    29\n 3 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   670        15     3    29\n 4 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   340        10     3    29\n 5 Redfish… Hypanu… M     &lt;NA&gt;              NA    NA   810        10     4    29\n 6 Corpus_… Carcha… F     &lt;NA&gt;             609   670   820        15     1    30\n 7 Corpus_… Sphyrn… M     &lt;NA&gt;             495   485   615        10     2    24\n 8 Corpus_… Sphyrn… F     &lt;NA&gt;             550   370   720        10     2    24\n 9 Corpus_… Sphyrn… M     &lt;NA&gt;             470   505   645        10     3    24\n10 Corpus_… Sphyrn… F     &lt;NA&gt;             540   565   720        10     3    24\n# ℹ 149 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you subset a data frame to contain only entries for different species in the genus Carcharhinus aka the sharky-sharks? There are four species in the data set - bullsharks (Carcharhinus leucas), spinner sharks (Carcharhinus brevipinna), blacktip sharks (Carcharhinus limbatus), and smalltail sharks (Carcharhinus porosus).\n\n\nFor numbers you likely aren’t just searching for exact matches, you also want to be able to set threshold values and select everything above or below. For example, you can select all rows with values greater than a certain value using &gt;.\n\ncatch %&gt;%\n  filter(FL &gt; 440)\n\n# A tibble: 907 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   496   565        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   476   569        10     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   495   570        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   490   575        10     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   486   581        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   503   589        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   489   590        10     1    28\n# ℹ 897 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a data frame containing only entries with a forklength shorter than 300mm, then create a data frame that contains only entries with a forklength equal to or smaller than 300mm.\n\n\nIn this second piece of code you used a single boolean operator to include two conditions, “smaller than” and “equal two”. That is a special case of wanting to retain data than fulfills one of either of two conditions and we have a specific boolean operator that can combine the two.\nThis is not always the case, for example, you might want to retain data that fulfills conditions in two different columns. In this case you can combine expressions using & to indicate that it must fulfill all conditions indicated or | to indicate that it must retain at least one of the.\nFor example to select only scalloped hammerheads that are also smaller than 300 cm you would use\n\ncatch %&gt;%\n  filter(Species == \"Sphyrna_lewini\" & FL &lt; 300)\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Corpus_C… Sphyrn… F     &lt;NA&gt;             192   210   280        15     3     6\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you subset a data frame to contain only entries for different species in the genus Carcharhinus that have a forklength larger than 500 cm?\n\n\nBy contrast, if you wanted all entries that are either gafftops or a fork length smaller than 300 cm you could use the following code:\n\ncatch %&gt;%\n  filter(Species == \"Sphyrna_lewini\" | FL &lt; 300)\n\n# A tibble: 409 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   285   314        10     2    28\n 3 Corpus_… Bagre_… U     &lt;NA&gt;              NA   299   348        10     1    30\n 4 Corpus_… Bagre_… U     &lt;NA&gt;              NA   297   367        10     2    30\n 5 Corpus_… Bagre_… U     &lt;NA&gt;              NA   298   362        10     3    30\n 6 Corpus_… Bagre_… U     &lt;NA&gt;              NA   290   350        10     2    24\n 7 Redfish… Bagre_… U     &lt;NA&gt;              NA   254   284        10     4     8\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA    50   574        10     1    25\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   280   340         3     3    25\n10 Redfish… Bagre_… U     &lt;NA&gt;              NA   294   353        10     4    16\n# ℹ 399 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select fish that have a forklength either smaller than 200 cm or larger than 300cm?"
  },
  {
    "objectID": "07_data-transformation-i.html#sneak-peak-grouping-rows-for-specific-wrangling-actions",
    "href": "07_data-transformation-i.html#sneak-peak-grouping-rows-for-specific-wrangling-actions",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.6 Sneak peak: grouping rows for specific wrangling actions",
    "text": "7.6 Sneak peak: grouping rows for specific wrangling actions\nWe have already see that it can be helpful to subset rows based on conditions that are met by the content of more than one column. In those cases, we were creating conditions based on Boolean operators.\nIn many cases we might be interested in subsetting a dataframe in a way where our conditions cannot be expressed by a TRUE/FALSE scenario using Boolean operators.\nFor example, we might want to extract the data entry for the longest fish in the data set based on forklength.\nThe function max() can be used to get the maximum value for a vector of numbers. In this case, the vector we are looking at is the FL column of the catch dataframe.\n\ncatch %&gt;%\n  filter(FL == max(FL, na.rm = TRUE))\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nThat’s great, now we now what the largest fish is that we caught.\nWhat about if we wanted to subset the dataframe to retain the largest fish based on forklength for each species?\n\n\n\n\n\n\n Consider this\n\n\n\nConceptually lay out the individual steps that you would need to complete to do this (don’t worry about whether or not you actually know how to code this).\n\n\nThe tidyverse has a central concept call “split-apply-combine”, which means that occasionally we want to group entries in a dataframe (split), do some sort of manipulation (apply), but end up with a single data frame (combine). We will look at how useful this is in the next chapter but let’s take a quick sneak peak at how this is implemented in dplyr using group_by().\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  filter(FL == max(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# A tibble: 12 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Corpus_… Bagre_… U     &lt;NA&gt;              NA   575   640        10     2    24\n 2 Aransas… Rhizop… F     &lt;NA&gt;             580   637   790        10     2    25\n 3 Aransas… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n 4 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 5 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 6 Aransas… Carcha… F     &lt;NA&gt;             690   757   940        10     2    22\n 7 Corpus_… Sphyrn… F     &lt;NA&gt;             520   578   770        10     1    21\n 8 Corpus_… Carcha… U     &lt;NA&gt;             335   415   475        10     1     3\n 9 Aransas… Sciade… U     &lt;NA&gt;              NA   480   548        10     2    18\n10 Aransas… Sciade… U     &lt;NA&gt;              NA   480   580        10     2    18\n11 Corpus_… Sciaen… U     &lt;NA&gt;              NA   841   950        10     3    25\n12 Corpus_… Synodu… U     &lt;NA&gt;              NA   173   185        10     3    30\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nThis is also an example of how we can use the pipe (%&gt;%) to string a bunch of commands, in this example we are saying “take the object catch, and then group rows by Species and then for each group retain only the maximum forklength value for that group and then ungroup them again.”\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you group rows by Species and then retain the individual caught on the largest hook size for each species?\n\n\n\n\n\n\n\n\nProtip\n\n\n\nSpecifically for cases where we want to retain the largest or smallest values, we can use family of of functions called slice() which allow us to subset rows based on their position.\nFor example, we can retain the largest 5 individuals per species based on forklength using slice_max()\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  slice_max(order_by = FL, n = 5)\n\n# A tibble: 66 × 12\n# Groups:   Species [14]\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Corpus_… Bagre_… U     &lt;NA&gt;              NA   575   640        10     2    24\n 2 Corpus_… Bagre_… U     &lt;NA&gt;              NA   574   676        10     2    21\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   565    NA        10     1    17\n 4 Redfish… Bagre_… U     &lt;NA&gt;              NA   564   651        15     1    29\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   555   541        10     3    13\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   555    NA        15     1    17\n 7 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 8 Redfish… Carcha… M     &lt;NA&gt;             792   882  1092        15     2    16\n 9 Redfish… Carcha… M     &lt;NA&gt;             740   820  1020        10     4     1\n10 Redfish… Carcha… M     &lt;NA&gt;             660   722   880        10     1    20\n# ℹ 56 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;"
  },
  {
    "objectID": "07_data-transformation-i.html#create-a-subset-with-only-unique-entries",
    "href": "07_data-transformation-i.html#create-a-subset-with-only-unique-entries",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.7 Create a subset with only unique entries",
    "text": "7.7 Create a subset with only unique entries\nOccasionally, you might want to create a subset of the data set that shows only the unique (distinct) entries for a specific column; this is especially common during an exploratory analysis of a data set that you are getting an overview of. This can be achieved using the function distinct().\nFor example, we might want to know which years the survey took place.\n\ncatch %&gt;%\n  distinct(Year)\n\n# A tibble: 4 × 1\n   Year\n  &lt;dbl&gt;\n1  2015\n2  2016\n3  2017\n4  2018\n\n\nNotice how that dropped all the other columns. You can switch that off using .keep_all = FALSE.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you produce a table with only one representative per species?\n\n\nYou can also combine columns. For example if we wanted to determine the individual sets of the data set we could use\n\ncatch %&gt;%\n  distinct(Day, Month, Year, Set)\n\n# A tibble: 197 × 4\n     Day Month  Year   Set\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    28     7  2015     1\n 2    28     7  2015     2\n 3    28     7  2015     3\n 4    28     7  2015     4\n 5    29     7  2015     1\n 6    29     7  2015     2\n 7    29     7  2015     4\n 8    30     7  2015     1\n 9    30     7  2015     2\n10    30     7  2015     3\n# ℹ 187 more rows\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you produce a table showing all the species caught per station using distinct(), presented as arranged alphabetically by Site and Species within Site?"
  },
  {
    "objectID": "08_data-transformation-ii.html#adding-new-variables",
    "href": "08_data-transformation-ii.html#adding-new-variables",
    "title": "8  Data Transformation:",
    "section": "8.1 Adding new variables",
    "text": "8.1 Adding new variables\nSo,turns out selecting columns and filtering based on content in rows is pretty straightforward.\nBut frequently when we are processing our raw data sets we end up wanting to compute additional metrics or use the existing raw data to create new categories.\nThe function mutate() can be used to create new columns. Frequently, this is done based on columns already existing in the data frame. This is a very powerful function with endless possibilities, but we are going to stick to some of the basics for now3.3 Rest assured if your answer is “Oh, could I …” the answer is “Yes”.\nLet’s say you wanted create a column that contained the difference between the fork length and the stretch total length4:4 By default mutate() appends (adds) the new column as the last column. So we can see our results better we’ll used select() to move it to be the first column in the dataframe)\n\ncatch %&gt;%\n  mutate(difference = STL - FL) %&gt;%\n  select(difference, everything())\n\n# A tibble: 2,325 × 13\n   difference Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size\n        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1         66 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10\n 2         70 Aransas_… Bagre_… U     &lt;NA&gt;              NA   425   495        10\n 3         86 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   502        15\n 4         91 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   507        10\n 5         92 Aransas_… Bagre_… U     &lt;NA&gt;              NA   418   510        15\n 6         81 Aransas_… Bagre_… U     &lt;NA&gt;              NA   434   515        10\n 7         93 Aransas_… Bagre_… U     &lt;NA&gt;              NA   427   520        15\n 8         86 Aransas_… Bagre_… U     &lt;NA&gt;              NA   446   532        10\n 9         73 Aransas_… Bagre_… U     &lt;NA&gt;              NA   465   538        10\n10         89 Aransas_… Bagre_… U     &lt;NA&gt;              NA   450   539        10\n# ℹ 2,315 more rows\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nYou should now have a column called difference at the end of the data frame5.5 Instead of - to substract, you can other mathematical operators such as + to add , * to multiple, and / to divide values when creating a new column.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called ratio, that is the ratio of the fork to stretch total length?\n\n\nYou can also create a column that contains a logical value (TRUE/FALSE). For example we might need a column that indicates if the Sex is unknown.\n\ncatch %&gt;%\n  mutate(unknown_sex = Sex == \"U\") %&gt;%\n  select(unknown_sex, everything())\n\n# A tibble: 2,325 × 13\n   unknown_sex Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size\n   &lt;lgl&gt;       &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10\n 2 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10\n 3 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15\n 4 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10\n 5 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15\n 6 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10\n 7 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15\n 8 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10\n 9 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10\n10 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10\n# ℹ 2,315 more rows\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nYou should know have a column called unknown_sex where if the animal that was caught was not sexed contains the value TRUE, if it was identified as male or female it would say FALSE.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called post_2017 that is TRUE if fish were caught after 2017?\n\n\n\n\nFor that last problem, a “conditional mutate” using an ifelse statement (if this then do that, else do that) could have come in handy. Another option is case_when() which allows you to create multiple sets of conditions as opposed to ifelse which sets up a TRUE/FALSE dichotomy (file this information away for “maybe useful later”)."
  },
  {
    "objectID": "08_data-transformation-ii.html#group_by-and-mutate",
    "href": "08_data-transformation-ii.html#group_by-and-mutate",
    "title": "8  Data Transformation:",
    "section": "8.2 group_by() and mutate()",
    "text": "8.2 group_by() and mutate()\nMany problems in data science require you to split your data set into subsets according to some grouping variable, apply a function, and then combine the results. dplyr is designed to make this straightforward; you have already sen an example of this while you were learning about filter().\nSimilarly, you can combine mutate() with group_by().\n\n\nThe function mean() will calculate the mean value of a vector of numbers, the argument na.rm=TRUE tells the function to ignore any NA-values in the data set.\nFor example, let’s say you wanted to create a column that is the difference between the fork length of an individual and the mean fork length of that species.\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  mutate(diff_mean = FL-mean(FL, na.rm = TRUE))\n\n# A tibble: 2,325 × 13\n# Groups:   Species [14]\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 2,315 more rows\n# ℹ 3 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;, diff_mean &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called that contains the difference between the fork length of an individual and the mean fork length of that species for each month?"
  },
  {
    "objectID": "08_data-transformation-ii.html#create-new-data.frame-based-on-another",
    "href": "08_data-transformation-ii.html#create-new-data.frame-based-on-another",
    "title": "8  Data Transformation:",
    "section": "8.3 Create new data.frame based on another",
    "text": "8.3 Create new data.frame based on another\nNot infrequently we are more interested in summary (descriptive) stats of a data set rather than all the raw data - Tidyverse got you covered with the function summarize().\nFor example, we might want to calculate the mean and standard deviation of the measured fork length.\n\ncatch %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n            sd_FL = sd(FL, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_FL sd_FL\n    &lt;dbl&gt; &lt;dbl&gt;\n1    406.  103.\n\n\n\n\nRemember, that earlier we’ve have used the function max() to obtain the largest value in a vector.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow could you use summarize to get the maximum forklength?\n\n\nThat’s cool but really we could have also just used\n\nmean(catch$FL, na.rm = TRUE)\n\n[1] 405.9179\n\nmax(catch$FL, na.rm = TRUE)\n\n[1] 1140\n\n\nto get that information, since we are only interested in one column (vector).\nsummarize() becomes especially powerful once we leverage group_by() to start calculating summary stats for entries grouped by a grouping variable.\nFor example we can calculate summary stats by species and generate a table to include in a report.\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n           median_FL = median(FL, na.rm = TRUE),\n           max_FL = max(FL, na.rm = TRUE),\n           min_FL = min(FL, na.rm = TRUE),\n           sd_FL = sd(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# A tibble: 14 × 6\n   Species                    mean_FL median_FL max_FL min_FL sd_FL\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bagre_marinus                 433.      445     575     45  65.4\n 2 Carcharhinus_brevipinna       644.      648     900    489  69.2\n 3 Carcharhinus_leucas           769       702    1140    624 167. \n 4 Carcharhinus_limbatus         613.      579     757    538 101. \n 5 Carcharhinus_porosus          415       415     415    415  NA  \n 6 Hypanus_americanus            NaN        NA    -Inf    Inf  NA  \n 7 Hypanus_sabina                NaN        NA    -Inf    Inf  NA  \n 8 Rhinoptera_bonasus            NaN        NA    -Inf    Inf  NA  \n 9 Rhizoprionodon_terraenovae    412       396     637    306  73.6\n10 Sciades_felis                 299.      297     480    102  41.9\n11 Sciaenops_ocellatus           793       793     841    745  67.9\n12 Sphyrna_lewini                471.      548.    578    210 174. \n13 Sphyrna_tiburo                622.      605     861    370 114. \n14 Synodus_foetens               173       173     173    173  NA  \n\n\n\n\n\n\n\n\n Consider this\n\n\n\nIf you look closely you should see that you are getting a few NA, NaN, -Inf, and Inf values - any guesses why? You might want to pull up the catch data frame in the view panel to see what is going on with those species.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow could you use summarize() to calculate a range of summary stats for the stretch total length for individuals grouped by sex?\n\n\nSo far, we have been manipulating our data frame using code and printing it directly to the console (and our quarto document). This can be useful for example to generate tables for reports. However, in many cases we want to create a new object that has been manipulated according to our code and then we will further process, visualize, or analyze that dataframe down the line.\n\nsummary &lt;- catch %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n           median_FL = median(FL, na.rm = TRUE),\n           max_FL = max(FL, na.rm = TRUE),\n           min_FL = min(FL, na.rm = TRUE),\n           sd_FL = sd(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nWhen you execute this code, you’ll notice that the code (and probably a warning) is printed to the console but there is no output. Instead, if you look at the environment in the bottom left panel you should now see a new object called summary. Per usual, you can pull that up in the Editor/View pane (top left) using either View(summary) in the console or by clicking on the object in the environment.\nYou will be presenting results in reports over the course of the semester, when you knit an quarto file you will get tables formatted in a standard way according to defaults in the resulting html file. If you want finer control over the output, you can use the kable() function. This will allow you to further format the table, for example, you may specify the number of digits printed using the argument digits =.\nBy adding a chunk options for a label as #| label: tbl-sum-stats and table caption as #| tbl-cap: \"Summary statistics for the forklength of each species, you can further modify the output that adheres to typical reporting standards for reports and research articles.\n\nkable(\n  summary,\n  digits = 1\n)\n\n\n\n\n\n\nSpecies\nmean_FL\nmedian_FL\nmax_FL\nmin_FL\nsd_FL\n\n\n\n\nBagre_marinus\n433.4\n445.0\n575\n45\n65.4\n\n\nCarcharhinus_brevipinna\n643.7\n648.0\n900\n489\n69.2\n\n\nCarcharhinus_leucas\n769.0\n702.0\n1140\n624\n167.3\n\n\nCarcharhinus_limbatus\n613.2\n579.0\n757\n538\n101.0\n\n\nCarcharhinus_porosus\n415.0\n415.0\n415\n415\nNA\n\n\nHypanus_americanus\nNaN\nNA\n-Inf\nInf\nNA\n\n\nHypanus_sabina\nNaN\nNA\n-Inf\nInf\nNA\n\n\nRhinoptera_bonasus\nNaN\nNA\n-Inf\nInf\nNA\n\n\nRhizoprionodon_terraenovae\n412.0\n396.0\n637\n306\n73.6\n\n\nSciades_felis\n298.9\n297.0\n480\n102\n41.9\n\n\nSciaenops_ocellatus\n793.0\n793.0\n841\n745\n67.9\n\n\nSphyrna_lewini\n470.8\n547.5\n578\n210\n174.4\n\n\nSphyrna_tiburo\n621.5\n605.0\n861\n370\n114.4\n\n\nSynodus_foetens\n173.0\n173.0\n173\n173\nNA\n\n\n\nTable 8.1: Summary statistics for the forklength of each species in the catch data"
  },
  {
    "objectID": "08_data-transformation-ii.html#combining-verbs",
    "href": "08_data-transformation-ii.html#combining-verbs",
    "title": "8  Data Transformation:",
    "section": "8.4 Combining verbs",
    "text": "8.4 Combining verbs\nWe’ve already combined most of our dplyr verbs with group_by().\nWhen you are wrangling data you will find that making use of the pipe (%&gt;%) to combine select(), filter(), mutate(), and summarize() as a series of commands will be necessary to get your data set in the correct format and further process it.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nExecutre the following the code chunk. Then describe what each line is doing to manipulate the data frame.\n\ncatch %&gt;%\n  select(-PCL, -Hook_Size) %&gt;%\n  separate(Species, into = c(\"genus\", \"species\"), remove = TRUE) %&gt;%\n  unite(Date, Day, Month, Year) %&gt;%\n  filter(genus == \"Carcharhinus\" & Sex %in% c(\"F\", \"M\")) %&gt;%\n  group_by(Site, genus, species, Sex) %&gt;%\n  filter(FL == max(FL)) %&gt;%\n  arrange(species)\n\n# A tibble: 11 × 9\n# Groups:   Site, genus, species, Sex [11]\n   Site               genus species Sex   Observed_Stage    FL   STL   Set Date \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Aransas_Bay        Carc… brevip… M     &lt;NA&gt;             640   792     3 22_9…\n 2 Redfish_Bay        Carc… brevip… F     &lt;NA&gt;             900  1090     1 16_6…\n 3 Redfish_Bay        Carc… brevip… M     &lt;NA&gt;             882  1092     2 16_6…\n 4 Corpus_Christi_Bay Carc… brevip… M     &lt;NA&gt;             699   860     1 25_1…\n 5 Corpus_Christi_Bay Carc… brevip… F     &lt;NA&gt;             704   880     4 2_10…\n 6 Aransas_Bay        Carc… leucas  F     &lt;NA&gt;            1140  1410     1 25_5…\n 7 Corpus_Christi_Bay Carc… leucas  F     YOY              694   854     2 10_6…\n 8 Aransas_Bay        Carc… leucas  M     &lt;NA&gt;             812   912     4 22_9…\n 9 Redfish_Bay        Carc… leucas  M     &lt;NA&gt;             840  1010     3 29_9…\n10 Aransas_Bay        Carc… limbat… F     &lt;NA&gt;             757   940     2 22_6…\n11 Corpus_Christi_Bay Carc… limbat… M     &lt;NA&gt;             610   770     2 30_8…\n\n\n\n\nGenerate the code that will manipulate the data frame as follows6:6 some bullet points may require more than one line of code; you do not have to perform the steps in the sequence presented, play around a little bit to see how to code this more efficiently\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 1:\n\norder columns so Day, Month, Year, Set are at the beginning.\nretain all male individuals in the genus Carcharhinus.\nget rid of columns containing information on observed stage, precaudal length, and hook size\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 2:\n\ncreate a new variable called Set_ID consisting of Day, Month, Year, and Set number.\ndetermine the number of individuals per species per set7.\n\n7 There is a function called n() that allows us to count rows fulfilling a specific condition\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 3:\n\nremove all gafftops\ncalculate mean forklength for each species by sex and month of the year."
  },
  {
    "objectID": "09_tidy-data.html#producing-tidy-data-sets",
    "href": "09_tidy-data.html#producing-tidy-data-sets",
    "title": "9  Tidy data",
    "section": "9.1 Producing tidy data sets",
    "text": "9.1 Producing tidy data sets\nThe last set of functions that we need to get comfortable with allow us to create tidy data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nList the three characteristics of a tidy data set. Explain why a tidy data set is sometimes also describe as a long data set.\n\n\nLet’s read out data set back into our R session.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at our data set and argue whether or not it is a tidy data set. The easiest way to do this is to determine if it fullfills all the characteristics.\n\n\nLet’s quickly reformat our catch data as follows\n\ncatch_length &lt;- catch %&gt;%\n  unite(SetID, Year, Month, Day, Set, sep = \"_\") %&gt;%\n  select(SetID, Site, Species, Sex, PCL, FL, STL)\n\nhead(catch_length)\n\n# A tibble: 6 × 7\n  SetID       Site        Species       Sex     PCL    FL   STL\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   287   353\n2 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   425   495\n3 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   416   502\n4 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   416   507\n5 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   418   510\n6 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   434   515\n\n\nWe can turn this into a tidy data set using pivot_longer(). To do this we have to identify columns that will be used as the key (cols =) and then name the column that will hold those values (names_to()) and the column that will hold the value (values_to()).\nIn this case, we have made three observations about length for each specimen, in order to have rows with unique observations we want a column that identifies what type of observation was made, for example called Measurement. This is called the “key” because it allows us to “unlock” what type of measurement the individual observation is, i.e. this column will let us know whether an observation (row) is pre-caudal length, fork length, or stretch total length.\nWe will designate another column Length to hold the values for each measurement.\nWe can identify the columns that need to be gathered either by name or since we have re-arranged our dataframe so they are the last columns by column number.\n\ntidy_length &lt;- catch_length %&gt;%\n  pivot_longer(names_to = \"Measurement\", values_to = \"Length\", cols = 5:7)\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly outline advantages to using tidy data sets.\n\n\nWith this data set it would be straightforward for us to e.g. calculate mean values for each length measurement by species using group_by() and summarize().\n\ntidy_length %&gt;%\n  group_by(Species, Measurement) %&gt;%\n  summarize(mean = mean(Length, na.rm = TRUE))\n\n# A tibble: 42 × 3\n# Groups:   Species [14]\n   Species                 Measurement  mean\n   &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;\n 1 Bagre_marinus           FL           433.\n 2 Bagre_marinus           PCL          NaN \n 3 Bagre_marinus           STL          517.\n 4 Carcharhinus_brevipinna FL           644.\n 5 Carcharhinus_brevipinna PCL          583.\n 6 Carcharhinus_brevipinna STL          804.\n 7 Carcharhinus_leucas     FL           769 \n 8 Carcharhinus_leucas     PCL          691.\n 9 Carcharhinus_leucas     STL          936.\n10 Carcharhinus_limbatus   FL           613.\n# ℹ 32 more rows"
  },
  {
    "objectID": "09_tidy-data.html#convert-a-tidy-data-set-to-wide-format",
    "href": "09_tidy-data.html#convert-a-tidy-data-set-to-wide-format",
    "title": "9  Tidy data",
    "section": "9.2 Convert a tidy data set to wide format",
    "text": "9.2 Convert a tidy data set to wide format\nDespite all the advantages of tidy data sets you can see from the table above that frequently when we are presenting results in a table it may be advantageous in terms of layout to have a non-tidy format.\nThis can be done using pivot_wider() which works like pivot_longer() but in reverse. You designate which column is the key (names_from =), i.e. these will become the column names in the new table. Then you need to identify which column in your current data frame contains the values that should be filled out/spread into the columns that will be generated from your key (values_from =).\nSince we don’t have values for precaudal length, we probably want to use filter() to remove those rows first.\n\n\nMore notes on naming things … recall, that we said that filenames should not contain spaces or special characters? We set similar rules for naming objects. Well, column names is a similar conundrum. Including spaces or species characters as a column name creates problems when we are using functions like select() to subset by column name or mutate() to create new columns based on exisiting columns. Similarly, if the column name is a number you will have problems. If you do have unconvential column names you can rename them using rename() or you can use backticks and either side of the name to indicate that it is a column name.\n\ntidy_length %&gt;%\n  filter(!Measurement == \"PCL\") %&gt;%\n  group_by(Species, Measurement) %&gt;%\n  summarize(mean = mean(Length, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = \"Measurement\", values_from = \"mean\")\n\n# A tibble: 14 × 3\n# Groups:   Species [14]\n   Species                       FL   STL\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1 Bagre_marinus               433.  517.\n 2 Carcharhinus_brevipinna     644.  804.\n 3 Carcharhinus_leucas         769   936.\n 4 Carcharhinus_limbatus       613.  776.\n 5 Carcharhinus_porosus        415   475 \n 6 Hypanus_americanus          NaN   954.\n 7 Hypanus_sabina              NaN   349.\n 8 Rhinoptera_bonasus          NaN   819 \n 9 Rhizoprionodon_terraenovae  412   510.\n10 Sciades_felis               299.  343.\n11 Sciaenops_ocellatus         793   932.\n12 Sphyrna_lewini              471.  628 \n13 Sphyrna_tiburo              622.  792.\n14 Synodus_foetens             173   185 \n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCalculate the number of individuals per species caught per month in 2018 and present that data in a wide formate to make it easy to compare the number of species (species) per month (columns). As a bonus create an additional column with total catch of that species for 2018."
  },
  {
    "objectID": "10_relational-data.html#composition-of-elasmobranch-communities-compare-across-sites",
    "href": "10_relational-data.html#composition-of-elasmobranch-communities-compare-across-sites",
    "title": "10  Relational data",
    "section": "10.1 Composition of elasmobranch communities compare across sites",
    "text": "10.1 Composition of elasmobranch communities compare across sites\nLet’s start by reading in the data set we will use for this analysis3.3 This is a data set that has been cleaned up to contain only the elasmobranchs caught during the survey since that is the taxonomic group we are interested in\n\nelasmos &lt;- read_delim(\"data/longline_elasmobranchs.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nProduce a table that contains the number of times a species was caught at each site and overall during the long-lining survey and give a brief description of the pattern(s) you see. Briefly, compare the list of species that were caught to the species identified in the longterm TWPD gill net monitoring program.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nYour table should look something like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\nTotal\n\n\n\n\nCarcharhinus_brevipinna\n12\n46\n12\n70\n\n\nCarcharhinus_leucas\n3\n4\n1\n8\n\n\nCarcharhinus_limbatus\n1\n1\n0\n2\n\n\nCarcharhinus_porosus\n0\n1\n0\n1\n\n\nHypanus_americanus\n3\n1\n7\n11\n\n\nHypanus_sabina\n9\n2\n0\n11\n\n\nRhinoptera_bonasus\n0\n0\n1\n1\n\n\nRhizoprionodon_terraenovae\n1\n5\n8\n14\n\n\nSphyrna_lewini\n0\n4\n0\n4\n\n\nSphyrna_tiburo\n1\n18\n16\n35\n\n\n\nTable 10.1: Number of individuals per caught per site and overall across all sites and years.\n\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can use replace(is.na(.), 0) to replace NA values in all columns with a 0.\n\n\nWe are not only interested in which species are observed at each site, we also want to know what at what life stages different species are using the estuaries. Typically, we can classify sharks as young-of-the-year (YOY), juveniles (JUV), or mature (MAT). There are ways to observe this in the field, for example YOY can be identified using their umbilical scar and in male sharks whether or not the claspers are calcified is an indication of maturity.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDetermine how many individuals have information on their life history stage.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nAnother way to determine the life history stage is to used previously information on length-at-maturity and how quickly YOY grow during their first year of life. This information is species-specific and has been determined for various species using life-history studies that rely on data sets that contain information on size, level of maturity and age4.4 Sharks can be aged using their vertebrae similar to how we can use growth rings on trees to age them.\nFor example, (Carlson and Baremore 2005) determined the following length/history stage relationships for spinner sharks (C. brevipinna)\n\nCarlson, John K., and Ivy Baremore. 2005. “Growth Dynamics of the Spinner Shark (Carcharhinus Brevipinna) Off the United States Southeast and Gulf of Mexico Coasts: A Comparison of Methods.” Fishery Bulletin 103 (2). https://aquadocs.org/handle/1834/26223.\n\nYOY\n\nfemales &lt; 844mm\nmales &lt; 812mm\n\nJuveniles\n\nfemales 844 - 1360mm\nmales 812 - 1380mm\n\nmature (adults)\n\nfemales &gt; 1360 mm\nmales &gt; 1380 mm\n\n\nWhile (Neer, Thompson, and Carlson 2005) published these details for bull sharks (C. leucas)\n\nNeer, J. A., B. A. Thompson, and John K. Carlson. 2005. “Age and Growth of Carcharhinus Leucas in the Northern Gulf of Mexico: Incorporating Variability in Size at Birth - Neer - 2005 - Journal of Fish Biology - Wiley Online Library.” Journal of Fish Biology 67 (2): 370–83. https://onlinelibrary.wiley.com/doi/full/10.1111/j.0022-1112.2005.00743.x.\n\nYOY\n\nfemales &lt; 700mm\nmales &lt; 700mm\n\nJuveniles\n\nfemales 700 - 2250mm\nmales 700 - 2100mm\n\nmature (adults)\n\nfemales &gt; 2250mm\nmales &gt; 2100mm\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nFirst, conceptually describe how you could add this information to your data sheet in excel as a new column called Estimated_Stage.\n\n\nNow, let’s consider how we could use our data wrangling skills to add a new column Estimated_Stage that contains life history stage based on length estimates. Let’s first work this out for the two species above to keep it simple.\nWhen confronted with a more complex problems like this it can be helpful to first walk through the individual steps necessary5.5 Many people find it helpful to write things out in ‘pseudo-code’ first and then work out what the code needs to look like for the specific language they are working in\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly outline what you think our approach should look like - even if you don’t know the functions you need to achieve this.\n\n\nThere are two approaches we can take.\n\n\n\n\n\n\nSolution 1\n\n\n\n\n\nThe first solution involves sub-setting your data.frame using filter() to contain only individuals that fulfill the conditions of specific length ranges that fit the ranges above for each life history stage and the add a new column with the correctly assigned life history stage6.\n\n# C. brevipinna, Carlson & Baremore 2005\n\nC.brevipinna_YOY &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&lt;=812 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&lt;=844 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&lt;=844) %&gt;%\n  mutate(Estimated_Stage=\"YOY\")\n\nC.brevipinna_JUV &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;812 & FL&lt;=1380 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;844 & FL&lt;=1360 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;844 & FL&lt;=1360) %&gt;%\n  mutate(Estimated_Stage=\"JUV\")\n\nC.brevipinna_MAT &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;1380 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;1360 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;1360) %&gt;%\n  mutate(Estimated_Stage=\"MAT\")\n\n\n# C. leucas, Neer et al. 2005\n\nC.leucas_YOY &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & FL&lt;=700) %&gt;%\n  mutate(Estimated_Stage=\"YOY\")\n\nC.leucas_JUV &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;700 & FL&lt;=2100 | Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;700 & FL&lt;=2250 | Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;700 & FL&lt;=2250) %&gt;%\n  mutate(Estimated_Stage=\"JUV\")\n\nC.leucas_MAT &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;2100 | Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;2250 | Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;2250) %&gt;%\n  mutate(Estimated_Stage=\"MAT\")\n\nNow you have a bunch of individual data.frames that we need to put back together into a single data.frame. We can do this using bind_rows() which will combine data.frames that have the same set of columns.\n\nelasmos_stage &lt;- bind_rows(C.brevipinna_YOY, C.brevipinna_JUV, C.brevipinna_MAT,\n                           C.leucas_YOY, C.leucas_JUV, C.leucas_MAT)\n\n\n\n\n6 Remember, you can use & and | to combine two conditionsThis solution fits into our general scheme of “split-apply-combine” - except that we actually created multiple objects during our “split” stage. Is there a way to do this without creating individual objects?\n\n\n\n\n\n\nSolution 2\n\n\n\n\n\nIndeed, our second option circumvents having to first create subsets of the initial data.frame using something called a “conditional mutate”.\n\nelasmos_stage &lt;- elasmos %&gt;%\n  filter(Species %in% c(\"Carcharhinus_leucas\", \"Carcharhinus_brevipinna\")) %&gt;%\n  mutate(Estimated_Stage = case_when(Species == \"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&lt;=812 |\n                                       Species == \"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&lt;=844 |\n                                       Species == \"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&lt;=844 ~ \"YOY\",\n         Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;812 & FL&lt;=1380 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;844 & FL&lt;=1360 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;844 & FL&lt;=1360 ~ \"JUV\",\n         Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;1380 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;1360 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;1360 ~ \"MAT\",\n         Species==\"Carcharhinus_leucas\" & FL&lt;=700 ~ \"YOY\",\n         Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;700 & FL&lt;=2100 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;700 & FL&lt;=2250 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;700 & FL&lt;=2250 ~ \"JUV\",\n         Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;2100 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;2250 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;2250 ~ \"MAT\"))\n\nThis is of course a fairly complicated conditional mutate as we are generally combining multiple conditions per category. In this case we could also leave out the | and instead add a ~ STAGE to each line depending on our coding preferences.\n\n\n\nNormally, we would have to extend our code to estimate life history stage for all of our sampled individuals but I have done this for you and you can load that file from your data folder.\n\nelasmos &lt;- read_delim(\"data/elasmos_complete.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse this data set to create a table with the number of individuals per life history stage caught at each site.\n\n\nThis is what you table should look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nSite\nTotal\nYOY\nJUV\nMAT\nUND\n\n\n\n\nCarcharhinus_brevipinna\nAransas_Bay\n12\n11\n0\n0\n1\n\n\nCarcharhinus_brevipinna\nCorpus_Christi_Bay\n46\n45\n0\n0\n1\n\n\nCarcharhinus_brevipinna\nRedfish_Bay\n12\n8\n3\n0\n1\n\n\nCarcharhinus_leucas\nAransas_Bay\n3\n0\n3\n0\n0\n\n\nCarcharhinus_leucas\nCorpus_Christi_Bay\n4\n4\n0\n0\n0\n\n\nCarcharhinus_leucas\nRedfish_Bay\n1\n0\n1\n0\n0\n\n\nCarcharhinus_limbatus\nAransas_Bay\n1\n0\n1\n0\n0\n\n\nCarcharhinus_limbatus\nCorpus_Christi_Bay\n1\n1\n0\n0\n0\n\n\nCarcharhinus_porosus\nCorpus_Christi_Bay\n1\n0\n1\n0\n0\n\n\nHypanus_americanus\nAransas_Bay\n3\n0\n0\n3\n0\n\n\nHypanus_americanus\nCorpus_Christi_Bay\n1\n0\n0\n1\n0\n\n\nHypanus_americanus\nRedfish_Bay\n7\n0\n3\n4\n0\n\n\nHypanus_sabina\nAransas_Bay\n9\n0\n0\n9\n0\n\n\nHypanus_sabina\nCorpus_Christi_Bay\n2\n0\n0\n2\n0\n\n\nRhinoptera_bonasus\nRedfish_Bay\n1\n0\n0\n1\n0\n\n\nRhizoprionodon_terraenovae\nAransas_Bay\n1\n1\n0\n0\n0\n\n\nRhizoprionodon_terraenovae\nCorpus_Christi_Bay\n5\n5\n0\n0\n0\n\n\nRhizoprionodon_terraenovae\nRedfish_Bay\n8\n8\n0\n0\n0\n\n\nSphyrna_lewini\nCorpus_Christi_Bay\n4\n1\n3\n0\n0\n\n\nSphyrna_tiburo\nAransas_Bay\n1\n0\n1\n0\n0\n\n\nSphyrna_tiburo\nCorpus_Christi_Bay\n18\n0\n14\n3\n1\n\n\nSphyrna_tiburo\nRedfish_Bay\n16\n1\n9\n6\n0\n\n\n\nTable 10.2: Number of individuals per species caught at each site by life history stage.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe your results to compare total catch across sites accounting for differences in life history stage.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset your data to contain only YOY and generate a table to investigate whether they were caught across all years sampling occured. Summarize your results in 2-3 sentences.\n\n\nThis is what your table should look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSite\nSpecies\n2015\n2016\n2017\n2018\n\n\n\n\nAransas_Bay\nCarcharhinus_brevipinna\n7\n0\n3\n1\n\n\nAransas_Bay\nRhizoprionodon_terraenovae\n0\n0\n1\n0\n\n\nCorpus_Christi_Bay\nCarcharhinus_brevipinna\n0\n6\n16\n23\n\n\nCorpus_Christi_Bay\nCarcharhinus_leucas\n1\n3\n0\n0\n\n\nCorpus_Christi_Bay\nCarcharhinus_limbatus\n0\n1\n0\n0\n\n\nCorpus_Christi_Bay\nRhizoprionodon_terraenovae\n0\n3\n1\n1\n\n\nCorpus_Christi_Bay\nSphyrna_lewini\n0\n0\n1\n0\n\n\nRedfish_Bay\nCarcharhinus_brevipinna\n1\n0\n3\n4\n\n\nRedfish_Bay\nRhizoprionodon_terraenovae\n4\n4\n0\n0\n\n\nRedfish_Bay\nSphyrna_tiburo\n1\n0\n0\n0\n\n\n\nTable 10.3: Number of YOY caught at each site in each sampling year."
  },
  {
    "objectID": "10_relational-data.html#comparison-of-cpue-per-species-across-sites",
    "href": "10_relational-data.html#comparison-of-cpue-per-species-across-sites",
    "title": "10  Relational data",
    "section": "10.2 Comparison of CPUE per species across sites",
    "text": "10.2 Comparison of CPUE per species across sites\n\n\n\n\n\n\n Consider this\n\n\n\nConsider disadvantages of using absolute counts of occurrence to compare composition across sites. What measure could you use instead of total catch to fix this issue?\n\n\nCatch-per-unit-effort (CPUE) is an indirect measure of abundance. Essentially, it is a way to measure relative abundance and be able to account for differences in sampling effort - the key is defining how you will measure “effort”.\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss what measures we could use to determine effort.\n\n\nWe are going to calculate effort as “hook hours”.\nTo do this we need to know how many hooks were on the line per set7 and how long the entire line was in the water per set (this is called soak time), then we can easily calculate hook hours of each set as the number of hooks multiplied by the soak time. And then we can divide the number of e.g. sharks caught on a set (“catch) by hook hours (”effort”) to calculate CPUE.7 A set means that baited hooks on leaders (individual lines) where attached to the main line and that main line was then “set” in the water for a given period of time before pulling it back in and determining which fish were caught on hooks.\nYour data folder contains as tab-delimited file with set meta-data. This includes information that describes the set itself including date of the set, site, soak time, and location and also parameters describing the conditions of the set such as temperature, salinity, depth, and dissolved oxygen.\nLet’s read in the data set.\n\nset_meta &lt;- read_delim(\"data/set_data.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a quick look at the data set to determine what columns are included and what information we can learn about each individual set. How can you amend the data set to include hook hours?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCorrect, a simple mutate() will do the trick.\n\nset_meta &lt;- set_meta %&gt;%\n  mutate(Hook_Hours = Hooks * Soak_Time)\n\n\n\n\nNext we need to count the number of sharks caught per set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIf we look at our elasmo data.frame you will notice that we have a column called Set but that number indicates the nth set of a give sample day. How can you add a column called Set_ID that consists of the date and the set number?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nelasmos &lt;- elasmos %&gt;%\n  unite(Set_ID, Year, Month, Day, Set, sep = \"_\", remove = FALSE) %&gt;%\n  arrange(Set_ID)\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow create a new object called elasmos_set that contains the number of sharks caught per set.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nelasmos_set &lt;- elasmos %&gt;%\n  count(Set_ID)\n\n\n\n\nNow we have two data.frames one contains the information on how many sharks were caught per set and a second one that contains information about the set, including hook hours. This means that our next step will need to be to combine these two data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nEarlier we learned about bind_rows() which allows us to combine two data.frames that contain identical columns, i.e. row-wise. There is an equivalent function called bind_columns() which allows us to combine data.frames column-wise.\nConsider what the problem would be in using bind_columns() to combine these two data sets.\n\n\nHaving multiple tables containing data pertaining to the same question is referred to as relational data - we are interested in how the contents of a pair of table related to each other, not just the individual data sets. Combining two tables is called a join. In this case the type of join we want to execute is called a mutating join which means we can add new variables from one data.frame (set_meta) to matching observations in another (elasmos_set).\nIn order to do that we need to have one column (the key) that way the function can match observations in one data.frame by that key and then copy the matching observations in the columns from the second data.frame across.\nWhen performing a join, new columns are added to the right. We will use the function full_join() which means that all the rows from the left and right data.frame will be retained - when we used count() that excluded sets where no sharks were caught, by using a full_join() we can add those back in.\nWe currently do not have a matching column between the two data sets, so our first step will be to add a new column called Set_ID to our set_meta data.frame, then we can use full_join() to join the two tables. The argument by can be used to specify the column to use as the key. For our example here we have a column with the same name - in general, the function is “smart” enough to identify shared columns and so you do not necessarily have to specify it.\n\n\nYou can pull up the help page using ?full_join to learn how to join tables that have multiple columns in common or that might have a column in common though it is named differently between the two tables.\nNote the notation elasmo_set &lt;- full_join(elasmos_set, set_meta, by = \"Set_ID\") will produce the same result as the syntax we are using here.\n\n# add set id column\nset_meta &lt;- set_meta %&gt;%\n  unite(Set_ID, Year, Month, Day, Set, sep = \"_\", remove = FALSE)\n\n# join data sets\nelasmos_set &lt;- elasmos_set %&gt;%\n  full_join(set_meta) %&gt;%\n  replace_na(list(n = 0))\n\nNow we can calculate CPUE for sharks per site.\n\nelasmos_set &lt;- elasmos_set %&gt;%\n  mutate(CPUE = n/Hook_Hours)\n\nAnd from that we can easily calculate mean and standard deviation CPUE of catching sharks by site.\n\n\n\n\n\n\n\nSite\nmean_CPUE\nstd_CPUE\n\n\n\n\nAransas_Bay\n0.0048954\n0.0089735\n\n\nCorpus_Christi_Bay\n0.0135243\n0.0185803\n\n\nRedfish_Bay\n0.0069282\n0.0114742\n\n\n\nTable 10.4: mean +/- sd CPUE\n\n\nWe are going to perform a Kruskal-Wallis rank sum test to determine if there is significant heterogeneity among sites8.8 You are probably more familiar with the framework of using an ANOVA to test for significant heterogeneity and pairwise t-tests to test for equality of means of a set of values. KW is similar but is a non-parametric approach and does not make assumptions about the distribution of values.\n\n# KW to test for significant heterogeneity\nkruskal.test(CPUE ~ Site, data = elasmos_set)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  CPUE by Site\nKruskal-Wallis chi-squared = 12.325, df = 2, p-value = 0.002106\n\n\nAnd we will follow that using a Dunn’s test for pairwise comparisons.\n\n# post-hoc Dunn test\ndunnTest(CPUE ~ Site, data = elasmos_set, method = \"bh\")\n\n                        Comparison          Z      P.unadj       P.adj\n1 Aransas_Bay - Corpus_Christi_Bay -3.3553910 0.0007925288 0.002377586\n2        Aransas_Bay - Redfish_Bay -0.7980727 0.4248282807 0.424828281\n3 Corpus_Christi_Bay - Redfish_Bay  2.5669868 0.0102586520 0.015387978\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe your results and discuss what this result could mean for our overarching question of identifying shark nurseries.\n\n\nOf course, we are interested how CPUE compares across species and sites.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChoose one species and calculate the CPUE per set. For convenience convert CPUE to effort per 1000 hook hours and then calculate the mean CPUE per 1000 hooks per site for that species.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what that could look like for a single species.\n\nspecies &lt;- \"Carcharhinus_brevipinna\"\n\nspecies_CPUE &lt;- elasmos %&gt;%\n  filter(Species == species) %&gt;%\n  count(Set_ID) %&gt;%\n  full_join(set_meta) %&gt;%\n  replace_na(list(n = 0)) %&gt;%\n  mutate(CPUE = n/Hook_Hours,\n         CPUE_1000 = CPUE * 1000) %&gt;%\n  group_by(Site) %&gt;%\n  summarize(mean_CPUE = mean(CPUE_1000))\n\n\n\n\nFor better presentation, we probably want to convert that do a wider data set; your results should look like this.\n\n\n\n\n\n\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\n\n\n\n\n1.91\n7.88\n1.76\n\n\n\nCatch per unit effort (1000 hook hours) for each site.\n\nWe actually want to have this information for all species, rather than create individual data.frames for each species and then combine those using bind_rows(), I will show you a more efficient way of coding this using a for loop.\n\n# create empty list\nspecies_CPUE &lt;- list()\n\n# calculate hook hours for each species per site\nfor(species in unique(elasmos$Species)){\n  \n  species_CPUE[[species]] &lt;- elasmos %&gt;%\n    filter(Species == species) %&gt;%\n    count(Set_ID) %&gt;%\n    full_join(set_meta) %&gt;%\n    replace_na(list(n = 0)) %&gt;%\n    mutate(CPUE = n/Hook_Hours,\n           CPUE_1000 = CPUE * 1000)\n\n}\n\n# combine data frames in list into single \nCPUE &lt;- bind_rows(species_CPUE, .id = \"Species\")\n\nNext, we would want to run KW tests to determine if there are significant differences among sites for each species.\n\n# create empty dataframe for results\nresults &lt;- setNames(data.frame(matrix(ncol = 2, nrow = 0)), \n                    c(\"Species\", \"pvalue\")) %&gt;%\n  mutate(Species = as.character(Species),\n         pvalue = as.numeric(pvalue))\n\nfor(species in unique(CPUE$Species)){\n  \n  # filter CPUE per species\n  tmp &lt;- CPUE %&gt;%\n    filter(Species == species)\n  \n  # KW to test for significant heterogeneity\n  KW &lt;- kruskal.test(CPUE ~ Site, data = tmp)\n  \n  # extract p-value\n  df &lt;- data.frame(\"Species\" = species,\n                   \"pvalue\" = as.numeric(KW$p.value))\n  \n  results &lt;- bind_rows(results, df)\n\n}\n\nLet’s calculate mean CPUE per species and site, turn that into a wide table for easier comparison and add the p-values.\n\nCPUE_sign &lt;- CPUE %&gt;%\n    group_by(Species, Site) %&gt;%\n    summarize(mean_CPUE = mean(CPUE_1000)) %&gt;%\n    pivot_wider(names_from = Site, values_from = mean_CPUE) %&gt;%\n    left_join(results) %&gt;%\n    arrange(Species)\n\nOnce we’ve run that code to wrangle and transform our data we can compare CPUE for each species and site.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\npvalue\n\n\n\n\nCarcharhinus_brevipinna\n1.91\n7.88\n1.76\n0.00\n\n\nCarcharhinus_leucas\n0.49\n0.52\n0.14\n0.54\n\n\nCarcharhinus_limbatus\n0.14\n0.18\n0.00\n0.60\n\n\nCarcharhinus_porosus\n0.00\n0.15\n0.00\n0.37\n\n\nHypanus_americanus\n0.49\n0.14\n1.08\n0.25\n\n\nHypanus_sabina\n1.50\n0.29\n0.00\n0.00\n\n\nRhinoptera_bonasus\n0.00\n0.00\n0.16\n0.37\n\n\nRhizoprionodon_terraenovae\n0.14\n0.90\n1.29\n0.22\n\n\nSphyrna_lewini\n0.00\n0.50\n0.00\n0.02\n\n\nSphyrna_tiburo\n0.22\n2.96\n2.49\n0.00\n\n\n\nTable 10.5: Catch per unit effort (per 1000 hook hours) for each species by site, p-value indicates whether there are significant differences among sites for a given species.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nUse the table with CPUE per species in your lab manual to briefly describe the results comparing CPUE per species and site and relate that to our overarching question of identifying shark nurseries9.\n\n\n9 Normally, we would want to run additional pairwise tests for each species where there are significant differences among sites, but we’ll skip that step for now and stick to the big picture."
  },
  {
    "objectID": "10_relational-data.html#comparison-of-cpue-for-different-life-history-stages",
    "href": "10_relational-data.html#comparison-of-cpue-for-different-life-history-stages",
    "title": "10  Relational data",
    "section": "10.3 Comparison of CPUE for different life history stages",
    "text": "10.3 Comparison of CPUE for different life history stages\nOf course, we are not only interested in which species were caught at each site, we also want to know what life history stages those individuals were at when they were caught.\nWe will use a similar strategy as above to create a data frame with CPUE per site, species, and life history stage and produce a table comparing the means.\n\n# create empty list\nspecies_CPUE &lt;- list()\n\n# calculate hook hours for each species per site\nfor(species in unique(elasmos$Species)){\n  \n  for(stage in unique(elasmos$Estimated_Stage)){\n    \n      species_CPUE[[paste(species, stage, sep = \":\")]] &lt;- elasmos %&gt;%\n        filter(Species == species & Estimated_Stage == stage) %&gt;%\n        count(Set_ID) %&gt;%\n        full_join(set_meta) %&gt;%\n        replace_na(list(n = 0)) %&gt;%\n        mutate(Estimate_Stage = stage, \n               CPUE = n/Hook_Hours,\n               CPUE_1000 = CPUE * 1000)\n\n  }\n  \n}\n\n# combine data frames in list into single \nCPUE &lt;- bind_rows(species_CPUE, .id = \"Species_Stage\") %&gt;%\n  select(Species_Stage, Set_ID, Site, Hooks, Soak_Time, Hook_Hours, CPUE, CPUE_1000) %&gt;%\n  separate(Species_Stage, into = c(\"Species\", \"Stage\"), sep = \":\", remove = FALSE) %&gt;%\n    group_by(Species_Stage, Site) %&gt;%\n    summarize(mean_CPUE = mean(CPUE_1000)) %&gt;%\n    pivot_wider(names_from = Site, values_from = mean_CPUE) %&gt;%\n    filter(if_any(c(Aransas_Bay, Corpus_Christi_Bay, Redfish_Bay), ~ . &gt; 0)) %&gt;%\n    separate(Species_Stage, into = c(\"Species\", \"Stage\"), sep = \":\") %&gt;%\n    filter(!Stage == \"UND\")\n\nThis will produce the following table summarizing the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nStage\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\n\n\n\n\nCarcharhinus_brevipinna\nJUV\n0.0000\n0.0000\n0.4586\n\n\nCarcharhinus_brevipinna\nYOY\n1.7477\n7.7283\n1.1510\n\n\nCarcharhinus_leucas\nJUV\n0.4940\n0.0000\n0.1357\n\n\nCarcharhinus_leucas\nYOY\n0.0000\n0.5209\n0.0000\n\n\nCarcharhinus_limbatus\nJUV\n0.1443\n0.0000\n0.0000\n\n\nCarcharhinus_limbatus\nYOY\n0.0000\n0.1794\n0.0000\n\n\nCarcharhinus_porosus\nJUV\n0.0000\n0.1480\n0.0000\n\n\nHypanus_americanus\nJUV\n0.0000\n0.0000\n0.4745\n\n\nHypanus_americanus\nMAT\n0.4872\n0.1444\n0.6095\n\n\nHypanus_sabina\nMAT\n1.4967\n0.2940\n0.0000\n\n\nRhinoptera_bonasus\nMAT\n0.0000\n0.0000\n0.1614\n\n\nRhizoprionodon_terraenovae\nYOY\n0.1443\n0.8970\n1.2884\n\n\nSphyrna_lewini\nJUV\n0.0000\n0.3768\n0.0000\n\n\nSphyrna_lewini\nYOY\n0.0000\n0.1210\n0.0000\n\n\nSphyrna_tiburo\nJUV\n0.2217\n2.3098\n1.4763\n\n\nSphyrna_tiburo\nMAT\n0.0000\n0.4455\n0.8716\n\n\nSphyrna_tiburo\nYOY\n0.0000\n0.0000\n0.1468\n\n\n\nTable 10.6: Comparison of CPUE by life history stage for all observed life history stages in Aransas, Corpus Christi, and Redfish Bay.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe the results comparing CPUE per life history stage and site; these results are all statistically significant.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nRelate your all of our results back to the overarching question of identifying shark nurseries in Texas Bays and Estuaries to write a short conclusion in terms of what this study has (or has not demonstrated)."
  },
  {
    "objectID": "13_greenhouse-gases.html#unequivocal-and-unprecedented",
    "href": "13_greenhouse-gases.html#unequivocal-and-unprecedented",
    "title": "13  Climate change: Drivers",
    "section": "13.1 Unequivocal and unprecedented?",
    "text": "13.1 Unequivocal and unprecedented?\nCurrent climate change is concerning not (only) because of absolute warmth of the earth but due to rate at which it is occurring. What this means is that over the history of our planet there have been times at which temperatures have been higher that what we are currently experiencing. The component that is “unprecedented” is the rate of increase and impact that is having on our planet22 We will look at examples of that impact using various data set describing change in the earth-climate system.\nIn this chapter, we are going to dive into a series of data sets that will allow us to explore whether we do indeed have evidence that the climate change we are currently observing is indeed unequivocal, driven by anthropogenic effects, and unprecedented in its rate.\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we get started let’s consider what data sets we need to investigate whether rates of temperature increase are indeed “unprecedented”, to identify patterns of atmospheric CO2 concentrations consistent with anthropogenically driven climate change, and what we would expect our results to look like if the IPCC assessment is indeed correct in their claims.\nTake a few minutes to write out your thoughts for the following prompts:\n\nwhat variables/measurements do we need?\nwhat comparisons do we need to be able to make to determine if the rate of temperature increase is unprecedented?\nwhat would patterns consistent with anthropogenically driven climate change look like?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe will also need to consider limitations of our approach. In this case, we will need to assess whether our approach is investigating a causal or mechanistic effect in and of itself or if we are uncovering evidence consistent with a known mechanism/process."
  },
  {
    "objectID": "13_greenhouse-gases.html#current-rates-of-air-temperature-change",
    "href": "13_greenhouse-gases.html#current-rates-of-air-temperature-change",
    "title": "13  Climate change: Drivers",
    "section": "13.2 Current rates of air temperature change",
    "text": "13.2 Current rates of air temperature change\nLet’s start by taking a look at changes in global mean air temperatures.\n\n\n\n\n\n\n Consider this\n\n\n\nSketch out what the patterns of air temperature over time would look like if the (A) earth is warming, (B) cooling, (C) not changing at all and discuss with your class mates3. Determine what variables you would plot on the x-axis, y-axis and what the slope would look like. Then practice describing figures with a 1-2 sentence description of what each scenario.\n\n\n3 You can do this on a piece of paper and snap a picture or sketch directly on a computer/mobile device in an appropriate app. Save your picture as scratch/glob-temp-patterns.jpg and you can use the markdown code below to import it. If you end up with a different filename or file format, adjust your code accordingly.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\nExpectations of change in air patterns over time for different scenarios\n\n\n\n\nThe air temperature data we will be using is compiled by the Goddard Institute for Space Studies (NASA) and can be accessed in on their webpage which also describes how their data set was compiled and processed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nAccess the GISS Surfact Temperature Analysis Webpage and scroll down the page to the section Tables of Global and Hemispheric Monthly Means and Zonal Annual Means and download the CSV version of the Global-mean monthly seasonal, and annual means data set.\nAfter you have downloaded the data set place it in your data folder in your project directory and read it into your Global Environment.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRemember to correctly specify the delim argument for a *.csv file.\n\n# read csv file\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThis data is presented as temperature anomalies, i.e. as deviations from the corresponding 1951 - 1980 mean. Explain what this means and argue why this is a more appropriate way to present this data than to simply use the measured global temperature.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\n\nGenerally, climate patterns are long-term patterns. To describe climate patterns we generally use at least 30 years worth of data.\nAccording to Oxford Languages, the definition of an anomaly is “something that deviates from what is standard, normal, or expected”.\nIn the context of climate change is in more important to understand the actual temperature or the temperature change?\n\n\n\n\nBefore we move on, let’s check out our temperature object in the Global Environment to see if the file read in okay - spoiler alert, it did not4.4 You should create a habit of always checking that your data has read in as expected, immediately determining that something is wrong and correcting it will minimize issues with troubleshooting down the line.\nTroubleshooting Skills: Your file didn’t read in correctly. Let’s figure out why.\n\n\n\n\n\n\n Consider this\n\n\n\nWhat ideas do you have for us to track down the issue? Document the process you used to identify and fix the issue using short bullet points for future reference.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA good starting point is to open the file in a text editor to see if we missed anything. Rstudio now has a built in text editor. Use the file navigation pane to navigate to the data folder. Clicking on the temperature file will create a pop up, select View File and it will pop up in the View pane in a separate tab next to your quarto pane5.\nSure enough there seems to be an additional line at the beginning which is probably causing the issue. One way to fix this is to simply delete the extra line … before you do this, remember our first module looking at data wrangling and the cardinal rules we set in place? One key principle is “DO NOT EDIT YOUR RAW FILES”: if we want to have a reproducible workflow we should avoid manually editing our data sets.\nInstead use ?read_delim to pull up the help file for the function. You should find an argument called skip which will let us tell the function how many extra lines there are at the beginning of the file.\n\n# read csv file skipping the first line\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\", skip = 1)\n\n# check that dataframe is in order\nhead(temperature)\n\n# A tibble: 6 × 19\n   Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug Sep   Oct   Nov   Dec  \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1  1880 -0.18 -0.24 -0.09 -0.16 -0.1  -0.21 -0.18 -0.1  -.15  -.23  -.22  -.17 \n2  1881 -0.19 -0.14  0.03  0.05  0.06 -0.18  0    -0.04 -.15  -.22  -.18  -.07 \n3  1882  0.16  0.14  0.04 -0.17 -0.15 -0.23 -0.16 -0.07 -.14  -.24  -.17  -.36 \n4  1883 -0.29 -0.37 -0.12 -0.18 -0.17 -0.08 -0.06 -0.14 -.21  -.11  -.23  -.11 \n5  1884 -0.13 -0.08 -0.36 -0.4  -0.34 -0.36 -0.3  -0.27 -.27  -.25  -.33  -.31 \n6  1885 -0.58 -0.34 -0.27 -0.42 -0.45 -0.43 -0.34 -0.31 -.29  -.24  -.24  -.11 \n# ℹ 6 more variables: `J-D` &lt;chr&gt;, `D-N` &lt;chr&gt;, DJF &lt;chr&gt;, MAM &lt;dbl&gt;,\n#   JJA &lt;dbl&gt;, SON &lt;chr&gt;\n\n\nThat seems to have done the trick! Put that in your bag of tricks for future reference.\n\n\n\n5 Be really careful not to accidentally edit the raw data file!Now that that is resolve, let’s take a slightly more detailed look at our data set to make sure there aren’t any additional changes we need to make. For example we need to determine if there are NA values and if they are properly formatted and we need to make sure all the columns are numeric.\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your data frame, and make a few notes to document what information each column contains, if anything is out of order, and ideas on how to clean up any issues you have identified.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNA values have been specified as ***, which has caused some columns to be formatted as character instead of numeric.\nWe can use replace() to search every column (we specify this using . instead of a specific column name) and mutate_if() which tells R to check every column and if it is a character data type (is.character) to convert it to numeric (as.numeric).\n\ntemperature &lt;- temperature %&gt;%\n  replace(. == \"***\", NA) %&gt;%\n  mutate_if(is.character, as.numeric)\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nNow that we are all set, let’s create a plot with the yearly mean global temperature anomaly across time. Briefly conceptually describe what parameters we need to plot on the x and y-axis to accomplish this.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are going to use ggplot2 which is part of the tidyverse to plot our figure. This will introduce you to some of the standard syntax. Fear not, we will work through the details of the framework on which ggplot2 relies in the next chapter so think of this as a sneak peak to get used to the syntax.\n\nggplot(temperature, aes(x = Year, y = `J-D`)) +             # define data set and columns to plot on x and y axis\n  geom_line(color = \"blue\", size = 1) +                     # plot line plot\n  labs(x = \"year\", y = \"temperature anomaly [C]\",           # determine labels\n       title = \"Annual mean global surface temperature relative to 1951-1980 average.\",\n       caption = \"source: NASA Goddard Institute for Space Studies\")\n\nHere’s what your figure should look like based on that code.\n\n\n\n\n\nFigure 13.1: Change in annual mean global surface temperature from 1880 - 2021 relative to 1951-1980 average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe & interpret the figure: Describe the change in the deviation of global temperature to the 1950-1980 mean. Include and explanation of what it means for values to be negative or positive.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThis figure gives us a qualitative view of changing global temperatures.\nFor a quantitative assessment we would want to determine the rate of change. For this case we would define the rate of change as the change in temperature divided by the change in time for a certain time period. A more general definition would be that you are calculating the slope of the line you have fit through the data as the change-in-y divided by the change-in-x6.6 If you compare the two figure you should see that fitting a linear regression is an oversimplification but it will allow us to make a quantitative comparison\nWe can visualize this by adding a layer to our figure using geom_smooth() and setting the method to lm (linear regression).\n\nggplot(temperature, aes(x = Year, y = as.numeric(`J-D`))) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", size = 1, se = FALSE) +\n  labs(x = \"year\", y = \"temperature anomaly [C]\",\n       title = \"Annual mean global surface temperature relative to 1951-1980 average.\",\n       caption = \"source: NASA Goddard Institute for Space Studies\")\n\nHere is what your figure will look like with that added layer.\n\n\n\n\n\nFigure 13.2: Linear regression (red) for change in annual mean global surface anomaly from 1880-2021 (blue). Temperature anomaly measured relative to 1951-1980.\n\n\n\n\nThat’s helpful in terms of a visualization but for a to really quantitatively assess the rate of change, we need to fit a linear regression as y = mx + b. With m as the slope and b as the intercept to determine the rate of change; with that equation we can then determine the rate of change by extracting m. The larger the slope m, the steeper the fitted line and the more rapid the change in temperature.\nWe can fit the linear regression using the function lm().\n\n# fit linear regression\nscore_model &lt;- lm(`J-D` ~ Year, data = temperature)\n\n# view summary of results\nsummary(score_model)\n\n\nCall:\nlm(formula = `J-D` ~ Year, data = temperature)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36171 -0.13297 -0.02646  0.12807  0.45284 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.515e+01  7.106e-01  -21.32   &lt;2e-16 ***\nYear         7.797e-03  3.641e-04   21.41   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1798 on 141 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7648,    Adjusted R-squared:  0.7631 \nF-statistic: 458.5 on 1 and 141 DF,  p-value: &lt; 2.2e-16\n\n\nWe are interested in the equation of the regression line, how well the data fits the line, and whether or not the regression is significant. We can use the output of the linear regression to determine that. The estimate columns shows the values for the intercept b and the slope m for the variable (in this case year).\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your output from the linear regression model and determine what the equation for our line of best fit looks like and use this to determine the rate of change, including what the units would be.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThe adjusted R2 value describes the proportion of variance of the dependent value explained by the independent variable. In our figure what is the dependent and the independent value? How well does the regression fit our data?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nInterpret the results of your analysis to determine whether you have evidence of a warming earth.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\n\nIdentify the time period where you observe the steepest rate of increase.\nCreate a subset of the data set to contain only that time frame.\nPlot you subset of data with a linear regression.\nPerform a statistical analysis to determine the rate of change.\nSummarize your results and compare them to your results above.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nThe time period with steep(est) rate of increase I selected is XXX - XXX.\n\n# create subset of data\n\n\n# Plot subset of data with a linear regression\n\n\n# linear regression\n\n[Summarize your results and compare them to your results above.]"
  },
  {
    "objectID": "13_greenhouse-gases.html#rate-of-change-of-atmospheric-co2",
    "href": "13_greenhouse-gases.html#rate-of-change-of-atmospheric-co2",
    "title": "13  Climate change: Drivers",
    "section": "13.3 Rate of change of atmospheric CO2",
    "text": "13.3 Rate of change of atmospheric CO2\nNext, let’s determine the rate of change of atmospheric CO2.\nDr. Charles David Keeling (1928 - 2005) began collecting atmospheric CO2 concentration data at the Mauna Observatory (Hawaii); this data set comprises the longest measurement of atmospheric CO2 concentrations7. This data set has been fundamental to our understand the role of human activities such as fossil fuel burning in driving climate change.7 Longest measurement using instruments - we will see later that we can use proxy data to indirectly measure CO2 levels for much longer time periods\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly argue why Hawaii is a good location for a long-term monitoring station for CO2.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe can access the data set directly from the Global Monitoring Laboratory. Select the Data tab, then download the csv data set Mauna Loa CO2 annual mean data.\nOnce you have downloaded the data set, make sure to move it to the data folder of your project directory, then read it into Rstudio using the follow code8.8 If you look at the raw data using a text editor you will quickly see why we need to include the skip = 55 argument\n\nCO2 &lt;- read_delim(\"data/co2_annmean_mlo.csv\", delim = \",\", skip = 55)\n\nhead(CO2)\n\n# A tibble: 6 × 3\n   year  mean   unc\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1959  316.  0.12\n2  1960  317.  0.12\n3  1961  318.  0.12\n4  1962  318.  0.12\n5  1963  319.  0.12\n6  1964  320.  0.12\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nExplore your data set to determine if it is correctly formatted and briefly describe what information it contains - bullet points are fine.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPlot the change in CO2 over time and use a linear regression to determine the rate of change in atmospheric CO2 over the entire data set based on your analysis. Briefly summarize your results and argue how confident you are in these results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n# plot the change over time \n\n\n# perform linea regression\n\n[Your results here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCompare your results of rates of change of average global temperature and atmospheric CO2 and describe the phenomenon that can be used to explain this.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\n\n\n\n\n\nFigure 13.3: Comparison of increase in mean global temperature anomaly relative to 1951-1980 (left) and increase in mean atmospheric CO2 concenttrations (right panel) measured a Mauna Loa Observatory. Regression lines indicating temporal trend included in red."
  },
  {
    "objectID": "13_greenhouse-gases.html#comparison-of-current-and-pre-historic-rates-of-change",
    "href": "13_greenhouse-gases.html#comparison-of-current-and-pre-historic-rates-of-change",
    "title": "13  Climate change: Drivers",
    "section": "13.4 Comparison of current and pre-historic rates of change",
    "text": "13.4 Comparison of current and pre-historic rates of change\nThe most recent IPCC assessment has labeled the increase in temperatures driving contemporary climate change as “unprecedented”. However, temperatures have changed in earth’s past, and temperatures have even been higher than what we are experiencing now. What is unprecedented is the rate at which this is occurring, at least according to the IPCC.\nShall we investigate?\nTo do this we will need to look at past climate change. The two data sets we just took a look at are measurements of temperature and CO2 using instrumentation, i.e. we have directly measured values for the parameters we are interested in at different points in time. Dr. Keeling was one of the first scientists to consider the importance of long-term monitoring sites. The rapid changes taking place in our environment have created a push to generate long-term data sets with a focus on making the accessible. We will take a look at some of these data sets later in this semester and you will likely use at least one of them for your own data science project.\nHowever, we frequently have questions that might extend beyond data sets like the two we used above.\nHow can we access data from before we had instrumentation? One way to do this we have to use so-called proxy data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nIn general, a proxy is an intermediary or subsistute, i.e. it is a parameter that can be used to represent the value of something in a calculation. Paleoclimatologists use preserved physical characteristics of the environment to stand in for direct measurements using instruments, typical examples would be ice cores, tree rings, ocean sediments, or fossil pollen. Briefly discuss the pros and cons of proxy data compared to direct measurements using specialized instruments.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nFor a deep dive on proxy data sets and climate archives, you can check out a detailed interactive brief on proxy data.\nFor our assessment we are going to explore proxy data derived from ice cores which next to ocean sediments give us some of the longest records of past climate conditions.\nHundreds of ice cores extracted from polar ice have proven valuable to understanding changes in atmospheric chemistry over pre-historic time. Here, we can make use of the fact that as the ice is formed, air bubbles are trapped. Because these air bubbles have remained frozen, they still have the same composition of gases as at the time they were trapped. The depth of an ice core is correlated to time, deeper ice is older. In other words, ice cores form an archive of atmospheric conditions over time. We can directly measure CO2 from the air bubbles trapped in the ice and we can measure isotopic ratios of oxygen in the water molecules of the core to derive temperature.\nVostok Ice core data set has been constructed using ice cores from the Vostock research station in the Antarctica and can be access through the Carbon Dioxide Information Analysis Center.\nLet’s start by taking a look at the temperature data. Use the code below to read the data set that has been downloaded and placed in the data folder for you into R as a data frame9.9 We are using a slightly different method from before which allows us to directly download the data into our data folder. We are also using read_table2() from the readr package due to the fact that our text file is formatted using neither white space nor tabs.\n\n# load dataset\nvostok_temp &lt;- read_table2(\"data/vostok_temp.txt\",\n                     skip = 60,\n                     col_names = c(\"depth\", \"age_ice\", \"deuterium\", \"temperature\"))\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at the data set and briefly describe what data is contained in the data set (you may want to take a peak at the original text file to get a better understanding).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow we can create a plot of the temperature data over time. The age is recorded as years before present. For better visualization we will convert this to “thousand years ago” by dividing that number by 1,000.\n\nggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature variation during glacial/interglacial periods\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n\n\n\n\n\nFigure 13.4: Temperature variation during glacial/interglacial periods derived from air bubbles in Vostock ice cores.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we move at looking at rates of change, we first need to determine how to properly read this plot. Think through the following aspects:\n\nWhat does it mean that time (on the x-axis) is represented as “thousand years ago” or “time before present”? How does this differ from the other time series we have plotted today?\nTemperature is being measured by proxy by looking at differences in isotope ratios; the data file lists this information as “Temperature variation”. What does 0C mean on this plot?\nConsider how long glacial and interglacial periods typically last - are we currently in a glacial or interglacial period?\nIn what parts of the figure is temperature increasing/decreasing (consider slope)?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow add a trend line to our ice core temperature data and run a linear regression model. Argue whether or not you think this trend line is a good representation of long-term temperature change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nPlot temperature data derived from vostok ice cores.\n\nggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature derived from Vostok ice core, Antarctica.\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n\n\n\n\n\nFigure 13.5: Change in temperature during glacial/interglacial periods derived from Vostok ice cores (blue) with fitted linear regression (red).\n\n\n\n\nAnd then we still need to fit a linear regression.\n\n# fit linear regression\nscore_model &lt;- lm(temperature ~ age_ice, data = vostok_temp)\n\n# view summary of results\nsummary(score_model)\n\n\nCall:\nlm(formula = temperature ~ age_ice, data = vostok_temp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7781 -2.3667 -0.5821  1.9830  7.7603 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.631e+00  8.462e-02 -54.725   &lt;2e-16 ***\nage_ice      7.850e-07  4.987e-07   1.574    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.896 on 3309 degrees of freedom\nMultiple R-squared:  0.0007483, Adjusted R-squared:  0.0004463 \nF-statistic: 2.478 on 1 and 3309 DF,  p-value: 0.1155\n\n\nRemember, when you “argue” an answer you need to state your conclusion and then support that statement.\n\n\n\nYou’re right - to determine if the currently observed rate of change is “unprecedented” or not, we need to identify past time periods with the fastest rate of change and calculate them.\n\n\nYou’ve already used plotly in a previous chapter to plot an interactive graph. This will make it a lot easier to identify specific time periods because as you hover over any part of the line graph the pop up will give you the data points. Previously we wrapped an entire function in the ggplotly() function. In this case, it is easier to first create an object that holds the ggplot() output and then use that as the argument for ggplotly().\nTo do this I will show you a little trick to plot an interactive figure:\n\np1 &lt;- ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = .75) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\nggplotly(p1)\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow you can use your cursor to identify individual points on the plot to select the subset you want plot. The plot that subset with a linear best line of fit and run a linear regression to get the slope to compare to current rates of change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere’s what your code could look like, we’ve previously used the technique where you create and object to hold your variables to make it easier to resue the code.\n\n# define time range\nmin_year_vostok &lt;- \nmax_year_vostok &lt;- \n\n# filter data set + plot\nvostok_temp_subset &lt;- vostok_temp %&gt;%\n  filter(age_ice &gt;= min_year_vostok & age_ice &lt;= max_year_vostok)\n\nggplot(vostok_temp_subset, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature change recorded in Vostok ice core (128357 - 138193 years ago).\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n# fit linear regression\nscore_model &lt;- lm(temperature ~ age_ice, data = vostok_temp_subset)\n\n# view summary of results\nsummary(score_model)\n\n\n\n\n\n\nFigure 13.6: Temperature trends (linear regression, red) recorded in Vostok ice core for time period from approx. 138 - 128 kyrs before present.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIdentify two more ranges with increasing temperatures and determine the rate of change during that time period.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow, let’s take a look at past patterns of changes in atmospheric CO2 over time.\n\n# load dataset\nvostok_ice &lt;- read_delim(\"data/vostok_ice.txt\", delim = \"\\t\",\n                     skip = 21,\n                     col_names = c(\"depth\", \"age_ice\", \"age_air\", \"CO2\"))\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSelect one time period with rapid increase in CO2 concentrations and apply what you have learned identifying past periods of rapid temperature increase to calculating the rate of change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "13_greenhouse-gases.html#final-conclusions",
    "href": "13_greenhouse-gases.html#final-conclusions",
    "title": "13  Climate change: Drivers",
    "section": "13.5 Final Conclusions",
    "text": "13.5 Final Conclusions\nNow it’s time to put everything together. You might want to refer back to the beginning of this section when we looked at some background information about the IPCC report, the atmospheric energy budget, formulated our central questions, thought about what data sets when can use to answer our question, and the limitations of our approach.\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we summarize and then interpret our results, let’s re-orient ourselves to what we’ve done with this analysis.\n\nWrite out the central question(s) we are asking.\nList the data sets you used to investigate and what metric you calculated for each.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWell-written papers frequently end their introduction section with a paragraph that summarizes what their study is investigating and how they are investigating that (set of) questions - it forms a “bridge” between the introduction that sets up relevant background information (why is my question important?) and the methods section which is a very detailed account of how data was acquired (experimental design), processed, and analyzed.\nIf you read several of these paragraphs you will realize that they all contain a statement that follows a general formula along these lines:\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF THE TYPE OF DATA SET GENERATED] to [METRICS THAT WERE CALCULATED].\n\nYou should always be able to make a 2-3 sentence statement summarizing what you are investigating and how you did it, it’s a good self-check that you know what you’re trying to accomplish.\n\n\n\n\n\n\n Consider this\n\n\n\nUse your results to compare current changes over the last approx. 200 years in atmospheric and CO2 concentrations and global temperature to pre-historic changes.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nBe strategic in how you structure your answer to put together all the parts of our analysis. For example, start with temperature and compare the rate of change you calculate for recent times to the rate of change you calculated during the more distant past (glacials/interglacials)10. In include your own rate of change + the answers your classmates posted in the slack channel. Then make a statement about whether or not we are currently observing an “unprecedent” change. Then do the same for CO2 concentrations. At this point you are just summarizing and describing your results, you can make clear statements of whether or not rates are positive/negative (your variables are increasing/decreasing), and whether you see the same/opposite trends but you should not yet interpret what those results mean. This would be equivalent to the results section of a lab report or research paper.\n[Your answer here]\n\n\n10 Remember to include units!\n\n\n\n\n\n Consider this\n\n\n\nInterpret your results to assess whether recent changes in temperature are due to natural vs anthropogenic factors. For your answer consider both arguments that attribute the change to anthropogenic factors and natural fluctuations.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nNow you are entering the interpretation/discussion component of your analysis. At this point your now what your key results are and you need to figure out what they mean; essentially you are finally getting around to answering those key questions you asked before designing your experiment/study. You are going to use your results as evidence for/support of your conclusion. This means that you need to demonstrate that your results are consistent with your conclusion. You also want to consider alternative explanations, limitations of your data etc. Of course, ideally you can exclude those based on e.g. evidence from other studies/data sets.\n[Your answer here]"
  },
  {
    "objectID": "13_greenhouse-gases.html#acknowledgments",
    "href": "13_greenhouse-gases.html#acknowledgments",
    "title": "13  Climate change: Drivers",
    "section": "13.6 Acknowledgments",
    "text": "13.6 Acknowledgments\nThese activities are based on the EDDIE Climate Change Module1111 O’Reilly, C.M., D.C. Richardson, and R.D. Gougis. 15 March 2017. Project EDDIE: Climate Change. Project EDDIE Module 8, Version 1."
  },
  {
    "objectID": "E_natural-disasters.html#catastrophic-events-are-sudden-events-causing-significant-damage",
    "href": "E_natural-disasters.html#catastrophic-events-are-sudden-events-causing-significant-damage",
    "title": "(Un)Natural Disasters",
    "section": "Catastrophic events are sudden events causing significant damage",
    "text": "Catastrophic events are sudden events causing significant damage\nA catastrophic event is a sudden and widespread event that causes significant damage, destruction, and often leads to severe consequences for human life, property, and the environment. These events are typically characterized by their scale, impact, and the difficulty in mitigating their effects.\nCatastrophic events can occur naturally or as a result from human activities, and they often overwhelm the ability of individuals, communities, or even entire regions to cope with or respond to them adequately. As a result, preparedness, early warning systems, and mitigation strategies are essential to minimize their impact and aid affected populations. Due to their scale disasters require coordinated response and recovery efforts from governments, emergency services, and humanitarian organizations.\n\n\n\n\n\n\n Consider this\n\n\n\nOne way to categories catastrophic events is according to their cause. Think about what events fall in the broad category of catastrophic events and identify major categories that you could place them into.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is one suggestion. Read through this categorization and consider whether you think this is a helpful classification or whether you would group events differently or perhaps you might want to add certain types of events or you might not consider some of these as catastrophic event at all.\n\nNatural Disasters, e.g. earthquakes, hurricanes/major storms, tsunamis, volcanic eruptions\nClimate-related catastrophes/extreme weather events, e.g. heatwaves, prolonged droughts, wildfires\nMan-made catastrophes, e.g. nuclear accidents, industrial accidents, terrorist attacks\nBiological catastrophes, epidemics, pandemics\nCosmic catastrophes: asteroid impacts2\n\n2 if you don’t think this one is important… just ask the dinosaurs.You could consider climate-related catastrophes a subset of natural disasters, however in our context of thinking through global change making it its own category could be helpful. We generally contrast natural vs man-made disasters, however with climate-related events we have a category where human activities though not directly causing these events certainly have had an indirect impact in terms of their prevalence by effecting climate change. We could also consider storms in the same category as climate-related events. In short, when creating these types of framework the categories might not be entirely exclusive and depending on the question at hand it might be helpful to set the criteria in slightly different ways."
  },
  {
    "objectID": "E_natural-disasters.html#man-made-disasters-are-primarily-caused-by-human-activities",
    "href": "E_natural-disasters.html#man-made-disasters-are-primarily-caused-by-human-activities",
    "title": "(Un)Natural Disasters",
    "section": "Man-made disasters are primarily caused by human activities",
    "text": "Man-made disasters are primarily caused by human activities\nMan-made disasters are primarily caused by human activities rather than natural processes including human error, negligence, technological failures along with intentional acts. Like all catastrophic events, they have a significant and wide-ranging impact on societies, environments, and economies.\nThey encompass a wide range of situations where human actions or decisions lead to significant harm, damage, and disruption to communities and environments. Parsing their causes emphasizes the importance of proper safety measures, responsible management of technology and resources, and underline the efforts that should be put toward prevention and mitigation of their negative impacts of human activities on both society and the environment.\n\n\n\n\n\n\n Consider this\n\n\n\nBrainstorm a list of man-made (unnatural) disasters and then create a framework of categories to classify them.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIndustrial accidents: chemical spills, industrial explosions, mine accidents\nNuclear incidents\nInfrastructure failures: building collapses, dam failures\nEnvironmental pollution/degradation: air/water pollution, deforestation/habitat degradation\nTerrorism/acts of violence: bombings, mass shootings\nTechnological disasters: cyber-security breaches, spacecraft failures"
  },
  {
    "objectID": "E_natural-disasters.html#natural-hazards-include-both-suddent-and-more-gradual-or-recurrent-events",
    "href": "E_natural-disasters.html#natural-hazards-include-both-suddent-and-more-gradual-or-recurrent-events",
    "title": "(Un)Natural Disasters",
    "section": "Natural hazards include both suddent and more gradual or recurrent events",
    "text": "Natural hazards include both suddent and more gradual or recurrent events\nWe can use the term natural hazard to encompass both sudden catastrophic events (natural disasters) and more gradual or recurrent events (extreme weather events) that pose risks to human communities and ecosystems. While extreme weather events can contribute to or be part of natural disasters (e.g. severe storms can cause flooding), not all extreme weather events necessarily lead to or are considered natural disasters. The classification often depends on the severity of the impacts and the response required to mitigate and manage the situation. It is a broader term that covers a spectrum of potentially harmful natural occurrences.\nAn extreme weather event refers to a specific instance of weather that deviates significantly from the average or expected conditions for a given region and time period. These events are characterized by their intensity, duration, or unusual nature that deviate significantly from the expected conditions. Extreme weather events can include heatwaves, severe storms (such as thunderstorms, tornadoes, or hailstorms), heavy rainfall leading to flooding, blizzards, and extreme cold snaps. While extreme weather events can be destructive and disruptive, they are typically considered as part of the broader spectrum of weather variability and are not always classified as natural disasters unless they result in significant harm. There is some irony in the fact that after spending decades emphasizing the difference between weather and climate, more recently we have seen an increasing number of scientists speak to the fact that the change in climate is indeed causing a change in the weather, specifically extreme weather events.\n\n\n\n\n\n\n Consider this\n\n\n\nYour assigned readings for this module include resources on the recently emerging field of extreme weather attribution. In this module we will explore a data set that lists all the observed disasters that are larger than a specific threshold. We have repeatedly discussed differences in descriptive, inferential/predictive, and mechanistic/causal analyses.\nCompare and contrast our exploratory analysis with extreme weather attribution in terms of the questions being asked and the type of analysis being done. Argue which you think is giving the best answer3.\n\n\n3 This will also require deciding what the “best answer” would beA natural disaster refers to a catastrophic event that occurs as a result of natural processes of the Earth. These events often cause significant damage to human life, property, and the environment. Natural disasters can encompass a wide range of events, including geological, meteorological, hydrological, and climatological events. Examples of natural disasters include earthquakes, hurricanes, tsunamis, volcanic eruptions, floods, wildfires, and landslides. These events can have widespread and severe impacts on communities and ecosystems.\nNatural disasters encompass a wide range of catastrophic events that result from natural processes on Earth. Natural disaster can originate from various sources and are not exclusively climate-driven. While some natural disasters are influenced by climate factors, others are primarily geological or hydrological in nature. While extreme weather events can contribute to or be part of natural disasters, not all extreme weather events necessarily lead to or are considered natural disasters. The classification often depends on the severity of the impacts and the response required to mitigate and manage the situation. The distinction between climate-driven and non-climate-driven events may not always be clear-cut, as certain events can be influenced by a combination of factors.\n\n\n\n\n\n\n Consider this\n\n\n\nBrainstorm a list of natural disasters and then create a framework of categories to classify them.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is a typical way we might group natural disasters based on their cause.\n\nGeological disasters: earthquakes, volcanic eruptions, tsunamis, landslides\nMeteorological disasters: (climate and weather-related): hurricans, tornados, severe storms, heatwaves, cold snaps, droughts\nHydrological disasters: floods, flash floods, mudslides\nBiological disasters: epidemics, pandemics\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThink through the range of natural disasters and consider which natural disasters you would expect to see exacerbated by climate change and whether you think that humans do contribute to some of these “natural” disasters and whether that impact is felt more in some of these categories compared to others."
  },
  {
    "objectID": "E_natural-disasters.html#words-matter",
    "href": "E_natural-disasters.html#words-matter",
    "title": "(Un)Natural Disasters",
    "section": "Words matter",
    "text": "Words matter\nThe term natural disaster implies they are the opposite of unnatural or man-made disasters and that humans have nothing to do with them and that they are just naturally occurring. However human actions have for example exacerbated climate-driven disasters. As a result, some people advocate for using the more general category catastrophic event as an alternative term that does not carry the implication of events being solely natural or unrelated to human influence and emphasizes the severity and impact of the event rather than its origin. As discussed “catastrophic events” encompasses a wide range of situations, including natural disasters, human-induced disasters, and events where natural processes and human actions interact to create significant harm and disruption. Using the term “catastrophic event” allows for a broader and more inclusive perspective that recognizes the complex interplay between natural processes and human activities in shaping disasters. It also avoids the misconception that humans are entirely disconnected from the causes and consequences of these events."
  },
  {
    "objectID": "16_exploratory-analysis-i.html#exploratory-analysis",
    "href": "16_exploratory-analysis-i.html#exploratory-analysis",
    "title": "16  Exploratory Analysis I",
    "section": "16.1 Exploratory Analysis",
    "text": "16.1 Exploratory Analysis\nAn exploratory analysis is systematic way of exploring data set primarily through visualizations. Generally, it is an iterative process that starts with a question or hypothesis and then, most of your exploratory analysis consists of visualizing, transforming, and even modeling your data to answer that question. Finally, you need to summarize your results and conclusions and then use those answers to refine your original question or generate new questions. Frequently, an exploratory analysis is the first step before deciding on what your formal analysis will look like and can serve to generate hypothesis for further exploration.\nPart of exploratory analysis is quality control of your data, determining if it meets your expectations and cleaning it up as needed. Frequently it includes using the existing raw data to transform your data into parameters that are more useful for your assessment."
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-1-formulate-a-question",
    "href": "16_exploratory-analysis-i.html#step-1-formulate-a-question",
    "title": "16  Exploratory Analysis I",
    "section": "16.2 Step 1: Formulate a question",
    "text": "16.2 Step 1: Formulate a question\nRecall, from our visualization of Climate Change indicators that we left the question of whether or not hurricanes are going “to get worse” unresolved even after comparing both changes in the number of names storms, major hurricanes, and their intensity.\nLet’s say we wanted to explore whether or not natural disasters have gotten worse as climate change has progressed. To do this we can turn to the International Disaster Database.\n\n\n\n\n\n\n Consider this\n\n\n\nGo to their website and use information you can find about the data collected to answer the following questions (use bullet points where appropriate):\n\nHow is (emergency) disaster defined?\nWhat are the major classifications of natural disasters included in the data set?\nWhich of these categories do you think are most likely impacted by climate change?\nWhat measures can you use to determine if natural disasters are “getting worse”? Should different categories have different measures?\nWhat other mechanisms do you think could result in an increase in natural disasters?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\n“Are natural disasters getting worse?” is a pretty general question. Based on your overview of what is in the data set, try to formulate at least three more specific questions that you could explore using this data set.\n\n\n\n\n\n\n\n\n Did it!"
  },
  {
    "objectID": "16_exploratory-analysis-i.html#read-wrangle-data-set",
    "href": "16_exploratory-analysis-i.html#read-wrangle-data-set",
    "title": "16  Exploratory Analysis I",
    "section": "16.3 Read & Wrangle data set",
    "text": "16.3 Read & Wrangle data set\nYou should have a copy of the tab-delimited file in your data folder. It contains an additional 6 lines at the top that have information on when and where it was downloaded.\n\ndisaster &lt;- read_delim(\"data/nat-disasters_emdat-query-2021-10-05.txt\", delim = \"\\t\", skip = 6)\n\nOur first step always is to take a look at the data set and make sure that it has read in correctly (i.e. that the columns are correctly separated, numeric columns are numeric, character columns are characters etc.) and is in a usable format.\n\n\n\n\n\n\n Consider this\n\n\n\nTake a quick look at the data set using View() to make sure everything has read in correctly. Quickly skim the dataframe to get an idea of how many columns/rows there are, how much missing data there is, that data types are correct etc.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nEven though data has read in correctly does not necessarily mean that it is in the most use-able format … remember plotting and filtering data sets requires typing variable names in over and over again. Therefore, we want to make sure that everything is set up in a way where we will be able to avoid mistakes that are mostly due to typos.\nFor example, let’s take a look at the column names:\n\ncolnames(disaster)\n\n [1] \"Dis No\"                          \"Year\"                           \n [3] \"Seq\"                             \"Glide\"                          \n [5] \"Disaster Group\"                  \"Disaster Subgroup\"              \n [7] \"Disaster Type\"                   \"Disaster Subtype\"               \n [9] \"Disaster Subsubtype\"             \"Event Name\"                     \n[11] \"Country\"                         \"ISO\"                            \n[13] \"Region\"                          \"Continent\"                      \n[15] \"Location\"                        \"Origin\"                         \n[17] \"Associated Dis\"                  \"Associated Dis2\"                \n[19] \"OFDA Response\"                   \"Appeal\"                         \n[21] \"Declaration\"                     \"Aid Contribution\"               \n[23] \"Dis Mag Value\"                   \"Dis Mag Scale\"                  \n[25] \"Latitude\"                        \"Longitude\"                      \n[27] \"Local Time\"                      \"River Basin\"                    \n[29] \"Start Year\"                      \"Start Month\"                    \n[31] \"Start Day\"                       \"End Year\"                       \n[33] \"End Month\"                       \"End Day\"                        \n[35] \"Total Deaths\"                    \"No Injured\"                     \n[37] \"No Affected\"                     \"No Homeless\"                    \n[39] \"Total Affected\"                  \"Reconstruction Costs ('000 US$)\"\n[41] \"Insured Damages ('000 US$)\"      \"Total Damages ('000 US$)\"       \n[43] \"CPI\"                             \"Adm Level\"                      \n[45] \"Admin1 Code\"                     \"Admin2 Code\"                    \n[47] \"Geo Locations\"                  \n\n\nApparently, they have not read our guidelines for naming things. Column headers with special characters and spaces are super annoying, you might recall from our previous adventures in data wrangling that every time we want to call a column with spaces we are going to have to use back ticks to tell R that it isn’t separate words but a single name.\nNow is probably a good time to learn how to rename columns. The most straightforward way is using rename().\nLet’s say we wanted to rename Dis No to DisNo the syntax is simple NewName = OldName.\n\ndisaster %&gt;%\n  rename(DisNo = `Dis No`) %&gt;%\n  head()\n\n# A tibble: 6 × 47\n  DisNo    Year Seq   Glide `Disaster Group` `Disaster Subgroup` `Disaster Type`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;          \n1 1900-9…  1900 9002  &lt;NA&gt;  Natural          Climatological      Drought        \n2 1900-9…  1900 9001  &lt;NA&gt;  Natural          Climatological      Drought        \n3 1902-0…  1902 0012  &lt;NA&gt;  Natural          Geophysical         Earthquake     \n4 1902-0…  1902 0003  &lt;NA&gt;  Natural          Geophysical         Volcanic activ…\n5 1902-0…  1902 0010  &lt;NA&gt;  Natural          Geophysical         Volcanic activ…\n6 1903-0…  1903 0006  &lt;NA&gt;  Natural          Geophysical         Mass movement …\n# ℹ 40 more variables: `Disaster Subtype` &lt;chr&gt;, `Disaster Subsubtype` &lt;chr&gt;,\n#   `Event Name` &lt;chr&gt;, Country &lt;chr&gt;, ISO &lt;chr&gt;, Region &lt;chr&gt;,\n#   Continent &lt;chr&gt;, Location &lt;chr&gt;, Origin &lt;chr&gt;, `Associated Dis` &lt;chr&gt;,\n#   `Associated Dis2` &lt;chr&gt;, `OFDA Response` &lt;chr&gt;, Appeal &lt;chr&gt;,\n#   Declaration &lt;chr&gt;, `Aid Contribution` &lt;dbl&gt;, `Dis Mag Value` &lt;dbl&gt;,\n#   `Dis Mag Scale` &lt;chr&gt;, Latitude &lt;chr&gt;, Longitude &lt;chr&gt;, `Local Time` &lt;chr&gt;,\n#   `River Basin` &lt;chr&gt;, `Start Year` &lt;dbl&gt;, `Start Month` &lt;dbl&gt;, …\n\n\nThis would be pretty annoying if we wanted to rename all of our columns by hand.\nFortunately, we can leverage the function janitor::clean_names() function2 which is designed to parse letter cases and separators - the default is to convert it to snake_case, i.e. all names are lowercase and words are separated by an underscore, it also deals with duplicate names and special characters.2 The notation format you see here specifies the Rpackage and then the function within it, as package::function().\nLet’s see what this does for us:\n\ndisaster &lt;- read_delim(\"data/nat-disasters_emdat-query-2021-10-05.txt\", delim = \"\\t\", skip = 6) %&gt;%\n  clean_names()\n\ncolnames(disaster)\n\n [1] \"dis_no\"                      \"year\"                       \n [3] \"seq\"                         \"glide\"                      \n [5] \"disaster_group\"              \"disaster_subgroup\"          \n [7] \"disaster_type\"               \"disaster_subtype\"           \n [9] \"disaster_subsubtype\"         \"event_name\"                 \n[11] \"country\"                     \"iso\"                        \n[13] \"region\"                      \"continent\"                  \n[15] \"location\"                    \"origin\"                     \n[17] \"associated_dis\"              \"associated_dis2\"            \n[19] \"ofda_response\"               \"appeal\"                     \n[21] \"declaration\"                 \"aid_contribution\"           \n[23] \"dis_mag_value\"               \"dis_mag_scale\"              \n[25] \"latitude\"                    \"longitude\"                  \n[27] \"local_time\"                  \"river_basin\"                \n[29] \"start_year\"                  \"start_month\"                \n[31] \"start_day\"                   \"end_year\"                   \n[33] \"end_month\"                   \"end_day\"                    \n[35] \"total_deaths\"                \"no_injured\"                 \n[37] \"no_affected\"                 \"no_homeless\"                \n[39] \"total_affected\"              \"reconstruction_costs_000_us\"\n[41] \"insured_damages_000_us\"      \"total_damages_000_us\"       \n[43] \"cpi\"                         \"adm_level\"                  \n[45] \"admin1_code\"                 \"admin2_code\"                \n[47] \"geo_locations\"              \n\n\nThat looks much better."
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-2-get-an-overview-of-the-data-set",
    "href": "16_exploratory-analysis-i.html#step-2-get-an-overview-of-the-data-set",
    "title": "16  Exploratory Analysis I",
    "section": "16.4 Step 2: Get an overview of the data set",
    "text": "16.4 Step 2: Get an overview of the data set\nOur next step always is to determine what information is contained in the data set and how it is organized.\nFirst, we will need to understand what information is contained in each column. Some of the column headers are more self-explanatory than others. One of the most helpful documents at this point is the metadata which will tell us what information is in each column. A good place to look for that is the data portal where you accessed the data set itself.\n\n\n\n\n\n\n Consider this\n\n\n\nDetermine where you can find meta-data for our data set and describe what information is contained in the data set based on the included columns. Indicate which you think will be most useful to answer our general question based on your answers above.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow, we can take a look at some of the specifics of how the information is organized in the data set and specifics on the information contained.\n\n\n\n\n\n\n Consider this\n\n\n\nBased on some of our previous exploits exploring data, come up with a checklist of at least five things to routinely check any time you are exploring a new data set, wherever possible include what function(s) you could use to look at that.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nFactors to consider are among other things understanding how many rows/columns there are, what variables are included, whether it is in a tidy format or not, what data type each variable is, and what the range/distribution of values is.\nYou can get dimensions of the data set using ncols() and nrows(), str() will give you dimensions, the class, and data types (classes) of individual columns.\n\n\n\nNext to all of the functons you just listed, another helpful functions is skimr::skim()\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the function on your data set and briefly describe what information you can get3.\n\nskim(disaster)\n\n\n\n\n\nName\ndisaster\n\n\nNumber of rows\n16121\n\n\nNumber of columns\n47\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n29\n\n\nlogical\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndis_no\n0\n1.00\n13\n13\n0\n16121\n0\n\n\nseq\n0\n1.00\n4\n4\n0\n1274\n0\n\n\nglide\n14540\n0.10\n4\n25\n0\n1086\n0\n\n\ndisaster_group\n0\n1.00\n7\n7\n0\n1\n0\n\n\ndisaster_subgroup\n0\n1.00\n10\n17\n0\n6\n0\n\n\ndisaster_type\n0\n1.00\n3\n21\n0\n15\n0\n\n\ndisaster_subtype\n3109\n0.81\n6\n32\n0\n27\n0\n\n\ndisaster_subsubtype\n15044\n0.07\n4\n23\n0\n12\n0\n\n\nevent_name\n12263\n0.24\n2\n76\n0\n1561\n0\n\n\ncountry\n0\n1.00\n4\n58\n0\n228\n0\n\n\niso\n0\n1.00\n3\n3\n0\n228\n0\n\n\nregion\n0\n1.00\n9\n25\n0\n23\n0\n\n\ncontinent\n0\n1.00\n4\n8\n0\n5\n0\n\n\nlocation\n1792\n0.89\n3\n2878\n0\n12755\n0\n\n\norigin\n12329\n0.24\n4\n118\n0\n660\n0\n\n\nassociated_dis\n12774\n0.21\n3\n29\n0\n30\n0\n\n\nassociated_dis2\n15415\n0.04\n3\n29\n0\n30\n0\n\n\nofda_response\n14427\n0.11\n3\n3\n0\n1\n0\n\n\nappeal\n13552\n0.16\n2\n3\n0\n2\n0\n\n\ndeclaration\n12866\n0.20\n2\n3\n0\n2\n0\n\n\ndis_mag_scale\n1190\n0.93\n3\n10\n0\n5\n0\n\n\nlatitude\n13393\n0.17\n1\n10\n0\n2372\n0\n\n\nlongitude\n13390\n0.17\n1\n12\n0\n2438\n0\n\n\nlocal_time\n15019\n0.07\n4\n8\n0\n778\n0\n\n\nriver_basin\n14835\n0.08\n1\n402\n0\n1213\n0\n\n\nadm_level\n8262\n0.49\n1\n3\n0\n3\n0\n\n\nadmin1_code\n11540\n0.28\n3\n399\n0\n3434\n0\n\n\nadmin2_code\n12152\n0.25\n3\n1571\n0\n3446\n0\n\n\ngeo_locations\n8262\n0.49\n10\n2525\n0\n6355\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nreconstruction_costs_000_us\n16121\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1996.76\n20.16\n1900.00\n1989.00\n2001.00\n2011.00\n2021\n▁▁▁▃▇\n\n\naid_contribution\n15444\n0.04\n125413.61\n2997874.55\n1.00\n175.00\n721.00\n3511.00\n78000000\n▇▁▁▁▁\n\n\ndis_mag_value\n11177\n0.31\n47369.51\n309485.37\n-57.00\n7.00\n152.00\n11315.00\n13025874\n▇▁▁▁▁\n\n\nstart_year\n0\n1.00\n1996.77\n20.15\n1900.00\n1989.00\n2001.00\n2011.00\n2021\n▁▁▁▃▇\n\n\nstart_month\n387\n0.98\n6.44\n3.39\n1.00\n4.00\n7.00\n9.00\n12\n▇▅▆▆▇\n\n\nstart_day\n3628\n0.77\n15.24\n8.95\n1.00\n7.00\n15.00\n23.00\n31\n▇▆▆▆▅\n\n\nend_year\n0\n1.00\n1996.83\n20.14\n1900.00\n1989.00\n2001.00\n2011.00\n2021\n▁▁▁▃▇\n\n\nend_month\n708\n0.96\n6.58\n3.35\n1.00\n4.00\n7.00\n9.00\n12\n▇▅▆▇▇\n\n\nend_day\n3556\n0.78\n15.78\n8.87\n1.00\n8.00\n16.00\n24.00\n31\n▇▇▇▇▆\n\n\ntotal_deaths\n4713\n0.71\n2844.11\n68620.96\n1.00\n6.00\n20.00\n63.00\n3700000\n▇▁▁▁▁\n\n\nno_injured\n12227\n0.24\n2621.70\n34407.82\n1.00\n14.00\n50.00\n200.00\n1800000\n▇▁▁▁▁\n\n\nno_affected\n6903\n0.57\n882551.63\n8574833.40\n1.00\n1245.50\n10000.00\n91941.00\n330000000\n▇▁▁▁▁\n\n\nno_homeless\n13692\n0.15\n73323.11\n523111.36\n3.00\n574.00\n3000.00\n17500.00\n15850000\n▇▁▁▁▁\n\n\ntotal_affected\n4507\n0.72\n716692.93\n7719586.75\n1.00\n650.00\n5967.00\n58255.00\n330000000\n▇▁▁▁▁\n\n\ninsured_damages_000_us\n15025\n0.07\n798651.42\n3057638.17\n34.00\n50000.00\n172500.00\n500000.00\n60000000\n▇▁▁▁▁\n\n\ntotal_damages_000_us\n10876\n0.33\n724783.54\n4723130.53\n2.00\n8300.00\n60000.00\n317300.00\n210000000\n▇▁▁▁▁\n\n\ncpi\n310\n0.98\n63.22\n26.73\n3.22\n45.69\n68.42\n84.25\n100\n▃▂▅▇▇\n\n\n\n\n\n\n\n3 You may want to adjust the width of your console pane toget a better look at the output.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nFor numerical columns mean values and percentiles can give you an overview of how the data is distributed. However, for columns that contain strings or characters we cannot produce those types of summary statistics. Rather, we are probably more interested on how many unique values those columns contain and what those unique entries are.\nskim() already gave us the number of unique entries for each column, we have two options we can use to determine what those values are.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nWe’ve previously learned about two functions that return the unique entries for a vector and for a dataframe. Apply both of them below to output the unique entries of the the disaster subgroups directly to the console/your html report. Then pick two more columns where you think it would be important to know how many and which values are represented4 and run a function to get that information.\n\n\n4 These should probably be categorical values.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe first is unique(). This function makes use of the fact that each column is a vector5.\n\nunique(disaster$disaster_subgroup)\n\n[1] \"Climatological\"    \"Geophysical\"       \"Meteorological\"   \n[4] \"Hydrological\"      \"Biological\"        \"Extra-terrestrial\"\n\n\nWhen you run this function it returns a vector; while this might be helpful in some contexts, it would be helpful if we could also view it in a tabular format. Do not fret - tidyverse got you.\n\ndisaster %&gt;%\n  distinct(disaster_subgroup)\n\n# A tibble: 6 × 1\n  disaster_subgroup\n  &lt;chr&gt;            \n1 Climatological   \n2 Geophysical      \n3 Meteorological   \n4 Hydrological     \n5 Biological       \n6 Extra-terrestrial\n\n\n\n\n\n5 Remember, we can access each column of a dataframe as `df$columnname"
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-3-chose-a-question-to-explore",
    "href": "16_exploratory-analysis-i.html#step-3-chose-a-question-to-explore",
    "title": "16  Exploratory Analysis I",
    "section": "16.5 Step 3: Chose a question to explore",
    "text": "16.5 Step 3: Chose a question to explore\nFrequently you will have a general question which is probably why you pulled a data set in the first place. In our example that would be something along the lines of “Are natural disasters getting worse with climate change?”.\nUsually, there will be more than one way to go after that question and you will likely want and need to do some extended exploration of the data set to generate additional questions (hypothesis) and ultimately find the one you are mot interested in.\nIn this case, we could focus on whether we are indeed observing an increase in frequency of events and/or we could compare if different groups of natural disasters that are more dependent on climate/weather conditions (storms, hurricanes) are increasing more rapidly compared to categories that have other origins (e.g. earthquakes or volcanic eruptions).\nOr, we could have already done some analysis and/or background reading that indicates that natural disasters linked to climate patterns are indeed predicted to get worse based on modeling. In that case we might be more interested whether the impact of natural disasters on humans is getting increasingly worse. We could be at a point where at least some of the changes to the climate system are inevitable and in that case the question of mitigation of effects and preparedness like early warning systems or infrastructure become more important. We cannot stop the events from happening but to an extent we can control how well we can deal with them.\nExploratory Analysis is fundamentally a very creative process and thinking outside the box can be what sets your analysis apart from what everyone else is doing. However, before we can start thinking outside the box we need to first figure out what “the box is” and use that as our starting point. Unfortunately, this does mean you should always start with the boring standard stuff, cover your bases and then from there work towards something more interesting. However, not infrequently the “boring” stuff is the most revealing!"
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-4-plot-the-data",
    "href": "16_exploratory-analysis-i.html#step-4-plot-the-data",
    "title": "16  Exploratory Analysis I",
    "section": "16.6 Step 4: Plot the data!",
    "text": "16.6 Step 4: Plot the data!\nWhile tables are sometimes more appropriate, most exploratory analysis involves generating plots that help you get an overview of the data in relation to your question.\n\n\n\n\n\n\n Consider this\n\n\n\nWe should make a distinction between exploratory figures and explanatory or final figures. Briefly contrast the two in terms of their goal and audience and how this might be reflected in the presentation of the visualization.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n16.6.1 Explore the distribution of the data\nA good place to start is to get an idea of the variation or variance of the data you are interested in.\n\n\n\n\n\n\n Consider this\n\n\n\nList 3 - 5 ways that you can assess the variance of a measured value. What metrics can you calculate and what options do we have to visualize?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nYou can calculate summary statistics - we have already encountered the base R functions that will allow you to do this.\n\nmean (mean()), median (median())\nmininimum (min()), maximum (max()) values to get the range of values\nquantiles (quantile())\n\n(fivenum() will give you min, 25th percentile, median, 75th percentile, max)\nTo visualize, you can use\n\nbarplots for categorical values (geom_bar(stat = \"identity\"))\nhistograms for continuous values (geom_histogram(); here your individual bins become components of a categorical value)\nboxplots to compare distributions of continuous values (geom_boxplot())\n\n\n\n\nFor any of these assessments your should always ask the following questions\n\nWhich values are more common than others? Why?\nWhich values are more rare than others? Why?\nAre there distinct clusters that emerge? Why does/doesn’t that make sense?\nDo overall patterns meet my expectations? What could cause deviations? Why do I have these expectations of the data?\nAre there patterns that stand out? Are they unusual/unexpected? What could be causing them?\n\nIn each case, as you pick up on patterns you want to also ask “why” and “how come” questions6 … those are the ones that will lead you to the next steps of your exploration.6 Imagine a cute toddler right next to you asking “why” at every step.\nTypically, you should always explore outlier values to determine whether they are true outliers or if it is legitimate to remove them, along with patterns of missing data. Keep in mind that if there is too much missing data that could be an indication that you should be looking or a better data set.\nLet’s get started. You will hopefully quickly see that for any type of exploratory analysis you start with one plot/question to visualize a pattern and then typically go through a few steps of plotting that same data set component in a few different ways to get a view from a few different angles before moving on to the next thing. Be patient and try various things even if some things you pursue end up being dead ends.\nLet’s start by figuring out how many observations fall into each sub-category of natural disasters7.7 We finally get to see the alternative to stat = \"identity\" in using barplots! To plot distribution of categorical variable you are now telling ggplot to count the number of observations for each category\n\nggplot(disaster, aes(x = disaster_subgroup)) +\n  geom_bar(stat = \"count\")\n\n\n\n\nand we could do the same for our disaster types.\n\nggplot(disaster, aes(x = disaster_type)) +\n  geom_bar(stat = \"count\")\n\n\n\n\nWe probably want to be able to read those labels so let’s flip them by 90 degrees.\n\nggplot(disaster, aes(x = disaster_type)) +\n  geom_bar(stat = \"count\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nThis is giving us global numbers, we might want to break it up by region.\n\nggplot(disaster, aes(x = disaster_type)) +\n  geom_bar(stat = \"count\") +\n  facet_wrap(. ~ region) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nHm, that’s interesting.\nThe next step is that our questions is connected to understanding the “getting worse” component so we have to consider ways we can measure “worseness”. One way to do that is impact. In this case, we ould want to filter our data set to include only events for which damages where assessed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a new object called damages that includes only entries for observations of total damages in the US were made (total_damages_000_us) and then get an overview of distribution of the damage incurred by plotting a histogram..\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nRemember that you can subset rows using !is.na() as a condition to retain only observations that have a values (i.e. that aren’t na).\n\n\n\nThis is what your figure should look like\n\n#|echo: false\n\nggplot(damages, aes(x = total_damages_000_us)) +\n  geom_histogram()\n\n\n\n\n\n\n\nUgh, that plot does not initially look too helpful, it seems like almost all of our values vall into the same bin. This could be because we have some outliers that are skewing our histogram.\nWe can get a better idea by using e.g. a square root scale, which skews the y-axis in a systematic way8.8 This is the type of thing where in order to not be unintentionally mislead you would always want to add “Note the transformed y-scale”\n\nggplot(damages, aes(x = total_damages_000_us)) +\n  geom_histogram() +\n  scale_y_sqrt()\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nAnother option would be to set the binwidths manually.\nCreate a new column that contains your dollar amounts divided by 1,000,000 to make the x-axis more legible. Then create a histogram showing the damages binned by $5 Million. Name your x-axis to reflect that this is damages in millions of dollars.\nThen create a second plot with individual panels showing the damages broken down by disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your first figure should look like\n\n\n\n\n\nAnd this figure shows the variation in damages for each disaster type.\n\n\n\n\n\n\n\n\nBased on our second figure, it looks like we might want to limit our assessment to droughts, earthquakes, wildfires, floods, and storms, using boxplots insteed of historgrams is also going to make these comparisons easier to make.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset your damages dataframe to contain only observations of droughts, earthquakes, wildfires, floods, and storms; no need to creat new dataframe, just overwrite the one you currently have.\nThen create a boxplot comparing the distribution of damages in millions of dollars per disaster type. Make sure to update the x- and y-axis labels to be descriptive.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what your figure should look like\n\n\n\n\n\n\n\n\nFrom your figure, you’ve probably seen that the big differences seems to be what the extreme values are in eah group but it is difficult to tell how different the mean, median and general spread of the data are.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nMitigate this by creating a table (make sure it prints neatly to your console/html report) that contains the mean, median, standard deviation, minimum and maximum value of damages in Millions of USD for each disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what a table with summary stats should look like.\n\n\n\n\n\ndisaster_type\nmedian\nmean\nsd\nmin\nmax\n\n\n\n\nDrought\n0.1100\n0.8805004\n2.045504\n0.000050\n20.0\n\n\nEarthquake\n0.0388\n1.4403066\n10.581451\n0.000002\n210.0\n\n\nFlood\n0.0495\n0.4972023\n1.992408\n0.000003\n40.0\n\n\nStorm\n0.0760\n0.7409707\n4.311794\n0.000005\n125.0\n\n\nWildfire\n0.1020\n0.7286744\n2.037236\n0.000600\n16.5"
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-5-explore-relationships-among-variables",
    "href": "16_exploratory-analysis-i.html#step-5-explore-relationships-among-variables",
    "title": "16  Exploratory Analysis I",
    "section": "16.7 Step 5: Explore relationships among variables",
    "text": "16.7 Step 5: Explore relationships among variables\nAfter you’ve explored patterns within variables you are interested in you are also going to want to explore relationships (Covariation) among variables9.9 Remember, when we start looking at correlation, we always need to be careful about how/when we infer causation!\n\n\n\n\n\n\n Consider this\n\n\n\nList 3 - 5 ways that you can assess the covariance of a measured value.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nYou can calculate summary statistics and compare them, but for initial exploration of patterns visualizations are going to be much more handy.\nYou could create faceted barplots or have bars next to each other in a figure; faceted histograms can be helpful too. But especially with increasing number of comparisons, one of the most straightforward ways to compare distributions are boxplots.\nComparing two categorical values can be challenging - heat plots can be used to visualize the number of observations that fulfill both categories, for two continuous values you can use scatter plots (though these become less useful with increasing number of observations in your data set).\n\n\n\nAre damages incurred by natural disasters increasing over time?\nWe have already spent a little bit of time looking at more than one variable by looking at subgroups within our data set, specifically the disaster types and regional differences.\nHowever, given the initial question we are asking of the data set, the relationship we are really interested in is the change over time. This means we are technically comparing two continuous values but in this specific case we frequently describe as a time-series.\nLet’s take a look at the temporal patterns we can observe.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFirst create a plot that shows the change total damages incurred over time.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your plot should look like:\n\n\n\n\n\n\n\n\nThat’s helpful, but from this figure it is quite difficult to tell whether this is a consistent pattern, or if it is one that is driven for example by a specific type of disaster.\nWe could enhance this figure and our understanding of the data set by e.g. color coding by disaster type.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRe-plot the time-series of damages incurred but now color code the individual data points by disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure should look like.\n\nggplot(damages, aes(x = year, y = total_damages_000_us/1000000, fill = disaster_type)) +\n  geom_point(shape = 21, size = 2)\n\n\n\n\n\n\n\nWhile that gives us a bit of a better idea of the pattern in the data, it is still difficult to tell if there are differences between the disaster types - this also does not let us parse whether this is a consistent pattern across the globe or if it is for example, driven by specific geographic regions.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a closer look at the data by plotting the damage incurred by natural disasters. Color code individual data points by continent and create individual panels for each disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your plot could look like.\n\n\n\n\n\n\n\n\nIs the magnitude of the disaster correlated to the damages it incurs?\nAnother important relationship to explore is whether the damages incurred by a given natural disaster scales with its magnitude. Both frequency and intensity of climate-related natural disasters are expected to increase so this is an important relationship to understand.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThe magnitude of each of these types of natural disasters is measured in a very different way so to plot the relationship of the magnitude (dis_mag_value) and total damages incurred you will need to subset the data set to hold only observations for one disaster type to plot it.\nCreate a plot showing this relationship for storms, floods, earthquakes, droughts, and wildfires in individual plots using patchowrk10.\nRemember that you can use #| fig-height: and #| fig-width: to make sure that the figure looks good in your html report. This does require rendering your html, checking it and then adjusting the height and width of the figures in the output as needed.\n\n\n10 Check out the layout guidelines for patchwork if you need a refresher on how to combine plots.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your plot could look like. I additionally color coded the individual data points by year to see if there were any patterns that popped out.\n\n\n\n\n\n\n\n\nSo far we haven’t really found any patterns that really pop out - though keep in mind that “non-patterns” are also noteworthy in an of themselves11.11 Recall that we’ve discussed the importance of creating hypothesis that are falsifiable. Even if we thing there should be certain relationships, during an exploratory analysis we always need to be open to the idea that certain relationships are not in the data and that we cannot try to manipulate the data to make it seem as if there were.\ncan we create metrics that help us better understand important relationships?\nWe do know from our previous plots that we tend to have a pretty tight distribution with damages being pretty similar across events. However, you have probably also noticed that we always see that there are individual events that are outliers in terms of how much damage they incur.\nSince are looking at very broad-scale patterns of it might make sense to transform our data in order to get a new metric based on the data at hand that enables a more helpful and informative comparison.\nIn this case, it could be helpful to calculate the total costs incurred per year for each disaster type and then compare how those total values are changing over time.\nLet’s start by creating a new data frame that contains information on total damages incurred per year.\n\ntotal_yr &lt;- disaster %&gt;%\n  filter(!is.na(total_damages_000_us)) %&gt;%\n  group_by(year, disaster_type) %&gt;%\n  summarize(total_damages_yr = sum(total_damages_000_us)/1000000000)\n\nLet’s plot that data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse our new data frame to create a plot that shows the total damages incurred per year for each disaster type ina an individual panel..\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure should look like.\n\n\n\n\n\n\n\n\nNow we’re getting somewhere.\nAs we are looking at these plots, it is important to keep in mind that increase in total damages per year can be due to a combination of more individual events but all have small number of damages incurred or if we have more incidence of high cost events and of course, it could be a combination thereof.\nLet’s create a data set with a limited disaster types so we can elucidate patterns more easily, and while we’re at it also calculate frequency of each disaster type per year.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset the total_yr data set so that it only contains observations for droughts, earthquakes, extreme temperatures, floods, storms, and wildfires. Then create a data frame that contains total damages per year and disaster type as well as the tota number of events that occured that year.\nThen create a plot that contains the relationship of total damages incurred for each year with an indiviual panel for each disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your data set should look like:\n\n\n# A tibble: 6 × 4\n# Groups:   year [5]\n   year disaster_type total_damages_yr events_yr\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;     &lt;int&gt;\n1  1900 Storm                0.00003           1\n2  1902 Earthquake           0.000025          1\n3  1903 Flood                0.00048           1\n4  1905 Earthquake           0.0000488         2\n5  1906 Earthquake           0.000624          2\n6  1906 Storm                0.00002           1\n\n\nAnd then based off of that you can create the following plot.\n\n\n\n\n\n\n\n\nNow that we’ve discovered a better metric to be looking at, let’s quickly check what the relationship is of frequency and total damages.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your new data frame to plot the relationship between the number of events per year and the total damages incurred per year. Add a regression to help you spot relationshps.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what this figure should look like.\n\nggplot(total_yr, aes(x = events_yr, y = total_damages_yr)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\nBased off of those last two figure we’ve made, the number of events definitely appears to play a role in the total damages due to various disaster types per year, though we would need a more formal analysis to disentangle those effects.\nPick the visualizations that best communicate the pattern you want to highlight\nOur multi-panel time-series data seems to have the data transformed in a way that will give us an answer to our question - but it’s not really a great visualization if we wanted to share results from our exploratory analysis. Generally, you’ll hit a point where you know have the data transformed in a way that is useful and you could start playing around with optimizing the presentation of that data set.\nAt this point we are shifting from our exploratory figures to an explanatory figure that we might include in a report or an update to our collaborators.\nOne of the problems with our multiplot visualization is that the scales for all of the y-axis are the same because we have some outlier that compress the scale. Maybe a traditional scatterplot is not ideal. Because we are interested in changes in magnitude we could try a less traditional plot, like a bubble chart. Instead of plotting the total damages incurred on the y-axis, we can plot our disaster types and then adjust the size of each bubble according to damages incurred.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a bubble chart using the following code. Add comments to describe what each function/parameter is doing. Then describe the figure. Be specific in terms of how the data is encoded and what the general pattern and notable results are.\n\nggplot(total_yr, aes(x = year,                            #\n                     y = disaster_type,                   #\n                     size = total_damages_yr/1000000)) +  #\n  geom_point()                                            #\n\n\n\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow we’re cooking!\nLet’s think of some ways that we can refine this plot. We have overlapping points, we also might want additional bin sizes.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the following code and then comment it line by line to describe what each function/argument is doing.\n\nggplot(total_yr, aes(x = year,\n                     y = disaster_type,\n                     size = total_damages_yr/1000000,\n                     fill = disaster_type)) +\n  geom_point(shape = 21, alpha = .5) +\n  scale_size_continuous(breaks = c(10, 25, 50, 100, 200),\n                        name = \"damages [USD]\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nLet’s see if we can fix our legend to something more visible, we might want different colors, and we’ll need to add axis labels, plot titles etc.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the following code and then comment line by line to describe what each function/argument is doing.\n\nggplot(total_yr, aes(x = year,\n                     y = disaster_type,\n                     size = total_damages_yr/1000000,\n                     fill = disaster_type)) +\n  geom_point(shape = 21,\n             alpha = .5) +\n  scale_fill_viridis_d() +\n  scale_size_continuous(breaks = c(5, 10, 25, 50, 100, 200),\n                        name = \"damages [USD]\") +\n  labs(x = \"total damages incurred [USD]\", y = \"disaster type\",\n       title = \"Economic costs incurred by natural disasters (1900 - 2020)\",\n       subtitle = \"The size of the bubble represents the total damages incurred by all events in a given category.\",\n       caption = \"data: EMDAT/International Disaster Database\") +\n  theme_standard +\n  theme(legend.position = \"bottom\") +\n  guides(fill = FALSE,\n         size = guide_legend(override.aes = list(fill = \"black\", alpha = 1)))\n\n\n\n\n\n\ncan we infer/predict relationships between our sample and the whole population?\nA final step in our exploratory analysis is starting to ask whether patterns are specific to the data set (descriptive analysis) or whether there are true non-random relationships between variables (inferential analysis). The overview of the patterns in the data that we get through the exploratory analysis are really important to help us determine what questions/hypothesis are worth exploring more in depth.\nTypical questions we might further explore at this point include e.g.\n\nIs this pattern due to random change or does the independent variable have significant explanatory power?\nHow can the relationship implied by the pattern be described (e.g. linear regression)?\nHow strong is the relationship (R2 value)?\nWhat other variables might affect or explain the relationship (e.g. are these variable correlated because they are both correlated to another variable that is responsible for the mechanism?)\nDoes this relationship hold across subgroups of the relationship itself?\n\nFitting trend lines helps elucidate patterns (especially in time series). This is generally the point where exploratory analysis transitions into a more formal analysis that involves statistical tests and fitting models. Keep in mind that we want to have a good grasp on whether our analysis is descriptive, inferential/predictive or causal/mechanistic in nature. Exploratory analysis is “quick and dirty”, you are mainly focused on summarizing the data and highlighting broad-scale patterns that emerge. It is useful for assessing the quality of our data, eye-balling if patterns are what you expect (was your hypothesis correct(ish)), and getting and idea of what the next steps in a more formal analysis should be."
  },
  {
    "objectID": "16_exploratory-analysis-i.html#step-6-iterate-your-exploration",
    "href": "16_exploratory-analysis-i.html#step-6-iterate-your-exploration",
    "title": "16  Exploratory Analysis I",
    "section": "16.8 Step 6: Iterate your exploration",
    "text": "16.8 Step 6: Iterate your exploration\nNow that we have an answer(ish) to our question - the first thing we should do is question our answer12. Always consider limitations of your data, whether there are alternative explanations, if your data has some inherent issues you need to be aware of etc. It can also be helpful to find at least one external data source to assess whether your answers are roughly in line with other (expected) measurements.12 especially if your answer conforms to your a priori expectations!\nIt is called an exploratory analysis for a reason, because it is just the beginning. Usually this is the beginning of a more sophisticated analysis. Important questions to ask always include\n\nIs your data appropriate for the question your are asking?\nDo you need additional data to refine your answer?\nDo you have the right question or do you need to refine your question?\n\nExploratory Analysis frequently is more about hypothesis generating, rather than hypothesis testing. So you data exploration can be an important step to defining the precise (set of) question(s) you want to explore. Not infrequently, the exploration of an initial question leads you to generating additional (perhaps more interesting) questions.\nSo for example, one of the first questions we might ask is whether the pattern we see with higher damages being incurred holds if we compare this to loss of life.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTransform your data set similar to the way we did it above to create a data set that contains the number of deaths observed for each disaster type in each year. Then use that new data frame to create bubble chart showing the change in the number of deaths per year for each disaster type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what the first few lines of your data set will look like:\n\ndeaths &lt;- disaster %&gt;%\n  filter(!is.na(total_deaths)) %&gt;%\n  group_by(year, disaster_type) %&gt;%\n  summarize(total_deaths_yr = sum(total_deaths))\n\nhead(deaths)\n\nAnd here is what the plot should look like.\n\nggplot(deaths, aes(x = year, y = disaster_type, size = total_deaths_yr/100000, fill = disaster_type)) +\n  geom_point(shape = 21, alpha = .5) +\n  scale_fill_viridis_d() +\n  scale_size_continuous(breaks = c(.5, 1, 5, 10, 15, 30),\n                        name = \"100k deaths\") +\n  labs(x = \"total deaths incurred\", y = \"disaster type\",\n       title = \"Total deaths due to natural disasters (1900 - 2020)\",\n       subtitle = \"The size of the bubble represents the total loss of life incurred by all events per year.\",\n       caption = \"data: EMDAT/International Disaster Database\") +\n  theme_standard +\n  theme(legend.position = \"bottom\") +\n  guides(fill = FALSE,\n         size = guide_legend(override.aes = list(fill = \"black\", alpha = 1)))\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast the total damages incurred per year with the total number of deaths due to each disaster type13.\nThen use what you have learned from the assigned readings and other media to discuss what could be causing the differences/similarities in these two figures.\nConclude by making a statement about what we have learned in terms of whether natural disasters are indeed getting worse.\n\n\n13 A good way to do this is to briefly describe each figure to summarize the broad pattern(s), then point out what they have in common and what sets them apart\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nAdditionally we could break down regional patterns, or even pull in additional data of GDP of individual countries to compare costs incurred scaled to the economies of different countries.\nIt’s always helpful to take notes as you go to jot down questions and also ideas on what might explain certain patterns you see."
  },
  {
    "objectID": "17_exploratory-analysis-ii.html#now-you",
    "href": "17_exploratory-analysis-ii.html#now-you",
    "title": "17  Exploratory Analysis II",
    "section": "17.1 Now you!",
    "text": "17.1 Now you!\nFormulate two specific questions this data set can answer, consider that you have options like looking at specific geographic subsets, or different categories of natural disasters, you can also consider whether your would expect different types of natural disasters to show similar or different patterns.\nUse headers, plain text, code chunks etc. as appropriate to write a mini report for each question - this should include stating your question as concisely as possible, exploratory figures, a final explanatory figure, description & discussion of your results. Comment your code to generate your figures line by line with descriptive comments.\nFor each question, generate 3 - 4 exploratory figures that can help answer your question, these can include different visualizations of the same variables or different (combinations of) variables. For each question, chose the figure that summarizes your results best and refine your visualization so that it best summarizes your results, this includes a figure title, labeled axis, color choice, how data is encoded etc.\n\n\n\n\n\n\nProtip\n\n\n\nIt’s possible that you have some additional ideas on how you would like to refine your figure but you’re not sure how you can achieve that using your current ggplot skill set. If that is the case, do not get hung up on cosmetics, rather make a note and we can refine figures based on all of our feedback in class.\n\n\nWrite a figure legend that would accompany your figure in a report or paper, write a 3 - 5 sentence description of each figure (i.e. this should communicate the results with sufficient detail that somebody who has not seen the figure still fully understand them) and briefly discuss your results in 3-5 sentences in the context of the question you were asking (i.e. what are your conclusions, are there limitation to your data, alternative explanations, …).\nFor each initial question, jot down 2-3 ideas of where you would take your exploratory analysis in the next step.\n\n17.1.1 Question 1:\n\n\n17.1.2 Questions 2:"
  },
  {
    "objectID": "17_exploratory-analysis-ii.html#get-feedback",
    "href": "17_exploratory-analysis-ii.html#get-feedback",
    "title": "17  Exploratory Analysis II",
    "section": "17.2 Get feedback",
    "text": "17.2 Get feedback\nOne important aspect of an exploratory analysis that we haven’t addressed yet is that is important to refine your analysis by getting feedback from other people to see if they notice things that you missed, have an idea of what additional relationship you might explore or how to better transform your data for a better metric. Our goal is clear communication; you have spent more time thinking about this question and the visualization so some things might be obvious to you that others might not catch right away, similar a set of colors might look good to you, but somebody else might struggle to tell things apart. The figure might be too busy etc. Sometimes it can also be helpful to have other people help you refine your question.\nFor part two of this assignment\n1.Chose one question, your final figure and accompanying legend, description, and discussion and share it in our #hwassignments channel on slack. Make sure your figure is posted by Monday (10/9). 2. Take a look your classmates’ posts on slack, identify what you like about the visualization along with some constructive points for improvement. You should post your feedback by Tuesday afternoon. You can use the “reply” feature in slack to reply directly to their post (10/10). 3. Read through the constructive criticism of your own figures of things and make some notes on what you want to improve. This could also include some things that you may have wanted to improve by weren’t sure how to achieve (10/11)."
  },
  {
    "objectID": "17_exploratory-analysis-ii.html#think-ahead",
    "href": "17_exploratory-analysis-ii.html#think-ahead",
    "title": "17  Exploratory Analysis II",
    "section": "17.3 Think ahead",
    "text": "17.3 Think ahead\nYour final project can be summed up as “find a data set - do something with it”.\nChances of finding things are always a lot higher when you have at least a rough idea of what you are looking for. In preparation for formulating a mini-proposal for your final project, identify a broad area of interest for you and brainstorm 2-3 ideas of more specific questions you’re interested in.\nShare your initial thoughts and brainstorming in the #data-project channel on slack. This is also part of your initial topic submission assignment. You only have to do it once.\nDon’t worry, this is not final, just a starting point. As we take a look at databases and data repositories you might find something in a different area you are interested in exploring."
  },
  {
    "objectID": "18_visualizations.html#best-practices-for-effective-figures",
    "href": "18_visualizations.html#best-practices-for-effective-figures",
    "title": "18  Making good figures",
    "section": "18.1 Best practices for effective figures",
    "text": "18.1 Best practices for effective figures\nEffective figures are used to tell a story. You can think of your exploratory analysis as the first step in determining what story the data tells (a fact finding mission if you will). Once you have determined what your specific question hypothesis is, the next step is narrowing down what analysis you will perform. Then you will identify your central results and interpret them in the context of your question. Your next step is communicating your results. At this point you need to identify which figures serve to communicate individual, specific points in the overall narrative.\nIn our example, we have refined our question to “Have impacts due to natural disasters increased at a global scale?”. To answer this question, we are calculating the total annual loss of life and annual total economic damages incurred for each disaster type.\n\ndisaster &lt;- read_delim(\"data/nat-disasters_emdat-query-2021-10-05.txt\", delim = \"\\t\", skip = 6) %&gt;%\n  clean_names()\n\ntotal_yr &lt;- disaster %&gt;%\n  filter(!is.na(total_damages_000_us)) %&gt;%\n  group_by(year,disaster_type) %&gt;%\n  summarize(total_damages_yr = sum(total_damages_000_us))\n\ndeaths &lt;- disaster %&gt;%\n  filter(!is.na(total_deaths)) %&gt;%\n  group_by(year, disaster_type) %&gt;%\n  summarize(total_deaths_yr = sum(total_deaths)) \n\nThen we created bubble plot showing the total loss of life and total damages incurred for each disaster type in each year.\n\n\n\n\n\nImpacts due to natural disasters (1900 - 2020). Impacts were calculated as total annual economic costs (top) and total annual loss of life (bottom) due to each disaster type per year. The size of the bubble is scaled to cost [$] and total deaths [100k]. Data was obtained from EMDAT/International Disaster Database\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nSummarize the overall narrative for the “story” we want to communicate based on our central question and state the specific point that each of these figures makes.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nData visualizations should be clear, accessible, transparent, and honest. For each of these categories list at least three criteria/guidelines that detail how data visualizations fulfill this category. Assess whether our figure(s) above fulfill these criteria and list at least three things that could be improved based this set of criteria.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "18_visualizations.html#best-practices-for-communicating-results",
    "href": "18_visualizations.html#best-practices-for-communicating-results",
    "title": "18  Making good figures",
    "section": "18.2 Best practices for communicating results",
    "text": "18.2 Best practices for communicating results\nVisualizations are a key component to effectively communicating your results. A good rule of thumb is that your written description of your results should allow the reader to understand what you are trying to communicate even without visualizations and that your title & legend should be descriptive enough that even without the written results (some would even say methods) the reader would understand the results as well.\n\n\n\n\n\n\n Consider this\n\n\n\nGive a description of what the components of an effective figure title & legend should look like and assess whether the figure legend above fulfills these criteria. List points for improvement.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a description of what an effective description of results should look like and write a description for the figure(s) above.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a description of the components of an effective discussion and write a brief discussion of the results of our example above (Refer back to the end of chapter 16 for this …you can use your answer to the last question as a starting point).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "18_visualizations.html#practice-the-best-practices",
    "href": "18_visualizations.html#practice-the-best-practices",
    "title": "18  Making good figures",
    "section": "18.3 Practice the best practices!",
    "text": "18.3 Practice the best practices!\n\n\n\n\n\n\n Consider this\n\n\n\nUse one of the questions you explored for chapter 17 and complete the following tasks:\n\nState the overall broad question you are asking\nSummarize your “methods” in 2-3 sentences: How are you transforming/analyzing the data to specifically answer your question?\nSummarize the narrative of the story you want to communicate with this analysis and specify how the visualization serves the overall narrative.\nGenerate your figure. Use bullet points to assess whether your visualization is clear, accessible, transparent, and honest.\nWrite an effective title/legend for your figure.\nWrite an effective description of your results.\nWrite an effective interpretation discussion/interpretation of your results.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "18_visualizations.html#spatial-visualization",
    "href": "18_visualizations.html#spatial-visualization",
    "title": "18  Making good figures",
    "section": "18.4 Spatial visualization",
    "text": "18.4 Spatial visualization\nYou have probably noticed that for some of the geographic comparisons were are making the most effective way to communicate those results would be to create a map with countries color coded according to the metric we are assessing.\nLet’s learn a straightforward method for visualization of spatial data. Run each of the code chunks below and comment the code line by line to describe what each function/argument is doing.\nInstall these packages as needed:\n\ninstall.packages(\"rnaturalearth\")\n\ninstall.packages(\"rnaturalearthdata\")\n\ninstall.packages(\"sf\")\n\nNow we can load these libraries.\n\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\n\nThe first thing we need is is some maps! The package rnaturalearth provides a map of countries of the entire world. Let’s load a map as an sf class.\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nclass(world)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe see that world is both a data.frame and a class sf(simple features) which is a class of R object designed specifically for plotting maps.\nLet’s take a look at the column names\n\ncolnames(world)\n\n [1] \"scalerank\"  \"featurecla\" \"labelrank\"  \"sovereignt\" \"sov_a3\"    \n [6] \"adm0_dif\"   \"level\"      \"type\"       \"admin\"      \"adm0_a3\"   \n[11] \"geou_dif\"   \"geounit\"    \"gu_a3\"      \"su_dif\"     \"subunit\"   \n[16] \"su_a3\"      \"brk_diff\"   \"name\"       \"name_long\"  \"brk_a3\"    \n[21] \"brk_name\"   \"brk_group\"  \"abbrev\"     \"postal\"     \"formal_en\" \n[26] \"formal_fr\"  \"note_adm0\"  \"note_brk\"   \"name_sort\"  \"name_alt\"  \n[31] \"mapcolor7\"  \"mapcolor8\"  \"mapcolor9\"  \"mapcolor13\" \"pop_est\"   \n[36] \"gdp_md_est\" \"pop_year\"   \"lastcensus\" \"gdp_year\"   \"economy\"   \n[41] \"income_grp\" \"wikipedia\"  \"fips_10\"    \"iso_a2\"     \"iso_a3\"    \n[46] \"iso_n3\"     \"un_a3\"      \"wb_a2\"      \"wb_a3\"      \"woe_id\"    \n[51] \"adm0_a3_is\" \"adm0_a3_us\" \"adm0_a3_un\" \"adm0_a3_wb\" \"continent\" \n[56] \"region_un\"  \"subregion\"  \"region_wb\"  \"name_len\"   \"long_len\"  \n[61] \"abbrev_len\" \"tiny\"       \"homepart\"   \"geometry\"  \n\n\nYou can see that it contains quote a bit of information including some data about each country. The column geometry has the information about the country “shapes” (multipolygons) we want to plot.\n\nworld %&gt;%\n  select(geometry) %&gt;%\n  head()\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -70.06611 ymin: -18.01973 xmax: 74.89131 ymax: 60.40581\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n                        geometry\n1 MULTIPOLYGON (((-69.89912 1...\n2 MULTIPOLYGON (((74.89131 37...\n3 MULTIPOLYGON (((14.19082 -5...\n4 MULTIPOLYGON (((-63.00122 1...\n5 MULTIPOLYGON (((20.06396 42...\n6 MULTIPOLYGON (((20.61133 60...\n\n\nWe can plot the map using ggplot::geom_sf() and the same syntax we generally use. We do not need to specify x or y coordinates as ggplot recognizes that we have passed a object of the class sf and that it contains a column called geometry.\n\nggplot(data = world) +\n  geom_sf()\n\n\n\n\nWe can manipulate the geom_sf layer using the same arguments we have used for other plot types. For example, let’s make all the countries orange.\n\nggplot(data = world) + \n    geom_sf(color = \"black\", fill = \"orange\") \n\n\n\n\nOr, we could color code them according to the column pop_est in the world data.frame which contains population estimates for each country.\n\nggplot(data = world) +\n    geom_sf(aes(fill = pop_est)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n  coord_sf() \n\n\n\n\nLet’s say we want to create a map that compares the number of droughts that occurred in each country from 2010 - 2020. First we would need to transform our raw data.\n\ndroughts &lt;- disaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  group_by(iso) %&gt;%\n  summarize(droughts_total = n())\n\nNow we can use left_join() to join the world and drought data.frames. Note that I used the column iso to calculate the number of droughts, this is equivalent to the column iso_a3 in the world database. These are internationally recognized codes that designate every country and most independent areas with either a two or in this case three-letter abbreviations.\n\nworld &lt;- world %&gt;%\n  left_join(droughts, by = c(\"iso_a3\" = \"iso\"))\n\nhead(world)\n\nSimple feature collection with 6 features and 64 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -70.06611 ymin: -18.01973 xmax: 74.89131 ymax: 60.40581\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n  scalerank      featurecla labelrank     sovereignt sov_a3 adm0_dif level\n1         3 Admin-0 country         5    Netherlands    NL1        1     2\n2         1 Admin-0 country         3    Afghanistan    AFG        0     2\n3         1 Admin-0 country         3         Angola    AGO        0     2\n4         1 Admin-0 country         6 United Kingdom    GB1        1     2\n5         1 Admin-0 country         6        Albania    ALB        0     2\n6         3 Admin-0 country         6        Finland    FI1        1     2\n               type       admin adm0_a3 geou_dif     geounit gu_a3 su_dif\n1           Country       Aruba     ABW        0       Aruba   ABW      0\n2 Sovereign country Afghanistan     AFG        0 Afghanistan   AFG      0\n3 Sovereign country      Angola     AGO        0      Angola   AGO      0\n4        Dependency    Anguilla     AIA        0    Anguilla   AIA      0\n5 Sovereign country     Albania     ALB        0     Albania   ALB      0\n6           Country       Aland     ALD        0       Aland   ALD      0\n      subunit su_a3 brk_diff        name     name_long brk_a3    brk_name\n1       Aruba   ABW        0       Aruba         Aruba    ABW       Aruba\n2 Afghanistan   AFG        0 Afghanistan   Afghanistan    AFG Afghanistan\n3      Angola   AGO        0      Angola        Angola    AGO      Angola\n4    Anguilla   AIA        0    Anguilla      Anguilla    AIA    Anguilla\n5     Albania   ALB        0     Albania       Albania    ALB     Albania\n6       Aland   ALD        0       Aland Aland Islands    ALD       Aland\n  brk_group abbrev postal                    formal_en formal_fr note_adm0\n1      &lt;NA&gt;  Aruba     AW                        Aruba      &lt;NA&gt;     Neth.\n2      &lt;NA&gt;   Afg.     AF Islamic State of Afghanistan      &lt;NA&gt;      &lt;NA&gt;\n3      &lt;NA&gt;   Ang.     AO  People's Republic of Angola      &lt;NA&gt;      &lt;NA&gt;\n4      &lt;NA&gt;   Ang.     AI                         &lt;NA&gt;      &lt;NA&gt;      U.K.\n5      &lt;NA&gt;   Alb.     AL          Republic of Albania      &lt;NA&gt;      &lt;NA&gt;\n6      &lt;NA&gt;  Aland     AI                Åland Islands      &lt;NA&gt;      Fin.\n  note_brk   name_sort name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13\n1     &lt;NA&gt;       Aruba     &lt;NA&gt;         4         2         2          9\n2     &lt;NA&gt; Afghanistan     &lt;NA&gt;         5         6         8          7\n3     &lt;NA&gt;      Angola     &lt;NA&gt;         3         2         6          1\n4     &lt;NA&gt;    Anguilla     &lt;NA&gt;         6         6         6          3\n5     &lt;NA&gt;     Albania     &lt;NA&gt;         1         4         1          6\n6     &lt;NA&gt;       Aland     &lt;NA&gt;         4         1         4          6\n   pop_est gdp_md_est pop_year lastcensus gdp_year                    economy\n1   103065     2258.0       NA       2010       NA       6. Developing region\n2 28400000    22270.0       NA       1979       NA  7. Least developed region\n3 12799293   110300.0       NA       1970       NA  7. Least developed region\n4    14436      108.9       NA         NA       NA       6. Developing region\n5  3639453    21810.0       NA       2001       NA       6. Developing region\n6    27153     1563.0       NA         NA       NA 2. Developed region: nonG7\n               income_grp wikipedia fips_10 iso_a2 iso_a3 iso_n3 un_a3 wb_a2\n1 2. High income: nonOECD        NA    &lt;NA&gt;     AW    ABW    533   533    AW\n2           5. Low income        NA    &lt;NA&gt;     AF    AFG    004   004    AF\n3  3. Upper middle income        NA    &lt;NA&gt;     AO    AGO    024   024    AO\n4  3. Upper middle income        NA    &lt;NA&gt;     AI    AIA    660   660  &lt;NA&gt;\n5  4. Lower middle income        NA    &lt;NA&gt;     AL    ALB    008   008    AL\n6    1. High income: OECD        NA    &lt;NA&gt;     AX    ALA    248   248  &lt;NA&gt;\n  wb_a3 woe_id adm0_a3_is adm0_a3_us adm0_a3_un adm0_a3_wb     continent\n1   ABW     NA        ABW        ABW         NA         NA North America\n2   AFG     NA        AFG        AFG         NA         NA          Asia\n3   AGO     NA        AGO        AGO         NA         NA        Africa\n4  &lt;NA&gt;     NA        AIA        AIA         NA         NA North America\n5   ALB     NA        ALB        ALB         NA         NA        Europe\n6  &lt;NA&gt;     NA        ALA        ALD         NA         NA        Europe\n  region_un       subregion                 region_wb name_len long_len\n1  Americas       Caribbean Latin America & Caribbean        5        5\n2      Asia   Southern Asia                South Asia       11       11\n3    Africa   Middle Africa        Sub-Saharan Africa        6        6\n4  Americas       Caribbean Latin America & Caribbean        8        8\n5    Europe Southern Europe     Europe & Central Asia        7        7\n6    Europe Northern Europe     Europe & Central Asia        5       13\n  abbrev_len tiny homepart droughts_total                       geometry\n1          5    4       NA             NA MULTIPOLYGON (((-69.89912 1...\n2          4   NA        1             65 MULTIPOLYGON (((74.89131 37...\n3          4   NA        1             26 MULTIPOLYGON (((14.19082 -5...\n4          4   NA       NA              1 MULTIPOLYGON (((-63.00122 1...\n5          4   NA        1             12 MULTIPOLYGON (((20.06396 42...\n6          5    5       NA             NA MULTIPOLYGON (((20.61133 60...\n\n\nNow our world data.frame has a column specifying the number of droughts that occurred.\n\nggplot(data = world) +\n    geom_sf(aes(fill = droughts_total)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    coord_sf(expand = FALSE) +\n  labs(x = \"longitude\", y = \"latitude\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nUse the figure we just generated and go through our steps for creating good visualizations according to our best practices:\n\nState the overall broad question you are asking\nSummarize your “methods” in 2-3 sentences: How are you transforming/analyzing the data to specifically answer your question?\nSummarize the narrative of the story you want to communicate with this analysis and specify how the visualization serves the overall narrative.\nGenerate your figure - use the code above but optimize the figure as needed. Use bullet points to assess whether your visualization is clear, accessible, transparent, and honest.\nWrite an effective title/legend for your figure.\nWrite an effective description of your results.\nWrite an effective interpretation discussion/interpretation of your results.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "19_figure-types.html#section",
    "href": "19_figure-types.html#section",
    "title": "19  Making good figures",
    "section": "19.1 ",
    "text": "19.1"
  },
  {
    "objectID": "A_hello-world.html",
    "href": "A_hello-world.html",
    "title": "Hello world",
    "section": "",
    "text": "The phrase “hello world” is a popular and simple introductory program that is often used to demonstrate the basic syntax and structure of a programming language. It’s typically one of the first programs that people write when learning a new programming language.\nIt is usually a simple program that prints the text Hellow World to the output, typically a console or terminal window. Writing and running a “hello world” program helps beginners understand the essential steps of writing, compiling, and executing code in a specific programming language.\nTo create a “Hello, World!” program in R, you can use the print() function:\nprint(\"Hello, World!\")\n\nIn this section, we will get an overview of what Data Science is, install R/Rstudio and orient ourselves in how to use Rstudio and learn the basics about using R as an object-oriented language."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for a Changing Planet",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#course-format",
    "href": "index.html#course-format",
    "title": "Data Science for a Changing Planet",
    "section": "Course Format",
    "text": "Course Format\nThis is a lecture/lab course meeting three times a week (MWF 10:25-11:15am & 11:30am - 12:20pm). Key concepts will be introduced in reading and annotation assignments, short ‘prelab’-type tutorials, or exercises that you will be expected to complete before class as specified on Canvas.\nLecture/lab time will incorporate brief refreshers and more in-depth demonstrations of some of the more complex topics (participatory code-alongs), with a the focus on applying the concepts and ‘learning by doing’ by directly applying new skills to case studies and data sets from ecology, environmental science and public health."
  },
  {
    "objectID": "index.html#byod-bring-your-own-device-to-lab",
    "href": "index.html#byod-bring-your-own-device-to-lab",
    "title": "Data Science for a Changing Planet",
    "section": "BYOD: Bring your own device (to lab)",
    "text": "BYOD: Bring your own device (to lab)\nYou are required to bring your laptop for use during class times; a tablet will not be sufficient to participate though you are welcome to bring an additional device to have an extra screen to follow along an electronic version of the lab handbook. Make sure to have a power cable and/or fully charged battery! If you run into computer issues please let Dr. O’Leary know as soon as possible so we can come up with a back-up plan such as organizing a laptop to use during/outside of class time to complete activities."
  },
  {
    "objectID": "20_reproducibility-crisis.html#frabrication-falsification-and-plagiarism-oh-my",
    "href": "20_reproducibility-crisis.html#frabrication-falsification-and-plagiarism-oh-my",
    "title": "20  Scientific misconduct and the reproducibility crisis",
    "section": "20.1 Frabrication, falsification, and plagiarism (oh my!)",
    "text": "20.1 Frabrication, falsification, and plagiarism (oh my!)\n\n\n\n\n\n\n Consider this\n\n\n\nThe National Science Foundation defines scientific misconduct in the categories of fabrication, falsification, and plagiarism. Give a brief definition of each term.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake the set of descriptions that describe hypothetical1 scenarios of decision making during data generation and analysis. Classify each as either scientific misconduct according to NSF’s definition or not.\nIdentify scenarios that you are not sure of which category they fall into and be ready to discuss with the class.\n\n\n1 Hypothetical because not referring to a specific case study but these are very realistic scenarios of daily decision-making during research.\n\n\n\n\n\n Did it!\n\n\n\n[Snap a picture, move it to your images folder and then insert it here].\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake your scenarios and rank them along a continuum of ethical to unethical.\nConsider these aspects to establish your ranking:\n\nare some unethical practices worse than others?\nwhich scenarios do you think are more common than others?\ndo you think some are easy to get away with?\nhow easy do you think it is to detect if something like this has taken place?\nwhose responsibility is it to ensure unethical conduct does not take place during the research process?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nAfter discussion with the class identify at least five major categories of misconduct and unethical behavior and give an example for each. Briefly discuss why for some categories identifying misconduct and unethical conduct is more clear cut while for others it can be difficult to draw a definitive line.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "20_reproducibility-crisis.html#p-hacking-and-data-dredging",
    "href": "20_reproducibility-crisis.html#p-hacking-and-data-dredging",
    "title": "20  Scientific misconduct and the reproducibility crisis",
    "section": "20.2 P-hacking and data dredging",
    "text": "20.2 P-hacking and data dredging\n\n\n\n\n\n\n Consider this\n\n\n\nImagine you are a social scientist interested in how political parties impact the US economy.\nFirst, develop a hypothesis of whether Democrats or Republicans being in office positively or negatively impacts the US economy.\nNow, use real data going back to 1948 to investigate. To publish your data you would need a statistically significant result. Fortunately you can hack your way to scientific glory using fivethirtyeight’s interactive tool. Describe how you were able to confirm your hypothesis by manipulating which group of politicians to include, how you measured economic performance and other options.\nFinally, formulate a second opposing theory and see if you can generate a statistically significant result for that2.\n\n\n2 Pro Tip: Find a p-hacking buddy and test alternate hypotheses and then swap your results!\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nCongratulations, you just became a successful p-hacker. The practices of p-hacking and data dredging have become increasing common in the era of big data.\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe what the practices of p-hacking and data-dredging entail.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe what the reproducibility crisis is and argue which fields of science you would expect to be more/less heavily impacted and how the increasing availability of large data sets and deployment of complex methods of analysis (including machine learning) have contributed.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "20_reproducibility-crisis.html#you-keep-using-that-word-but",
    "href": "20_reproducibility-crisis.html#you-keep-using-that-word-but",
    "title": "20  Scientific misconduct and the reproducibility crisis",
    "section": "20.3 You keep using that word, but …",
    "text": "20.3 You keep using that word, but …\n\n\n\n\n\n\n Consider this\n\n\n\nReplicability and reproducibility of studies both generally refer to the practice of validating the results obtained by duplicating them. However, exact definitions of the terms vary among fields of research. Briefly, argue how you would rank different levels of confidence in the results of a study based on whether it was been repeated with the same results using (combinations of) the same or different teams, the same or different experimental set-ups, and/or the same or different data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThe National Science Foundation (NSF) defines “replicability” as “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”.\n\n\n\n\n\n\n Consider this\n\n\n\n(Goodman, Fanelli, and Ioannidis 2016) propose a framework that defines three categories based on the goals as related to transparency & compete reporting of methods, producing new evidence and drawing the same conclusion. Briefly compare and contrast the categories of methods reproducibility, results reproducibility, and inferential reproducibility.\n\n\n\nGoodman, Steven N., Daniele Fanelli, and John P. A. Ioannidis. 2016. “What Does Research Reproducibility Mean?” Science Translational Medicine 8 (341): 341ps12–12. https://doi.org/10.1126/scitranslmed.aaf5027.\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss how (lack of) reproducibility can undermine confidence in the scientific process from the general public and/or allow special interest groups to manipulate information to intentionally sow distrust.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "21_reproducibility-research.html",
    "href": "21_reproducibility-research.html",
    "title": "21  Scientific misconduct and the reproducibility crisis",
    "section": "",
    "text": "Learning Objectives\nAfter completing this tutorial you should be able to\n\n\n\nDownload the directory for this project here, make sure the directory is unzipped and move it to your bi328 directory. You can open the Rproj for this module either by double clicking on it which will launch Rstudio or by opening Rstudio and then using File &gt; Open Project or by clicking on the Rproject icon in the top right of your program window and selecting Open Project.\nThere should be a file named 21_reproducibility-research.qmd in that project directory. Use that file to work through this tutorial - you will hand in your rendered (“knitted”) quarto file as your homework assignment. So, first thing in the YAML header, change the author to your name. You will use this quarto document to record your answers. Remember to use comments to annotate your code; at minimum you should have one comment per code set1 you may of course add as many comments as you need to be able to recall what you did. Similarly, take notes in the document as we discuss discussion/reflection questions but make sure that you go back and clean them up for “public consumption”.1 You should do this whether you are adding code yourself or using code from our manual, even if it isn’t commented in the manual… especially when the code is already included for you, add comments to describe how the function works/what it does as we introduce it during the participatory coding session so you can refer back to it."
  },
  {
    "objectID": "22_research-compendium.html",
    "href": "22_research-compendium.html",
    "title": "22  Research Compendium",
    "section": "",
    "text": "Learning Objectives\nAfter completing this tutorial you should be able to\n\n\n\nDownload the directory for this project here, make sure the directory is unzipped and move it to your bi328 directory. You can open the Rproj for this module either by double clicking on it which will launch Rstudio or by opening Rstudio and then using File &gt; Open Project or by clicking on the Rproject icon in the top right of your program window and selecting Open Project.\nThere should be a file named 22_research-compendium.qmd in that project directory. Use that file to work through this tutorial - you will hand in your rendered (“knitted”) quarto file as your homework assignment. So, first thing in the YAML header, change the author to your name. You will use this quarto document to record your answers. Remember to use comments to annotate your code; at minimum you should have one comment per code set1 you may of course add as many comments as you need to be able to recall what you did. Similarly, take notes in the document as we discuss discussion/reflection questions but make sure that you go back and clean them up for “public consumption”.1 You should do this whether you are adding code yourself or using code from our manual, even if it isn’t commented in the manual… especially when the code is already included for you, add comments to describe how the function works/what it does as we introduce it during the participatory coding session so you can refer back to it."
  },
  {
    "objectID": "21_reproducibility-research.html#what-even-are-these-methods-you-speak-of",
    "href": "21_reproducibility-research.html#what-even-are-these-methods-you-speak-of",
    "title": "21  Scientific misconduct and the reproducibility crisis",
    "section": "21.1 What even are these ‘Methods’ you speak of?",
    "text": "21.1 What even are these ‘Methods’ you speak of?\n\n\n\n\n\n\n Consider this\n\n\n\nThink back on scientific papers you have read or lab reports you have written and list the main sections of the typical format you encounter.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief description of what the “Methods” or “Method & Materials” section of a scientific paper or report contains and assess whether or not you think this section generally meets the standard you have just laid out.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nOne framework to categorize components of the “methods” of a study is to place them into three steps that build on each other.\n\nAcquire data\nProcess data\nAnalyze data\n\n\n\n\n\n\n\n Consider this\n\n\n\nUse the example of our recent (re)analysis of the Shark Nursery study or the fungi diversity study as an example to describe what each of these steps entails.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "21_reproducibility-research.html#tools-of-the-trade",
    "href": "21_reproducibility-research.html#tools-of-the-trade",
    "title": "21  Scientific misconduct and the reproducibility crisis",
    "section": "21.2 Tools of the trade",
    "text": "21.2 Tools of the trade\nTiny history lesson Lotus123 is a what launched IBM Personal Computers into offices around the world …\n\n\n\nLotus123\n\n\nMy very first PC adventures included running Lotus123 off of big floppy disks. Microsoft developed Excel and the Office package and quickly PCs where not only in offices and homes around the world. Spreadsheet applications were initially focused mainly on managing and organizing data (think HR department and payroll) but increasingly complicated calculations were possible and soon Excel sneakily made its way into scientific research.\nToday, the tools used by scientists to analyze their data vary from highly specialized tools for very specific tasks (each with their own required data format), to large software packages like SAS and STATA. Many of these tools have GUIs (graphic user interfaces) and are what is frequently referred to as “point & click” or “WYSIWYG”2.2 WYSISWG = What you see is what you get. Compare this to WYWIWYG = what you want is want you get.\n\n\n\n\n\n\n Consider this\n\n\n\nRely on your experience of having used programs like Excel, SAS/STATA, or other specialized programs in a statistics class or other lab courses you have taken to compare and contrast using those types of programs for data analysis to programing languages like R, Perl or Python. Discuss the benefits and drawbacks of both approaches.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "21_reproducibility-research.html#impacts-of-flawed-data-analysis",
    "href": "21_reproducibility-research.html#impacts-of-flawed-data-analysis",
    "title": "21  Scientific misconduct and the reproducibility crisis",
    "section": "21.3 Impacts of Flawed Data Analysis",
    "text": "21.3 Impacts of Flawed Data Analysis\n\n21.3.1 Case Study 1\n\n\n\n\n\n\n Consider this\n\n\n\nPull up this paper on the connection of CSF interleukin-6 and depression. Quickly skim the abstract and summarize the main result. Do this before untoggling the solution button for part 2 of this question.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nOkay, fine - that was a little bit of a trick question; did you notice the retraction notice?\nYou can access a more detailed version of the retraction here. Read the retraction notice and briefly describe why the paper needed to be retracted.\nAfter the authors fixed their mistake they published a corrected version of the paper. Compare the titles & abstracts of the retracted and updated papers to determine if their mistakes had a significant impact on the reported results.\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief summary of key lessons you have learned from the case study.\n\n\n\n\n21.3.2 Case Study 2\nBefore today’s class you should have completed these readings:\n\nShariff et al 2016 “What is the association between religious affiliation and children’s altruism?”\nRetraction notices of Decety et al. 2015 “RETRACTED: The Negative Association between Religiousness and Children’s Altriusm across the World”\nDig a little deeper: How a study based on a typo made news everywhere - and the retraction didn’t.\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss why the paper needed to be retracted; as part of your discussion compare and contrast the correspondence published by Shariff et al. to the retraction notice from the original authors Consider the following (and of course add anything else you noticed/think is important).\n\nWhy was the paper retracted?\nWhat is the central statement being made in the correspondence/retraction notice?\nWhat is the tone of the correspondence/retraction notice?\nHow could this mistake have been avoided?\nHow could bias of both sets of authors/previous knowledge have played into the conclusions of the paper/noticing and identifying the mistakes (consider both sets of authors)?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThe results of the initial study were widely published. By contrast, the retraction was not; matter of fact most people “doing their own research” on the interwebs are considerably more likely to come across an article describing the initial findings and we all know from personal experience how infrequently we look up the original study being described - so chances are they might never discover that the study has been retracted.\nUse this specific example to illustrate what Brandolini’s Law (i.e. the Bullshit Asymmetry Principle) is.\n\n\n\nBrandolini’s Law\n\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief summary of key lessons you have learned from this case study.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n21.3.3 Case Study 3\nBefore today’s class you should have read key sections from Herndon et al. 2014 “Does high public debt consistently stifle economic growth? A critique of Reinhart & Rogoff”:\n\nSection 1 (Introduction)\nSection 2 (Public impact and policy relevance)\nIntroduction to section 3. (Replication) + Section Headers\nSection 4 (Conclusion)\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly summarize the major points of criticism of the original paper.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss the tools that were used for analysis and determine how they could have contributed to the flawed analysis being published (remember, it did get through peer-review!)\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nRecall our initial classification scientific misconduct along a continuum of “Responsible - Ethical - Unethical/Misconduct” and use that to compare the flawed analysis laid out here to the issues that were uncovered in the previous two case studies. Consider factors including content, when/why mistakes were uncovered, and intent.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nConsider the policy and decision-making implications of this (and the other two papers) and discuss if you think retracting the actual paper with mistakes in it is sufficient.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nPropose strategies that could be used to avoid flawed analysis demonstrated in this (and the other) case studies to be published in the first place and whose role it should be to “enforce” responsible conduct.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nGive a brief summary of key lessons you have learned from this case study.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "22_research-compendium.html#an-organized-reproducible-and-shareable-workflow",
    "href": "22_research-compendium.html#an-organized-reproducible-and-shareable-workflow",
    "title": "22  Research Compendium",
    "section": "22.1 An organized, reproducible and shareable workflow",
    "text": "22.1 An organized, reproducible and shareable workflow\nIn Chapter 4 of our labmanual you were introduced to Project management and Rmarkdown Basics. We said that the goal of open science and reproducible research is to make scientific methods, data, and results more transparent, available and reproducible. In that section, you were introduced to Rprojects and quarto documents and the structure of our Project Folders for this semester with the reasoning that this structure would allow you to establish a reproducible workflow that is organized, reproducible, and shareable.\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss the advantages of designing a workflow that fulfills the three-fold goal of being organized, reproducible, and shareable.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "22_research-compendium.html#research-compendium",
    "href": "22_research-compendium.html#research-compendium",
    "title": "22  Research Compendium",
    "section": "22.2 Research Compendium",
    "text": "22.2 Research Compendium\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly define what a research compendium is including central components/principles to be effective, then assess whether our typical structure of a project folder fulfills these criteria2.\n\n\n2 A good place to to start would be to look at what subdirectories we typically include, whether that is consistent or if some folders only show up occassionaly and to think through what the function of each folder is and how they contribute to the core principles that comprise a good research compendium\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nFor your data science project you will be asked to submit your entire research compendium along with the rendered quarto report. Go ahead and create a research compendium in your bi3249 directory right now if you haven’t already and initialize the subdirectories, Rproject and quarto documents you will need. Use bullet points to list the contents and hierarchy of your research compendium. Then give a brief description of your sete up - write it as if you were sharing your research compendium with a collaborator that needs to know what function each component has and what you underlying design is.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "F_reproducible-research.html#is-science-broken",
    "href": "F_reproducible-research.html#is-science-broken",
    "title": "Responsible research is reproducible research",
    "section": "Is science broken?",
    "text": "Is science broken?\nThe so-called reproducibility crisis describes the fact that recent studies have found that particularly in psychology, social sciences, and the biomedical field many published studies cannot be reliably replicated or reproduced by other scientists. This lack of reproducibility suggests that the original findings may be unreliable and has raised concerns about the credibility and validity of scientific findings as a whole. Several causes contributed to the reproducibility crisis including favoring the publication of positive results over negative ones (publication bias), manipulating data analysis to achieve statistically significant results (p-hacking), running comparisons in large data sets until significant results are found (data dredging), or small sample sizes.The consequences of thes practices are far reaching as trust in science is undermined, resources are wasted on non-replicable findings, and the advancement of knowledge in certain fields is hindered, as well as other implications for policy and decision-making based on scientific findings. The scientific community has recognized the need to address the reproducibility crisis and efforts are being made to promote transparency in how methods and results are reported, for example through open science practices which emphasis sharing data and making research materials publicly available."
  },
  {
    "objectID": "F_reproducible-research.html#responsible-research-is-reproducible-research",
    "href": "F_reproducible-research.html#responsible-research-is-reproducible-research",
    "title": "Responsible research is reproducible research",
    "section": "Responsible Research is Reproducible Research",
    "text": "Responsible Research is Reproducible Research\nReproducibility is a critical component of Responsible Conduct of Research (RCR) because it directly relates to the integrity and ethical conduct of scientific investigations as it encourages transparency in research. Scientists who make their methods, data, and analyses fully available to other facilitate accountability and enable scrutiny. This transparency is essential to maintaining the integrity of the research process which is a core principle of RCR. By allowing other researchers to independently verify and confirm the results of a study the credibility and trustworthiness of the research is reinforced and the integrity of the scientific process as a whole is maintained. When scientific studies an be successfully replicated and results are reproducible it reinforces the trust that society places in science and its ability to generate reliable and valid knowledge that can be used in policy and other decision-making processes."
  },
  {
    "objectID": "19_figure-types.html#data-visualizations",
    "href": "19_figure-types.html#data-visualizations",
    "title": "19  Know your plots",
    "section": "19.1 Data visualizations",
    "text": "19.1 Data visualizations\nData visualizations refer to any type of visual display of data intended to help us better understand the underlying data. Therefore, a visualization should have a specific point that supports the overal narrative you are trying to communicate to your audience.\n\n\n\n\n\n\n Consider this\n\n\n\nGiven this definition, argue whether or not you think a table should be considered a data visualization.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe have already looked at one way of classifying figures based on their purpose. We distinguished between exploratory plots that help us better understand and discover hidden patterns in the data and explanatory figures which are designed to clearly communicate insights with others. In both cases, we are using the figure to reveal patterns in the data.\nThe grammar of graphics defines the minimum components of a figure as\n\nA data set\nVariables to be encoded (mappings/aesthetics)\nThe plot type (geometry)\n\nTo make effective plots, we need to be able to make sure that we can recognize the type of variables we are trying to encode, i.e. we want to visualize. The reason why we call this “encoding” is that we are taking the raw data and summarizing it visually - this means that we have to create a consistent system where specific visual components represent raw or summarized data. This means that we have to look at our variables and determine if they are categorical or continuous, if they have specific groupings we want to highlight. Similarly, we need to determine if we are trying to visualize a relationship between multiple variables or rather relationships among multiple variables. Next to choosing what to display on an x- or y-axis2 and visual aspects like colors, shapes, or linetypes we need to make sure that the plot type (geometry) is appropriate for the type of data we are looking at and whether we are summarizing a relationship or a distribution.2 assuming we are visualizing within a Cartesian system, otherwise we would need to think about how to map variables onto the coordinates comprising that system.\nLet’s take a closer look at some standard plot types (geometries) specifically from the perspective of what type of data they display."
  },
  {
    "objectID": "19_figure-types.html#matching-mappings-and-geometries-to-variable-type",
    "href": "19_figure-types.html#matching-mappings-and-geometries-to-variable-type",
    "title": "19  Know your plots",
    "section": "19.2 Matching Mappings and Geometries to Variable Type",
    "text": "19.2 Matching Mappings and Geometries to Variable Type\nLet’s take another look at our natural disaster data set and the variables it encompasses.\n\ndisaster &lt;- read_delim(\"data/nat-disasters_emdat-query-2021-10-05.txt\", delim = \"\\t\", skip = 6) %&gt;%\n  clean_names()\n\ncolnames(disaster)\n\n [1] \"dis_no\"                      \"year\"                       \n [3] \"seq\"                         \"glide\"                      \n [5] \"disaster_group\"              \"disaster_subgroup\"          \n [7] \"disaster_type\"               \"disaster_subtype\"           \n [9] \"disaster_subsubtype\"         \"event_name\"                 \n[11] \"country\"                     \"iso\"                        \n[13] \"region\"                      \"continent\"                  \n[15] \"location\"                    \"origin\"                     \n[17] \"associated_dis\"              \"associated_dis2\"            \n[19] \"ofda_response\"               \"appeal\"                     \n[21] \"declaration\"                 \"aid_contribution\"           \n[23] \"dis_mag_value\"               \"dis_mag_scale\"              \n[25] \"latitude\"                    \"longitude\"                  \n[27] \"local_time\"                  \"river_basin\"                \n[29] \"start_year\"                  \"start_month\"                \n[31] \"start_day\"                   \"end_year\"                   \n[33] \"end_month\"                   \"end_day\"                    \n[35] \"total_deaths\"                \"no_injured\"                 \n[37] \"no_affected\"                 \"no_homeless\"                \n[39] \"total_affected\"              \"reconstruction_costs_000_us\"\n[41] \"insured_damages_000_us\"      \"total_damages_000_us\"       \n[43] \"cpi\"                         \"adm_level\"                  \n[45] \"admin1_code\"                 \"admin2_code\"                \n[47] \"geo_locations\"              \n\n\nLine plots display quantitative trends over time\nLet’s take a closer look at the occurrence of Meteorological disasters and determine how many occurred in each year.\n\n# count number of meteorological disasters per year\nmetereol &lt;- disaster %&gt;%\n  filter(disaster_subgroup == \"Meteorological\") %&gt;%\n  count(year)\n\n# preview data\nhead(metereol)\n\n# A tibble: 6 × 2\n   year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  1900     1\n2  1902     1\n3  1903     2\n4  1904     1\n5  1905     1\n6  1906     3\n\n\nOur data set now has two columns, year and number of occurences. This is what is frequently referred to as a time series, i.e. a data set that tracks a sample over time.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize this data using a line graph. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded3, and why a line plot is the appropriate geometry for this type of data set4. Try out a few different bin sizes and then briefly explain why you chose this specific bin size.\n\n\n4 A good way to do this is to highlight what the advantages are of visualizing the data in this way3 Think about what your 1-2 intro sentences to this figure would be if you were explaining it to somebody else, i.e. how would you orient them in terms of how to interpret the figure\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your visualization could look like\n\n\nCode\nggplot(metereol, aes(x = year, y = n)) +\n  geom_line(binwidth = 5, color = \"black\", fill = \"orange\") +\n  labs(title = \"Yearly occurences of metereological-caused natural disasters\",\n       subtitle = \"Number of natural disasters recorded as metereological by the International Disaster \\nDatabase per year for 1900 - 2021\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\nA line graph (line chart or line plot) displays data points as a series of individual markers or points connected by straight lines; frequently the individual points are not plotted. It is most appropriately used to illustrate how one variable changes over time. Time is generally displayed on the x-axis and the magnitude of the continuous variable is encoded as the position on the y-axis.\nHere is how the data is encoded:\n\nThe x-axis typically represents the independent variable; for a time series data set this will be time.\nThe y-axis represents the dependent variable, showing the values or measurements corresponding to each data point.\nData points on the x/y axis are connected by straight lines creating a continuous path that visually represents the change or trend in the data between the observed points.\nEach data point represents a specific observation (values) generally depicted by symbols, such as circles, squares, or triangles, to make them easily distinguishable. For high resolution data sets, data points are frequently not shown.\n\nLine graphs are particularly useful for the following purposes:\n\nVisualizing continuous data with a clear sequence of values, specifically showing trends over time.\nComparing multiple data (time) series using multiple lines on the same graph can be used to compare how different variables or categories change over the same time period.\nIdentifying patterns and anomalies\nInterpolating data Between points allow for the estimation of values between data points, which can be useful for making predictions or understanding trends between observed data - this should be done only when appropriate relationships have been shown.\n\nShowing a line implies continuity and it can be difficult to determine the resolution of individual data points. You should make your plot more honest and transparent by adding additional points to show how frequent the measurements really are.\n\n\nCode\nggplot(metereol, aes(x = year, y = n)) +\n  geom_point() +\n  geom_line(binwidth = 5, color = \"black\", fill = \"orange\") +\n  labs(title = \"Yearly occurences of metereological-caused natural disasters\",\n       subtitle = \"Number of natural disasters recorded as metereological by the International Disaster \\nDatabase per year for 1900 - 2021\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\nAn alternative would be to not connect the individual data points with a line but rather use a trend line that doesn’t have the same implication that each time point was measured and the information about the resolution is preserved.\n\n\nCode\nggplot(metereol, aes(x = year, y = n)) +\n  geom_smooth() +\n  geom_point() +\n  labs(title = \"Yearly occurences of metereological-caused natural disasters\",\n       subtitle = \"Number of natural disasters recorded as metereological by the International Disaster \\nDatabase per year for 1900 - 2021.\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\n\n\n\nHistograms and density plots convey information about a single quantitative variable\nLet’s say that we were less interested in the temporal trend but only in the variable n itself. We now have a a single quantitative variable (counts). The best way to visualize this type of data is using a histogram.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize this data using a histogram. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded5, and why a histogram is the appropriate geometry for this type of data set6. Try out a few different bin sizes and then briefly explain why you chose this specific bin size.\n\n\n6 A good way to do this is to highlight what the advantages are of visualizing the data in this way5 Think about what your 1-2 intro sentences would be if you were explaining it to somebody else\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your visualization could look like\n\n\nCode\nggplot(metereol, aes(x = n)) +\n  geom_histogram(binwidth = 5, color = \"black\", fill = \"orange\") +\n  labs(title = \"Distribution of yealy occurences of metereological-caused natural disasters\",\n       subtitle = \"Number of natural disasters recorded as metereological by the International Disaster \\nDatabase per year for 1900 - 2021\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\nA histogram is a graphical representation of the distribution of a data set. It’s a way to visualize the frequency or count of data within specific intervals or bins. Histograms are particularly useful for showing the shape of the data distribution, the central tendency, spread, and any significant patterns or outliers.\nThis is how your data is encoded:\n\nThe range of data is divided into a set of intervals or bins. These bins should be of equal width that represents a specific range of values. The width of the bins can vary based on the data and the context.\nThe y-axis of the histogram represents the frequency or count of data points in each bin, i.e. the height of each bar represents the number of observations in that bin.\nThe x-axis represents the data range, with each bin or interval marked along it.\n\nHistograms are useful fo the following reasons:\n\nThey provide a clear visual representation of how data is distributed. You can see whether the data is symmetric, skewed, unimodal, bimodal, or has other characteristics.\nYou can observe the central tendency (mean, median, mode) and the spread (variance, standard deviation) of the data.\nOutliers, which are data points that fall significantly outside the main distribution, are easily identified\n\n\n\n\nAn alternative would be a density plot.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize this data using a density plot. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type. Argue when your think a density plot or a histogram is the better geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your visualization could look like:\n\n\nCode\nggplot(metereol, aes(x = n)) +\n  geom_density(binwidth = 1, color = \"black\", fill = \"orange\", alpha = 0.5) +\n  labs(title = \"Yearly occurence of metereological-caused natural disasters\",\n       subtitle = \"Smoothed distribution of number of natural disasters classified as metereological by the \\nInternational Disaster Database per year 1900 - 2021\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\nA density plot is a graphical representation of the distribution of a continuous data set. It provides a smooth and continuous estimate of the probability density function of the data. Unlike a histogram, which divides the data into discrete bins and represents frequencies within those bins, a density plot creates a continuous curve that approximates the underlying distribution of the data. Density plots effectively highlight the distribution of a variable and help identify extreme values compared to those commonly occurring. Compared to histograms they are less sensitive to the number of bins chosen. However, you cannot easily determine the actual counts for a bin.\n\n\n\nBoxplots summarize a continuous, quantitative variable broken down by categorical variables\nLet’s take a look at the distribution of the number of disasters occurring for each natural disaster type. We could create individual histograms, e.g. by using facet_wrap() or facet_grid(), alternatively boxplots allow us to compare across categories.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize the distribution of number of natural disasters in each major category using a box plot. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nA box plot or box-and-whisker plot, is a graphical representation of the distribution of a data set. It displays key summary statistics that help you understand the central tendency, spread, and the presence of outliers within the data. Box plots are particularly useful for comparing and visualizing the distribution of multiple data sets.\nHere is how you data is encoded\n\nThe box in the plot represents the interquartile range (IQR) of the data. The IQR is the middle 50% of the data, containing the values between the 25th percentile (Q1) and the 75th percentile (Q3). The top and bottom edges of the box represent Q3 and Q1, respectively, while the width of the box signifies the IQR.\nthe line or bar inside the box represents the median (50th percentile) of the data. The median is the middle value when the data is ordered.\nhe whiskers are the lines extending from the box. They reach out to the minimum and maximum values within a defined range, typically defined as “1.5 times the IQR” from the quartiles.\nValues outside the whiskiers range are considered outliers and are plotted individually as points or dots.\n\nBox plots are useful for several purposes:\n\nvisual representation of the range of data values and the spread of the middle 50% of the data.\nIdentifying the central tendency indicated by the line in the box\nDetecting outliers.\nComparing distributions of multiple datasets\n\nAdding individual data points to a box plot can be a helpful technique to provide a more detailed view of the dataset and enhance the information conveyed by the plot. Box plots provide a summary of the central tendency, spread, and presence of outliers, while the individual data points offer a granular view of the individual observations and also give you an idea of how large the data set is on which the data set was built. Data points can provide additional context or insight into the dataset, such as the presence of clusters, gaps, or patterns that might not be apparent from the box plot alone. In some cases, the density of data points at specific values is important information. Adding individual data points shows the concentration of data at particular values, which may not be evident from the box plot alone. By overlaying data points, you can visually validate that the box plot accurately represents the dataset. It allows you to see if there are any unusual patterns or if the data points are distributed as expected.\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  geom_point() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nUsing geom_points() isn’t super helpful, instead we want to use geom_jitter which is going to offset our points. Adding jittered data points to a box plot is a common technique to address the issue of overplotting, particularly when dealing with categorical or discrete data. Overplotting occurs when multiple data points share the same or very similar x-values, making it challenging to visualize the full extent of the data distribution accurately. Jittering helps spread these points out slightly to reveal the underlying data more clearly.\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nSimilar to using a density plot instead of a histogram, you can use a violin plot instead of a box plot. A violin plot is a data visualization that combines aspects of a box plot and a kernel density plot to display the distribution and summary statistics of a data set. It is particularly useful for visualizing the distribution of data across different categories or groups.\nHere is how you data is encoded:\n\nThe main component of a violin plot is a series of symmetrical or mirrored violin-shaped curves, one for each category or group being compared. These shapes represent the probability density of the data at different values along the x-axis. The width of the violin at any given point corresponds to the density of data points at that value.\nThe width of the violins varies to show the density of data at different values. Wider sections indicate higher density, while narrower sections indicate lower density.\n\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_violin() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\n\n\n\nScatter plots visualize the relationship between two quantitative variables\nOur data set includes both natural disasters classified as meteorological and as climatological. We have discussed differences between climate and weather by describing weather as a snapshot compared to climate as a statistical portrait. But we also know that they are inherently linked. Let’s take a look how many events occur every year for both of those categories.\n\n\nCode\n# count number of metereologial/climatological disasters per year\ncounts &lt;- disaster %&gt;%\n  filter(disaster_subgroup %in% c(\"Meteorological\", \"Climatological\")) %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year)\n\n# preview data\nhead(counts)\n\n\n# A tibble: 6 × 3\n# Groups:   disaster_subgroup [1]\n  disaster_subgroup  year     n\n  &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n1 Climatological     1900     2\n2 Climatological     1903     1\n3 Climatological     1906     1\n4 Climatological     1910     9\n5 Climatological     1911     1\n6 Climatological     1918     1\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize this data using a scatter plot - keep in mind that you will need to reformat the data to plot the counts for the different categories on the x and y axis, respectively. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your visualization could look like:\n\n\nCode\ncounts %&gt;%\n  mutate(n = replace_na(n, 0)) %&gt;%\n  pivot_wider(names_from = \"disaster_subgroup\", values_from = \"n\") %&gt;%\n  ggplot(aes(x = Climatological, y = Meteorological)) +\n  geom_smooth(method = \"lm\", color = \"darkblue\") +\n  geom_point(shape = 21, color = \"black\", fill = \"darkorange\", size = 3) +\n  labs(title = \"Comparison of yearly occurence of climatological and metereological disasters.\",\n       subtitle = \"Number of natural disasters classified as metereological by the International Disaster \\nDatabase per year 1900 - 2021. Blue line indicates linear regression with grey area \\nvisualizing the 95% confidence interval.\",\n       x = \"number of meteorological disasters\", y = \"number of climatological disasters\") +\n  theme_standard\n\n\n\n\n\nA scatter plot is a graphical data visualization tool used to display individual data points in a two-dimensional coordinate system (Cartesian coordinates). It is particularly useful for showing the relationship or correlation between two continuous variables.\nThis is how the data is encoded:\n\nEach data point represents an individual observation or data value. These points are typically displayed as dots, circles, or other symbols and are placed at specific coordinates on the plot. Each data point has an x-coordinate (horizontal) and a y-coordinate (vertical).\nThe x-axis represents one of the continuous variables being studied, often referred to as the independent variable.\nThe y-axis represents the other continuous variable, referred to as the dependent variable.\nData points are plotted on the scatter plot based on their respective x and y values. The position of each point on the plot is determined by these coordinates.\n\nIn our example, it is not necessarily clear which is the dependent or independent variable as we are not necessarily hypothesizing that on type of disaster causes the other. In this case, we are expecting years with more meteorological disasters to also have more climatological disasters as they have similar causes. Additionally, the individual data points can be used to fit a regression, i.e. determine the mathematical relationship between the variables.\nScatter plots are helpful fo the following reasons.\n\nexamining the relationship between two variables to determine whether there’s a positive, negative, or no correlation between the variables.\nrevealing patterns, clusters, or trends in the data, making it easier to identify structure within the data set.\nOutlier detection\ncomparing the distributions of two or more data sets and for assessing how they relate to each other.\nscatter plots can be extended to represent more than two variables by using color, size, or shape to encode additional information.\n\n\n\n\nBarplots count the values within a single categorical variable\nLet’s take a look at how common different disaster types were in the 2010s.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a barplot to compare the prevalence of different disaster types in this decade. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\nRemember that we have created barplots with the stat argument set to identity or to count. Depending on how you wrangle the data both are options in this case.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = disaster_subgroup)) +\n  geom_bar(stat = \"count\", color = \"black\", fill = \"darkorange\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\nA bar plot, also known as a bar chart or bar graph, is a common type of data visualization that represents data using rectangular bars or columns. Bar plots are used to display and compare data in a categorical or discrete manner, typically showing the relationship between different categories or groups and the corresponding values.\nThis is how the data is encoded:\n\nIn a vertical bar plot, the bars are drawn vertically, with their heights representing the values of the categories or groups. In a horizontal bar plot, the bars are drawn horizontally, and their lengths indicate the values.\nAlong the x-axis (for vertical bar plots) or y-axis (for horizontal bar plots), labels or categories are displayed to indicate what each bar represents. These labels can be names, descriptions, or any categorical information.\nThe height (or length in the case of horizontal bar plots) of each bar represents the value associated with the corresponding category or group.\nDifferent categories or groups are often represented with different colors or patterns to make the bars visually distinguishable.\n\nBar plots are useful for a range of purposes:\n\ncomparing the values of different categories or groups by assessing the relative size or magnitude of each category in relation to the others.\ndisplaying the frequency or count of data within discrete categories\ndisplaying ranked data\nhighlighting disparities or differences between groups.\n\n\n\n\nGrouped bar plots display counts of values broken down across categorical variables\nSo far, we have had a global view in terms of exploring trends in natural disasters. Let’s take a look at the 2010s comparing the number of natural disasters occuring on each continent.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a barplot to compare the prevalence of different disaster types in this decade for each continent. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = continent, fill = disaster_subgroup)) +\n  geom_bar(stat = \"count\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\nA grouped bar plot, also known as a grouped bar chart or clustered bar chart, is a type of data visualization that extends the standard bar plot by grouping bars into clusters to facilitate comparisons between multiple categories or subgroups. It’s particularly useful when you want to display and compare data for multiple categories across multiple subcategories.\nGrouped bar plots are used for various purposes:\n\ncomparing the values of different categories across multiple subcategories or groups. This is especially useful when you want to understand how each subcategory contributes to the overall category.\nWhen you have multi-dimensional data, a grouped bar plot makes it easier to display and compare data for each combination of categories and subcategories.\nYou can use grouped bar plots to highlight differences and trends between subcategories within the same category.\n\n\n\n\nAn alternative to the grouped bar chart is the stacked bar chart.\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = continent, fill = disaster_subgroup)) +\n  geom_bar(stat = \"count\", position = \"stack\", color = \"black\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast the grouped and stacked version of the same data and argue which one you think is a more appropriate visualization.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nStacked bar plots show counts/proportions of values broken down across categorical variables\nLet’s find a situation in which a stacked bar chart is helpful. In the previous plot, our question was more focused on the number of each disaster type occurring on each continent. Alternatively, our focus could be on understanding whether there are certain geographic regions that contribute more or less to the total number of events by disaster type.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a stacked barplot to compare the proportion of different disaster types in this decade occuring on each continent. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like:\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = disaster_subgroup, fill = continent)) +\n  geom_bar(stat = \"count\", position = \"fill\", color = \"black\") +\n  labs(title = \"Geographic patterns of natural disaster types.\",\n       subtitle = \"Proportion of disaster of a given type occuring on each continent as recorded in the \\nInternational Disaster Database for the 2010s.\",\n       x = \"natural disaster type\", y = \"proportion of occurenes\") +\n  theme_standard\n\n\n\n\n\nA stacked bar plot, also known as a stacked bar chart, is a type of data visualization that builds upon the traditional bar plot by stacking multiple data series on top of each other within each category. This format allows you to see how individual data components contribute to the total value for each category. This visualization makes the most sense to use when you are comparing relative contributions/abundance. Each category on the x-axis is broken into sections that show the percentage or proportion by a subcategory. This means that stacked bar plots effectively illustrate part-to-whole relationships, allowing viewers to see how individual components contribute to the total value for each category and for exploring data composition, especially when you want to assess how categories are composed of various subcategories."
  },
  {
    "objectID": "19_figure-types.html#fa-person-chalkboard-pointers-2",
    "href": "19_figure-types.html#fa-person-chalkboard-pointers-2",
    "title": "19  Know your plots",
    "section": "19.3  Pointers",
    "text": "19.3  Pointers\nHere is what your visualization could look like:\n\n\nCode\nggplot(metereol, aes(x = n)) +\n  geom_density(binwidth = 1, color = \"black\", fill = \"orange\", alpha = 0.5) +\n  labs(title = \"Yearly occurence of metereological-caused natural disasters\",\n       subtitle = \"Smoothed distribution of number of natural disasters classified as metereological by the \\nInternational Disaster Database per year 1900 - 2021\",\n       x = \"number of events\", y = \"number of years\") +\n  theme_standard\n\n\n\n\n\nA density plot is a graphical representation of the distribution of a continuous data set. It provides a smooth and continuous estimate of the probability density function of the data. Unlike a histogram, which divides the data into discrete bins and represents frequencies within those bins, a density plot creates a continuous curve that approximates the underlying distribution of the data.. They effectively highlight the distribution of a variable and help identify extreme values compared to those commonly occurring. Compared to histograms they are less sensitive to the number of bins chosen. However, by contrast you cannot easily determine the actual counts for a bin.\nBoxplots summarize a continuous, quantitative variable broken down by categorical variables\nLet’s take a look at the distribution of the number of disasters occurring for each natural disaster type. We could create individual histograms, e.g. by using facet_wrap() or facet_grid(), alternatively boxplots allow us to compare across categories.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize the distribution of number of natural disasters in each major category using a box plot. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nA box plot, also known as a box-and-whisker plot, is a graphical representation of the distribution of a dataset. It displays key summary statistics that help you understand the central tendency, spread, and the presence of outliers within the data. Box plots are particularly useful for comparing and visualizing the distribution of multiple datasets.\nThey can be a bit difficult to interpret, so here is a quick break down of all the components:\n\nBox: The box in the plot represents the interquartile range (IQR) of the data. The IQR is the middle 50% of the data, containing the values between the 25th percentile (Q1) and the 75th percentile (Q3). The top and bottom edges of the box represent Q3 and Q1, respectively, while the width of the box signifies the IQR.\nLine Inside the Box (Median): A line or bar inside the box represents the median (50th percentile) of the data. The median is the middle value when the data is ordered.\nWhiskers: The whiskers are lines extending from the box. They reach out to the minimum and maximum values within a defined range, typically defined as “1.5 times the IQR” from the quartiles. Values outside this range are considered outliers and are plotted individually as points or dots.\nOutliers, which fall beyond the whiskers’ reach, are displayed as individual data points or dots.\n\nBox plots are useful for several purposes:\n\nVisualizing Data Spread: They provide a visual representation of the range of data values and the spread of the middle 50% of the data.\nIdentifying Central Tendency: The median line within the box indicates the central tendency of the data.\nDetecting Outliers: Box plots readily identify potential outliers as points outside the whiskers, making them useful for outlier detection.\nComparing Distributions: Box plots are excellent for comparing the distributions of multiple datasets, helping you understand the differences and similarities between them.\nSummarizing Data: Box plots offer a concise summary of essential statistics, making it easy to grasp the overall distribution at a glance.\n\nAdding individual data points to a box plot can be a helpful technique to provide a more detailed view of the dataset and enhance the information conveyed by the plot. Box plots provide a summary of the central tendency, spread, and presence of outliers, while data points offer a granular view of the individual observations and also give you an idea of how large the data set is on which the data set was built. Data points can provide additional context or insight into the dataset, such as the presence of clusters, gaps, or patterns that might not be apparent from the box plot alone. In some cases, the density of data points at specific values is important information. Adding individual data points shows the concentration of data at particular values, which may not be evident from the box plot alone. By overlaying data points, you can visually validate that the box plot accurately represents the dataset. It allows you to see if there are any unusual patterns or if the data points are distributed as expected.\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  geom_point() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nUsing geom_points() isn’t super helpful, instead we want to use geom_jitter which is going to offset our points. Adding jittered data points to a box plot is a common technique to address the issue of overplotting, particularly when dealing with categorical or discrete data. Overplotting occurs when multiple data points share the same or very similar x-values, making it challenging to visualize the full extent of the data distribution accurately. Jittering helps spread these points out slightly to reveal the underlying data more clearly\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\nSimilar to using a density plot instead of a histogram, you can use a violin plot instead of a boxplot. A violin plot is a data visualization that combines aspects of a box plot and a kernel density plot to display the distribution and summary statistics of a dataset. It is particularly useful for visualizing the distribution of data across different categories or groups.\nHere’s how a violin plot is constructed:\n\nViolin Shape: The main component of a violin plot is a series of symmetrical or mirrored violin-shaped curves, one for each category or group being compared. These shapes represent the probability density of the data at different values along the x-axis. The width of the violin at any given point corresponds to the density of data points at that value.\nViolin Width: The width of the violins varies to show the density of data at different values. Wider sections indicate higher density, while narrower sections indicate lower density.\n\n\n\nCode\ndisaster %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year) %&gt;%\nggplot(aes(x = disaster_subgroup, y = n, color = disaster_subgroup)) +\n  geom_violin() +\n  labs(title = \"Distribution of number of disasters per category.\",\n       subtitle = \"Number of disasters in each category recorded in the International Disaster Database every year.\",\n       x = \"natural disaster type\", y = \"number of records\") +\n  theme_standard\n\n\n\n\n\n\n\n\nScatter plots visualize the relationship between two quantitative variables\nOur data set includes both natural disasters classified as meteorological and as climatological. We have discussed differences between climate and weather by describing weather as a snapshot compared to climate as a statistical portrait. But we also know that they are inherently linked. Let’s take a look how many events occur every year for both of those categories.\n\n\nCode\n# count number of metereologial/climatological disasters per year\ncounts &lt;- disaster %&gt;%\n  filter(disaster_subgroup %in% c(\"Meteorological\", \"Climatological\")) %&gt;%\n  group_by(disaster_subgroup) %&gt;%\n  count(year)\n\n# preview data\nhead(counts)\n\n\n# A tibble: 6 × 3\n# Groups:   disaster_subgroup [1]\n  disaster_subgroup  year     n\n  &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n1 Climatological     1900     2\n2 Climatological     1903     1\n3 Climatological     1906     1\n4 Climatological     1910     9\n5 Climatological     1911     1\n6 Climatological     1918     1\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nVisualize this data using a scatter plot - keep in mind that you will need to reformat the data to plot the counts for the different categories on the x and y axis, respectively. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your visualization could look like:\n\n\nCode\ncounts %&gt;%\n  mutate(n = replace_na(n, 0)) %&gt;%\n  pivot_wider(names_from = \"disaster_subgroup\", values_from = \"n\") %&gt;%\n  ggplot(aes(x = Climatological, y = Meteorological)) +\n  geom_smooth(method = \"lm\", color = \"darkblue\") +\n  geom_point(shape = 21, color = \"black\", fill = \"darkorange\", size = 3) +\n  labs(title = \"Comparison of yearly occurence of climatological and metereological disasters.\",\n       subtitle = \"Number of natural disasters classified as metereological by the International Disaster \\nDatabase per year 1900 - 2021. Blue line indicates linear regression with grey area \\nvisualizing the 95% confidence interval.\",\n       x = \"number of meteorological disasters\", y = \"number of climatological disasters\") +\n  theme_standard\n\n\n\n\n\nA scatter plot is a graphical data visualization tool used to display individual data points in a two-dimensional coordinate system. It is particularly useful for showing the relationship or correlation between two continuous variables.\nThis is how the data is encoded:\n\nEach data point represents an individual observation or data value. These points are typically displayed as dots, circles, or other symbols and are placed at specific coordinates on the plot. Each data point has an x-coordinate (horizontal) and a y-coordinate (vertical).\nThe horizontal axis represents one of the continuous variables being studied, often referred to as the independent variable. This variable is typically placed on the x-axis.\nThe vertical axis represents the other continuous variable, referred to as the dependent variable. This variable is typically placed on the y-axis.\nData points are plotted on the scatter plot based on their respective x and y values. The position of each point on the plot is determined by these coordinates.\n\nGenerally, the independent variable is plotted on the x-axis and the dependent variable on the y-axis. In our example, it is not necessarily clear which is the dependent or independent variable as we are not necessarily hypothesizing that on type of disaster causes the other. In this case, we are expecting years with more meteorological disasters to also have more climatological disasters as they have similar causes. Additionally, the individual data points can be used to fit a regression, i.e. determine the mathematical relationship between the variables.\nScatter plots a powerful way to visually inspect the relationship between two continuous variables and can help you make inferences about the data, and perform a correlation analysis.\n\nScatter plots are excellent for examining the relationship between two variables. They can help you determine whether there’s a positive, negative, or no correlation between the variables.\nScatter plots can reveal patterns, clusters, or trends in the data, making it easier to identify structure within the dataset.\nOutliers, which are data points that fall far away from the main cluster of points, can be readily identified on a scatter plot.\nScatter plots are useful for comparing the distributions of two or more data sets and for assessing how they relate to each other.\nscatter plots can be extended to represent more than two variables by using color, size, or shape to encode additional information.\n\n\n\n\nBarplots count the values within a single categorical variable\nLet’s take a look at how common different disaster types were in the 2010s.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a barplot to compare the prevalence of different disaster types in this decade. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\nRemember that we have created barplots with the stat argument set to identity or to count. Depending on how you wrangle the data both are options in this case.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = disaster_subgroup)) +\n  geom_bar(stat = \"count\", color = \"black\", fill = \"darkorange\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\nA bar plot, also known as a bar chart or bar graph, is a common type of data visualization that represents data using rectangular bars or columns. Bar plots are used to display and compare data in a categorical or discrete manner, typically showing the relationship between different categories or groups and the corresponding values.\nTHis is how the data is encoded:\n\nIn a vertical bar plot, the bars are drawn vertically, with their heights representing the values of the categories or groups. In a horizontal bar plot, the bars are drawn horizontally, and their lengths indicate the values.\nAlong the x-axis (for vertical bar plots) or y-axis (for horizontal bar plots), labels or categories are displayed to indicate what each bar represents. These labels can be names, descriptions, or any categorical information.\nThe height (or length in the case of horizontal bar plots) of each bar represents the value associated with the corresponding category or group.\nThe plot usually includes a title that describes the content of the graph. The axes are labeled with appropriate units or descriptions to indicate what each represents.\nDifferent categories or groups are often represented with different colors or patterns to make the bars visually distinguishable.\n\nBar plots are useful for a range of purposes:\n\nBar plots are excellent for comparing the values of different categories or groups. They allow viewers to assess the relative size or magnitude of each category in relation to the others.\nBar plots are commonly used to display the frequency or count of data within discrete categories. For example, you might use a bar plot to show the number of items sold in different months or the number of votes for political candidates.\nYou can use bar plots to display ranked data, such as the popularity of products, cities, or other entities, from most to least popular.\nBar plots can be used to highlight disparities or differences between groups. For example, you might use them to compare the performance of different departments in a company.\n\n\n\n\nGrouped bar plots display counts of values broken down across categorical variables\nSo far, we have had a global view in terms of exploring trends in natural disasters. Let’s take a look at the 2010s comparing the number of natural disasters occuring on each continent.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a barplot to compare the prevalence of different disaster types in this decade for each continent. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = continent, fill = disaster_subgroup)) +\n  geom_bar(stat = \"count\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\nA grouped bar plot, also known as a grouped bar chart or clustered bar chart, is a type of data visualization that extends the standard bar plot by grouping bars into clusters to facilitate comparisons between multiple categories or subgroups. It’s particularly useful when you want to display and compare data for multiple categories across multiple subcategories.\nGrouped bar plots are used for various purposes:\n\nGrouped bar plots allow you to compare the values of different categories across multiple subcategories or groups. This is especially useful when you want to understand how each subcategory contributes to the overall category.\nWhen you have multi-dimensional data, a grouped bar plot makes it easier to display and compare data for each combination of categories and subcategories.\nYou can use grouped bar plots to highlight differences and trends between subcategories within the same category.\n\n\n\n\nAn alternative to the grouped barchart is the stacked barchart.\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = continent, fill = disaster_subgroup)) +\n  geom_bar(stat = \"count\", position = \"stack\", color = \"black\") +\n  labs(title = \"Prevalence of natural disaster types.\",\n       subtitle = \"Prevalence measured as the number of times a disaster of a given type was recorded in the \\nInternational Disaster Database in the 2010s.\",\n       x = \"natural disaster type\", y = \"number of occurenes\") +\n  theme_standard\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast the grouped and stacked version of the same data and argue which one you think is a more appropriate visualization.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nStacked bar plots show counts/proportions of values broken down across categorical variables\nLet’s find a situation in which a stacked bar chart is helpful. In the previous plot, our question was more focused on the number of each disaster type occurring on each continent. Alternatively, our focus could be on understanding whether there are certain geographic regions that contribute more or less to the total number of events by disaster type.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset our disaster dataframe to contain only observations from 2010 to 2020. Visualize this data using a stacked barplot to compare the proportion of different disaster types in this decade occuring on each continent. Be sure to include a descriptive title and informative legend. Then describe how the data is encoded and argue why this is an appropriate geometry to visualize this data type.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what your figure could look like:\n\n\nCode\ndisaster %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2020) %&gt;%\n  ggplot(aes(x = disaster_subgroup, fill = continent)) +\n  geom_bar(stat = \"count\", position = \"fill\", color = \"black\") +\n  labs(title = \"Geographic patterns of natural disaster types.\",\n       subtitle = \"Proportion of disaster of a given type occuring on each continent as recorded in the \\nInternational Disaster Database for the 2010s.\",\n       x = \"natural disaster type\", y = \"proportion of occurenes\") +\n  theme_standard\n\n\n\n\n\nA stacked bar plot, also known as a stacked bar chart, is a type of data visualization that builds upon the traditional bar plot by stacking multiple data series on top of each other within each category. This format allows you to see how individual data components contribute to the total value for each category. This visualization makes the most sense to use when you are comparing relative contributions/abundance. Each category on the x-axis is broken into sections that show the percentage or proportion by a subcategory. This means that stacked bar plots effectively illustrate part-to-whole relationships, allowing viewers to see how individual components contribute to the total value for each category and for exploring data composition, especially when you want to assess how categories are composed of various subcategories."
  },
  {
    "objectID": "23_phenology.html#section",
    "href": "23_phenology.html#section",
    "title": "23  Phenology",
    "section": "23.1 ",
    "text": "23.1"
  },
  {
    "objectID": "24_phenology-modeling.html#section",
    "href": "24_phenology-modeling.html#section",
    "title": "24  Phenology: Predicting patterns",
    "section": "24.1 ",
    "text": "24.1"
  },
  {
    "objectID": "23_phenology.html#introduction-to-phenology",
    "href": "23_phenology.html#introduction-to-phenology",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.1 Introduction to phenology",
    "text": "23.1 Introduction to phenology\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly define the terms phenology, phenophases, life history and life history traits and argue how you think climate change might impact these.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe are going to explore a data set that contains information on the phenology of Minnesota Species to determine to which extent seasonal events are tied to climate or whether they are dependent on other factors. To do this we are going to use a data set generated by the Minnesota Phenology Network.\n\n\n\n\n\n\n Consider this\n\n\n\nGo to the Minnesota Phenological Network’s website and explore the short introductory and history statements in the “Home” and “About” sections. Check out the “Meet the species” page that presents Minnesota’s seven superstar species. Get to know them in the “Read more” sections.\nPick two of the seven superstars and list the specific phenophases that associated with the species.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "23_phenology.html#explore-the-data-set",
    "href": "23_phenology.html#explore-the-data-set",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.2 Explore the data set",
    "text": "23.2 Explore the data set\nWe are going to use data from the Minnesota Phenology Network to explore how climate change might be impacting the phenology of species.\n\npheno &lt;- read_delim(\"data/mnpn_master_dataset_2018.v2.txt\", delim = \"\\t\") %&gt;%\n  clean_names()\n\nLet’s get an overview of the data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your exploratory analysis skills to get an idea of the dimensions of the data set, what variables are contained in the data set and what data types they are. What function can you use?\nWrite a brief description of the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nThat’s right - skim() will give you all the information in one handy place!\n\nskim(pheno)\n\n\nTable 23.1: Data overview MN phenology network data set.\n\n\n\n\n(a) Data summary\n\n\nName\npheno\n\n\nNumber of rows\n54741\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nday\n0\n1.00\n1\n10\n0\n4329\n1\n\n\nevent\n1\n1.00\n4\n36\n0\n120\n0\n\n\nspecies_common_name\n136\n1.00\n3\n48\n0\n1827\n0\n\n\ngenus\n594\n0.99\n3\n15\n0\n748\n0\n\n\nspecies\n1566\n0.97\n2\n24\n0\n1089\n0\n\n\ncounty\n83\n1.00\n2\n17\n0\n43\n0\n\n\nlifeform\n1\n1.00\n6\n7\n0\n3\n0\n\n\ngroup\n136\n1.00\n4\n22\n0\n17\n0\n\n\ninvasive\n53339\n0.03\n3\n3\n0\n1\n0\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1997.56\n16.53\n1941\n1986\n2004\n2011\n2017\n▁▂▂▂▇\n\n\ndataset\n0\n1\n6.56\n4.46\n1\n2\n7\n12\n13\n▇▃▅▁▇\n\n\nday_of_year\n84\n1\n173.95\n58.82\n2\n130\n164\n211\n366\n▁▇▇▃▁\n\n\n\n\n\n\n\nYou do still have to write your description though …\n\n\n\nOne thing you may have noticed is that we have a column day that contains the date - however, currently it is formatted as a character. This could cause issues down the line when we want to plot things.\nDates are notoriously difficult to deal with2. A useful package to deal with dates is lubridate.2 Remember when we listed out the many, many, many ways we can write a date and how conventions might differ between fields and countries?\nLet’s use functions from that package to create column called date that has the data type date, and while we are at it, we can also learn how to create new columns with the month and day.\n\npheno &lt;- pheno %&gt;%\n  mutate(date = mdy(day),            # converts character in format month day year to date\n         month = month(date),        # extract month\n         day = day(date))            # extract day\n\n\n# check class\nclass(pheno$date)\n\n[1] \"Date\"\n\n\nNow let’s get an idea on what data is contained in the data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDescribe how you can use the skimr output to determine how many unique entries (categories) we have for our categorical variables. Then use a function to print life forms, groups of species, and counties are contained in the data set to the console/your report.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nThe dplyr verb that can help you out here is distinct() or you could apply unique() to the column itself, which is a vector.\n\n# output unique entries as dataframe\npheno %&gt;%\n  distinct(lifeform)\n\n# A tibble: 4 × 1\n  lifeform\n  &lt;chr&gt;   \n1 PLANTS  \n2 ANIMALS \n3 &lt;NA&gt;    \n4 ABIOTIC \n\n# output unique entries as vector\nunique(pheno$lifeform)\n\n[1] \"PLANTS\"  \"ANIMALS\" NA        \"ABIOTIC\"\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a table that shows the number of species in each group of species.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nLet’s focus on woody plants for now.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a new object called woody that contains only woody plants.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDetermine what events are recorded for species in this category. Create an overview table with events organized alphabetically.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThat’s a lot of events. Let’s try to get an idea of when these different events occur throughout the year.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a figure that effectively summarizes when different events typically occur throughout the year.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince we are interested in distributions, you would want to look at histograms or box plots - this is one way your figure could look like:\n\n\n\n\n\nFigure 23.1: Distribution of life history events throughout the year for Minnesota woody plants."
  },
  {
    "objectID": "23_phenology.html#formulate-a-specific-question",
    "href": "23_phenology.html#formulate-a-specific-question",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.3 Formulate a specific question",
    "text": "23.3 Formulate a specific question\nSince we are interested in whether climate change is impacting phenology, let’s chose a specific event that we think is likely to be linked to changes in temperature and could be a good indicator of changing phenology.\n\n\n\n\n\n\n Consider this\n\n\n\nPick three events that you think could be good indicator events to look at and argue why you think they would be interesting to explore.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nLet’s start with the flowering date.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a subset called flowering that contains only records of flowering dates for woody plants. How could you plot this data to determine whether the flowering data has changed over time?\nDescribe how you expect the pattern to look like if the flowering date is occurring earlier, later, or not changing over time.\nPlot the data and then use your predicted patterns to assess whether the flowering date is changing over time.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nThis is what you want your figure to look like:\n\n\n\n\n\nFigure 23.2: Change in day of flowering for woody plants in Minnesota 1941 - 2018. Fill of individual points indicates the month of the flowering date; linear trendline is fitted in red.\n\n\n\n\n\n\n\nLet’s consider how useful this visualization is for answering our question. We combined all the woody species from all of Minnesota in the same plot. We probably don’t expect all the woody plants to react to changes in the same way or for that response to occur at the same pace. We should also consider that there could be differences based on the geographic location.\nIn this case, it might be more effective to narrow our question and pull out a specific species. Let’s use the American Elm (Ulmus americanus). This is a deciduous species with a geographic range throughout most of the eastern US and southeast Canada, i.e. Minnesota is at the northern limit of its range. Overall, this is a pretty hardy species that will grow to a considerable size and is frequently found in Urban settings. Like all elm species in Minnesota it is susceptible to the Dutch elm disease which caused by an invasive fungal pathogen.\n\n\n\n\n\n\n Consider this\n\n\n\nCreate a new object called elm that contains only entries for the American Elm.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\nSince the American Elm is so widespread, let’s also consider that we might want to eliminate geography as a confounding pattern.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDetermine whether we have data for more than one county. Use that information to determine whether you want (need) to narrow down your data set. Explain your choice.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nIt appears we have found our question!\n\n\n\n\n\n\n Consider this\n\n\n\nState the specific question we are asking and give a brief description of the data you will need and how you will analyze it to answer that question.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nJust so we are all in agreement - our question is: Has climate change changed the phenology of American Elm in Ramsey County, Minnesota?\nDo we already have all the data we need to answer that question?"
  },
  {
    "objectID": "23_phenology.html#determine-changes-in-the-flowering-date-of-american-elm-in-ramsey-county-mn",
    "href": "23_phenology.html#determine-changes-in-the-flowering-date-of-american-elm-in-ramsey-county-mn",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.4 Determine changes in the flowering date of American elm in Ramsey County, MN",
    "text": "23.4 Determine changes in the flowering date of American elm in Ramsey County, MN\nOur first step will be determining whether or not there has been a change in the flowering data of the American Elm.\n\n\n\n\n\n\n Consider this\n\n\n\nMake a prediction of how you think the flowering date of the American Elm may have changed over the 50 year time span recorded in our data set. Describe what your figure should look like if you are indeed correct, be specific and argue why you are expecting this pattern.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFilter your data so that it only contains entries for Ramsey County.\nPlot your data to determine if your prediction was correct; add a simple linear trend line to your plot to help identify the overall pattern.\nMake sure to give your visualization a title and add a legend3.\n\n\n3 You can do this using the chunk options (fig-cap) or directly on the figure itself\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nThis is what your figure should look like.\n\nelm &lt;- elm %&gt;%\n  filter(county == \"RAMSEY\")\n\nggplot(elm, aes(x = year, y = day_of_year, fill = month)) +\n  geom_point(shape = 21, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  scale_fill_viridis_c() +\n  labs(x = \"year\", y = \"flowering date\") +\n  theme_standard +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDetermine the rate of change (remember to include units!).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHint: You will need to determine the equation of your linear trend line to do this.\n\n\n\nCall:\nlm(formula = day_of_year ~ year, data = elm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.7698  -5.7744  -0.6021   8.5442  19.2974 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 258.27107  146.61188   1.762   0.0836 .\nyear         -0.07927    0.07439  -1.066   0.2912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.81 on 56 degrees of freedom\nMultiple R-squared:  0.01987,   Adjusted R-squared:  0.002371 \nF-statistic: 1.135 on 1 and 56 DF,  p-value: 0.2912\n\n\nDon’t get too hung up on whether the result is significant or not. The rate is still the rate and you will want to include that in your results.\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe your overall results4.\nFor your interpretation/discussion identify at least two possible mechanisms that could be causing this pattern. You will likely notice that you have a low R2 value and the linear regression is not significant. What does that mean in this context? How can you apply that to generating possible mechanisms for the pattern?.\n\n\n4 Remember, results are more than just figures! Practice being specific, i.e. consider how much the flowering data has shifted and at what rate\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "23_phenology.html#determine-temperature-change-in-ramsey-county",
    "href": "23_phenology.html#determine-temperature-change-in-ramsey-county",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.5 Determine temperature change in Ramsey County",
    "text": "23.5 Determine temperature change in Ramsey County\nIf we want to investigate whether there is a relationship between climate change and change in the flowering date, we need climate data for Ramsey Country for the same time period.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe what your climate data set should look like and argue why you want to include certain variables. Describe the pattern you would expect to see in your climate data over time to support your hypothesis of what is driving the pattern you uncovered in the phenology data set. Be specific.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nConsider what the ideal temporal and spatial resolution would need to be to “match” your phenology data.\n\n\n\nYou ask and you shall receive:\nGo to the DNR Minnesota Climate Trends website to access historical observations for specific Minnesota locations. Use the drop down menus options to select subsets using the check boxes to create a data set that meets the specifications you set out above.\nUnfortunately, the download button doesn’t work. Instead, you should cut and paste the data into a text editor or excel and save it as a tab-delimited file in your data folder. Name it MN_temp.txt.\nLets read in your temperature data. Unless you added headers in the text file it’s missing them. Similar to the way we can use skip if there are additional lines, we have an argument we can use to specify column names if they are missing.\n\ntemp &lt;- read_delim(\"data/MN_temp.txt\", delim = \"\\t\", col_names = c(\"year\", \"temperature\"))\n\n\n\n\n\n\n\n Consider this\n\n\n\nPlot your data to determine if your prediction was correct; add a simple linear trend line to your plot to help identify the overall pattern.\nGive your visualization a title and legend and describe your results. Be specific (i.e. don’t just state the overall trend, consider how much temperature has changed). For your interpretation/discussion consider how the data differs from your predictions and determine whether it supports your hypothesis of the mechanism.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what your figure could look like:\n\n\n\n\n\nFigure 23.3: Change in mean March temperature [Fahrenheit] for 1941 - 1991 (blue) and linear trendline (red).\n\n\n\n\nI used a linegraph, which is an option for visualizing change over time as we are doing here in this time series. Using a line graph instead of a scatterplot is appropriate when plotting a categorical data on the x axis, similarly as we have discussed a line plot is an appropriate visualization of a time series.\nWe can add both a trendline and a line connecting individual points. This can be helpful because the linear trendline shows us the overall trend of the data while the line connecting the individual points helps us visualize local change from point to point.\nFor categorical data we can also use barplots:\n\n\n\n\n\nFigure 23.4: Change in mean March temperature [Fahrenheit] for 1941 - 1991 (orange bars).\n\n\n\n\nWhy do you think it is better to use a scatter plot or line plot compared to the barplot?\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFor a good presentation of your results, e.g. through a presentation or poster, you would likely want to produce a figure where you have change in flowering date and change in mean temperature for March side by side. Practice making that type of figure here:\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "23_phenology.html#analyze-relationship-of-flowering-date-and-temperature",
    "href": "23_phenology.html#analyze-relationship-of-flowering-date-and-temperature",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.6 Analyze relationship of flowering date and temperature",
    "text": "23.6 Analyze relationship of flowering date and temperature\n\n\n\n\n\n\n Give it a whirl\n\n\n\nEspecially when we look at the two figures side by side the fact we see that there is a trend for earlier flowering dates for American elm and warmer March temperatures. Describe the analysis you want to perform to determine whether there is a significant relationship between the mean temperature in March and the flowering date for the American Elm in Ramsey Co.. Include what your dependent and independent variables are5.\nCreate a plot to visualize the relationship but hold off on the analysis component until the next step.\n\n\n5 Your are practicing writing up your methods. A good methods section includes not just what you want to do but why you are doing it/what that analysis is meant to achieve. A good way to start for this specific example would be e.g. “To determine whether {variable} depends on {variable}, we will {analysis}\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nTo be able to plot temperature vs flowering date those two variables will need to be in the same data set. You can combine them using left_join().\nHere’s what your figure could look like:\n\n\n\n\n\nFigure 23.5: Relationship of mean temperature in March and flowering date for American Elm in Ramsey Co., MN.\n\n\n\n\n\n\n\nYou’ve already learned how to fit linear regressions using the function lm(). As you can imagine, there is a different function for various models (and occasionally multiple functions for the same type of models) and each comes with a slightly different syntax. It also might be more helpful to have the output e.g. in a data.frame or tibble.\nThere is a group of packages that has been designed to make interacting with models more consistent and processing the output in a more user-friendly way. You can install and load them as a group by calling tidymodels. From the name you have probably guessed that these packages have been designed to play well with the tidyverse.\nHere’s how you can run a linear regression using parsnip which is the package designed to fit models using a tidy, unified interface. Essentially, in provides an interface to use different models behind the scene. This means that you can use a consistent syntax instead of having to figure it out for each model. You will see that having already used lm() without the interface that you are already familiar with the formula syntax that is used here.\nThe first step will be loading the tidymodels packages and specifying the model we want to use, in this case that would be a linear regression model.\n\nlinear_reg()                                      # specify model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThat’s not too exciting, because after we specify the type of model we need to tell R which engine (method) it should use to fit the model. For example, to use an ordinary least square regression, we would set the method to lm6.6 You could use the documentation page for linear_reg() to list all the possible engines.\n\nlinear_reg() %&gt;%                                  # specify model\n  set_engine(\"lm\")                                # Define computational engine\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nIt appears we are getting somewhere, the last things we still need to do is use the function fit() to tell it what variables and we want to use to train/fit the model. We use the same formula syntax you are familiar with from using lm(). We will also use the function tidy() to convert the output into a handy tibble/data.frame.\n\nlinear_reg() %&gt;%                                  # specify model\n  set_engine(\"lm\") %&gt;%                            # Define computational engine\n  fit(day_of_year ~ temperature, data = elm) %&gt;%  # define variables\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   148.       4.60       32.1 1.51e-34\n2 temperature    -1.57     0.155     -10.1 1.35e-13\n\n\nThis doesn’t look too different from the output you are used to seeing but we will see down the line that there are additional benefits to using the tidymodels framework when it comes to doing more sophisticated analysis.\nWe’ve previously used regressions, mainly because were interested in the slope to be able to calculate a rate of increase or decrease over time. Now we are interested in a relationship between two continuous variables and whether or not one of the (the independent value, in our case temperature) has significant explanatory power for the dependent variable (in this case the flowering date). So let’s think about how we should interpret the results of our regression.\nThe (Intercept) term (slope) tells us that for temperature = 0 Fahrenheit, the expected flowering date would be 148. The temperature term tells us that on average for every one degree Fahrenheit decrease in Temperature, we would expect the flowering date to occur 1.57 days earlier.\n\n\n\n\n\n\n Consider this\n\n\n\nAfter first thinking about the purely mathematical explanation, we would want to think about what is biologically meaningful/sensible. Is there part of our interpretation above that biologically doesn’t entirely make sense? What does this tell you about extrapolating beyond the reach of your data?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss your results, make a conclusion about the relationship between American elm flowering data and mean March temperature and then discuss that in the context of our original question.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nEspecially if you are writing a paper or report in the style of “Introduction/Methods/Results/Discussion”, a good discussion starts by reiterating the original (broad) question you asked with short description of how you specifically investigated it, your key results, and then interpreting/discussing those.\nYou can essentially follow a fill-in-the-blank-formula, which as you become more comfortable communicating your research will sound a little less formulaic and a little bit more you; adapt the following for your answer:\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. We found that [key results: in your case you would make a specific statement about the trend of earlier flowering dates, the trend of warmer March temperatures, and the relationship of temperature and flowering date].\n\nAnd then you would follow this with a brief discussion of whether your results and how they apply to your initial question, i.e. we are asking whether climate change will impact the phenology of plants - how does this specific example apply to that question?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "23_phenology.html#now-you",
    "href": "23_phenology.html#now-you",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.7 Now you!",
    "text": "23.7 Now you!\nWe asked a pretty general question to start us off with and used a specific species and a specific phenophase to investigate. To gather further evidence or determine if there are other patterns to observe we would want to investigate the phenology of further organisms.\nGood thing we have a very large data set to work with!\n\n\n\n\n\n\n Consider this\n\n\n\nGo back to the original data set with all the phenology records (object pheno). To find a good species to investigate we will want to make sure that we have sufficient data to make a meaningful statement about changes in the timing of phenophases.\nUse your data wrangling skills to identify all species in the data set with at least 30 years of data for a specific phenophase and locations (i.e. you want at least 20 entries over at least 20 years).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nYou need to generate a table that looks like this - bonus, organize your table group of species, species, and by years of data available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nspecies_common_name\nevent\ncounty\nmin_year\nmax_year\ntime_observed\nn_observations\n\n\n\n\nAMPHIBIANS & REPTILES\nSPRING PEEPER\nFIRST HEARD\nITASCA\n1984\n2016\n32\n35\n\n\nAMPHIBIANS & REPTILES\nWOOD FROG\nFIRST HEARD\nITASCA\n1984\n2016\n32\n40\n\n\nBIRDS\nAMERICAN ROBIN\nARRIVAL\nSHERBURNE\n1975\n2011\n36\n33\n\n\nBIRDS\nAMERICAN ROBIN\nFIRST FLOCK OF MIGRATORS\nITASCA\n1984\n2016\n32\n36\n\n\nBIRDS\nAMERICAN ROBIN\nLAST SEEN\nITASCA\n1984\n2014\n30\n56\n\n\nBIRDS\nBLUE WINGED TEAL\nARRIVAL\nSHERBURNE\n1980\n2013\n33\n34\n\n\nBIRDS\nBUFFLEHEAD\nARRIVAL\nSHERBURNE\n1982\n2012\n30\n30\n\n\nBIRDS\nCANADA GOOSE\nARRIVAL\nSHERBURNE\n1975\n2011\n36\n31\n\n\nBIRDS\nCOMMON GOLDENEYE\nARRIVAL\nSHERBURNE\n1982\n2012\n30\n30\n\n\nBIRDS\nCOMMON MERGANSER\nARRIVAL\nSHERBURNE\n1979\n2012\n33\n33\n\n\nBIRDS\nDARK EYED JUNCO\nFIRST SEEN\nITASCA\n1984\n2016\n32\n30\n\n\nBIRDS\nEASTERN BLUEBIRD\nARRIVAL\nSHERBURNE\n1975\n2013\n38\n30\n\n\nBIRDS\nGREAT BLUE HERON\nARRIVAL\nSHERBURNE\n1975\n2013\n38\n33\n\n\nBIRDS\nGREEN WINGED TEAL\nARRIVAL\nSHERBURNE\n1979\n2013\n34\n32\n\n\nBIRDS\nHOODED MERGANSER\nARRIVAL\nSHERBURNE\n1982\n2013\n31\n31\n\n\nBIRDS\nKILLDEER\nARRIVAL\nSHERBURNE\n1975\n2013\n38\n34\n\n\nBIRDS\nLESSER SCAUP\nARRIVAL\nSHERBURNE\n1982\n2012\n30\n30\n\n\nBIRDS\nMALLARD\nARRIVAL\nSHERBURNE\n1975\n2011\n36\n32\n\n\nBIRDS\nNORTHERN HARRIER\nARRIVAL\nSHERBURNE\n1979\n2012\n33\n30\n\n\nBIRDS\nNORTHERN PINTAIL\nARRIVAL\nSHERBURNE\n1979\n2013\n34\n33\n\n\nBIRDS\nNORTHERN SHOVELER\nARRIVAL\nSHERBURNE\n1979\n2012\n33\n33\n\n\nBIRDS\nPIED BILLED GREBE\nARRIVAL\nSHERBURNE\n1982\n2013\n31\n30\n\n\nBIRDS\nRED WINGED BLACKBIRD\nARRIVAL\nSHERBURNE\n1977\n2013\n36\n36\n\n\nBIRDS\nRING NECKED DUCK\nARRIVAL\nSHERBURNE\n1981\n2012\n31\n32\n\n\nBIRDS\nRUFFED GROUSE\nFIRST COURTSHIP/TERRITORIAL BEHAVIOR\nITASCA\n1985\n2016\n31\n48\n\n\nBIRDS\nSANDHILL CRANE\nARRIVAL\nSHERBURNE\n1980\n2013\n33\n33\n\n\nBIRDS\nWOOD DUCK\nARRIVAL\nSHERBURNE\n1979\n2013\n34\n35\n\n\nFORB\nBELLWORT\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n32\n\n\nFORB\nBLOODROOT\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n32\n\n\nFORB\nBLOODROOT\nLAST FLOWER\nHENNEPIN\n1957\n1991\n34\n33\n\n\nFORB\nBRIDAL WREATH\nFLOWERING\nRAMSEY\n1941\n1991\n50\n48\n\n\nFORB\nCANADA VIOLET\nFLOWERING\nHENNEPIN\n1959\n1991\n32\n30\n\n\nFORB\nCAROLINA PUCCOON\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n31\n\n\nFORB\nCOMMON MILKWEED\nFLOWERING\nITASCA\n1984\n2016\n32\n31\n\n\nFORB\nCOMMON SAINT JOHN’S WORT\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n40\n\n\nFORB\nCROWFOOT\nFLOWERING\nHENNEPIN\n1961\n1991\n30\n30\n\n\nFORB\nCUT-LEAVED TOOTHWORT\nFLOWERING\nHENNEPIN\n1960\n2001\n41\n38\n\n\nFORB\nDANDELION\nFLOWERING\nITASCA\n1985\n2016\n31\n33\n\n\nFORB\nFALSE RUE ANEMONE\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nFORB\nFALSE SOLOMON’S SEAL\nFLOWERING\nHENNEPIN\n1960\n1990\n30\n30\n\n\nFORB\nFIREWEED\nFLOWERING\nITASCA\n1984\n2016\n32\n31\n\n\nFORB\nGOLDEN RAGWORT\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n37\n\n\nFORB\nLARGE-FLOWERED TRILLIUM\nFLOWERING\nHENNEPIN\n1958\n1991\n33\n32\n\n\nFORB\nLARGE-FLOWERED TRILLIUM\nLAST FLOWER\nHENNEPIN\n1958\n1991\n33\n32\n\n\nFORB\nMARSH MARIGOLD\nFLOWERING\nHENNEPIN\n1959\n1991\n32\n31\n\n\nFORB\nMINNESOTA TROUT-LILY\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nFORB\nPURPLE TRILLIUM\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n33\n\n\nFORB\nPURPLE TRILLIUM\nLAST FLOWER\nHENNEPIN\n1957\n1991\n34\n30\n\n\nFORB\nRUE ANEMONE\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n33\n\n\nFORB\nRUE ANEMONE\nLAST FLOWER\nHENNEPIN\n1957\n1991\n34\n30\n\n\nFORB\nSHARP-LOBED HEPATICA\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n33\n\n\nFORB\nSHARP-LOBED HEPATICA\nLAST FLOWER\nHENNEPIN\n1958\n1991\n33\n32\n\n\nFORB\nSHOOTING STAR\nFLOWERING\nHENNEPIN\n1959\n1990\n31\n31\n\n\nFORB\nSHOWY LADY’S SLIPPER\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nFORB\nSKUNK CABBAGE\nFLOWERING\nHENNEPIN\n1957\n1992\n35\n34\n\n\nFORB\nSKUNK CABBAGE\nLAST FLOWER\nHENNEPIN\n1959\n1991\n32\n31\n\n\nFORB\nSNOW TRILLIUM\nFLOWERING\nHENNEPIN\n1957\n1991\n34\n34\n\n\nFORB\nSNOW TRILLIUM\nLAST FLOWER\nHENNEPIN\n1959\n1991\n32\n31\n\n\nFORB\nSPREADING DOGBANE\nFIRST FALL COLOR\nITASCA\n1985\n2016\n31\n31\n\n\nFORB\nSTARFLOWER\nFLOWERING\nITASCA\n1984\n2016\n32\n36\n\n\nFORB\nSWAMP BUTTERCUP\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n31\n\n\nFORB\nSWAMP MILKWEED\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nFORB\nWHITE TROUT-LILY\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nFORB\nYELLOW TROUT-LILY\nFLOWERING\nHENNEPIN\n1960\n1991\n31\n30\n\n\nMAMMALS\nWHITE TAILED DEER\nFIRST ANTLERS\nITASCA\n1984\n2016\n32\n72\n\n\nWOODY\nAMERICAN ELM\nFLOWERING\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nAMERICAN ELM\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nAMERICAN TAMARACK\nALL LEAVES COLORED (GROUP)\nITASCA\n1984\n2016\n32\n34\n\n\nWOODY\nAMERICAN TAMARACK\nLEAF BUDBREAK\nITASCA\n1984\n2016\n32\n45\n\n\nWOODY\nAPPLE\nFLOWERING\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nAPPLE\nLAST FLOWER\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nAPPLE\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nBEAKED HAZELNUT\nFIRST POLLEN VISIBLE\nITASCA\n1984\n2016\n32\n39\n\n\nWOODY\nBIG TOOTHED ASPEN\nLEAF BUDBREAK\nITASCA\n1985\n2016\n31\n36\n\n\nWOODY\nBUR OAK\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nCOMMON LILAC\nFLOWERING\nITASCA\n1985\n2016\n31\n31\n\n\nWOODY\nLILAC\nFLOWERING\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nLILAC\nFULL FLOWERING\nRAMSEY\n1941\n1991\n50\n50\n\n\nWOODY\nLILAC\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nPIN CHERRY [FIRE C, BIRD C]\nFLOWERING\nITASCA\n1984\n2015\n31\n30\n\n\nWOODY\nPUSSY WILLOW\nFLOWERING\nITASCA\n1984\n2016\n32\n70\n\n\nWOODY\nQUAKING ASPEN\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n50\n\n\nWOODY\nRED ELDERBERRY\nFLOWERING\nRAMSEY\n1941\n1991\n50\n50\n\n\nWOODY\nRED ELDERBERRY\nLEAF BUDBREAK\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nRED MAPLE\nCOLORED LEAVES\nANOKA\n1984\n2017\n33\n66\n\n\nWOODY\nSILVER MAPLE\nFLOWERING\nRAMSEY\n1941\n1991\n50\n51\n\n\nWOODY\nSPECKLED ALDER [HOARY A]\nFLOWERING\nITASCA\n1985\n2016\n31\n35\n\n\nWOODY\nTAMARACK\nCOLORED NEEDLES\nANOKA\n1984\n2017\n33\n39\n\n\nWOODY\nTREMBLING ASPEN\nLEAF BUDBREAK\nITASCA\n1984\n2016\n32\n49\n\n\n\nTable 23.2: Minnesota species with at least 30 years of data for a specific phenophase in the Minnesota Phenology database.\n\n\n\n\n\nWe have already looked at a tree - let’s see if we can broaden the scope of the organisms we investigate and chose examples from different groups of species. For efficiency, we’ll divvy up the work and have everyone chose a different species.\n\n\n\n\n\n\n Consider this\n\n\n\nCall dibs on the species you would like to investigate. Do a tiny-google and write a 3-5 sentences description of your species. List the phenological questions you could pose about the species you have chosen based on your data set7.\n\n\n7 If you were writing a report/paper this would be part of your introduction/background section.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe are going to practice putting together all the components of a data science-esque analysis.\n\n\n\nData Science Process (H.Wickham & Grolemund: R for Data Science\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChose the phenophase you want to investigate and go through the process of “Transform-Visualize-Analyze/Model” + “Communicate”.\n\nTransform: Create a subset of your data set that contains only the data for the species, phenophase, and location you have chosen.\nVisualize: Plot the change in the timing of the phenophase you have chosen over time.\nModel/Analyze: Calculate the rate of change using a linear regression.\nImport/Tidy/Transform: Determine what the appropriate temperature data is to match your phenophase data8, the download, and import it.\nVisualize: Plot the change in temperature over time.\nModel/Analyze: Calculate the reate of change using a linear regression.\nTransform: Combine the phenophase and temperature data.\nVisualize: Plot the relationship of temperature & phenophase\nModel/Analyze: Determine if the relationship of temperature/phenophase is significant.\n\n8 Remember this will include both matching the geographic location & what time of year the temperature should be from, and time span to compare match the years in your data setDescribe what you are doing as you go, i.e. describe your methods9.\nCreate a final multi-panel figure of your three visualizations and share it with your classmates in our slack channel along with your results (be specific.)\n\n\n9 Pro Tip: Use the instructions as your starting point, for some of them you would want to add a detail or two specific to your analysis.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCollect the results from all the species/phenophases we have analyzed, this includes the American Elm, your species and those of your classmates and discuss the overall results.\n\nrestate the overall broad question/central hypothesis\nsummarize in 2-3 sentences what data set you used (include all the species/phenophase analysis we’ve done as a class) and how your analyzed it.\nsummarize the results - which (if any) phenophase now occur earlier/later/have not changed? Which (if any) phenophases are significantly correlated with temperature? How are you reaching these conclusions?\ndiscuss your results - what mechanisms are consistent with the patterns your have observed? Make sure to connect your final conclusion(s) back to your initial question/hypothesis.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nA final question we should consider is what the potential impacts of changing phenologies could be. Consider that species within a biological community might be differentially impacted by climate change and that some might be directly impacted by climate change resulting in an altered phenology, while other might be indirectly impacted because of an altered phenology for a species they closely interact with (this is called a population asynchrony or phenological mismatch).\nDescribe how you could set up an analysis to to test whether one of the species we analyzed might experience a phenological mismatch. Your description should include what how what data you would need and how you would design your analysis.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "23_phenology.html#acknowledgments",
    "href": "23_phenology.html#acknowledgments",
    "title": "23  Phenology: Exploring patterns",
    "section": "23.8 Acknowledgments",
    "text": "23.8 Acknowledgments\nThese activities are based on the EDDIE Phenology Trends and Climate Change in Minnesota module.1010 Freeman, P. (2021). Phenology Trends and Climate Change in Minnesota (Project EDDIE)."
  },
  {
    "objectID": "24_phenology-modeling.html#model-checking",
    "href": "24_phenology-modeling.html#model-checking",
    "title": "24  Phenology: Inferring and Predicting patterns",
    "section": "24.1 Model checking",
    "text": "24.1 Model checking\nOur analysis of Flowering date in the American Elm shows that mean temperature in March explains a large proportion of variance observed in the flowering date, i.e. we have an expectation of how much earlier the flowering date will occur over every one degree change in temperature based on the linear regression we performed.\nThe obvious next question is, can we use that information to predict the flowering date if we have information on the temperatures in March?\nGenerally, the point of fitting models is that ideally we can model the relationship and then use that model to infer predict values of our independent variable (temperature) based on our dependent value (flowering date). This means we are no moving beyond summarizing our data (descriptive analysis), interpreting those summaries to identify relationships in the data set at hand (exploratory analysis), to trying to quantify whether these relationships will hold for a new sample and being able to predict/infer observations for individuals or populations.\nWelcome to the big leagues.\nLet’s go ahead and read phenology data back in and extract the American Elm data for Ramsey Co. and then join that to our temperature data.\n\n# mean march temperature\ntemp &lt;- read_delim(\"data/MN_temp.txt\", delim = \"\\t\", col_names = c(\"year\", \"temperature\"))\n\n# flowering date w/temp\npheno &lt;- read_delim(\"data/mnpn_master_dataset_2018.v2.txt\", delim = \"\\t\") %&gt;%\n  clean_names() %&gt;%\n  filter(species_common_name == \"AMERICAN ELM\" & event == \"FLOWERING\" & county == \"RAMSEY\") %&gt;%\n  select(year, day_of_year) %&gt;%\n  left_join(temp)\n\nNow, let’s create a linear regression modeling the relationship of the flowering date and temperature for the American Elm. This time we will assign it to an object so we can perform some additional steps.\n\nlm_fit &lt;- linear_reg() %&gt;%                        # specify model\n  set_engine(\"lm\") %&gt;%                            # Define computational engine\n  fit(day_of_year ~ temperature, data = pheno)    # define variables\n\nChoosing lm as our computational engine, means that we are performing an ordinary least square regression. Let’s quickly review what that means for how we are fitting our model.\n\n\n\n\n\nFigure 24.1: Observed (orange) and corresponding predicted (blue) flowering dates using linear regression model (black line) for American Elm in Ramsey Country.\n\n\n\n\nIn this figure you see the observed flowering dates and the corresponding predicted value for each temperature in the data set using the linear regression model. The vertical segments connecting those two points are the residuals, i.e. the distance between the observed and predicted value. When we fit the model, we (and by we I mean R) took the residuals, squared them, summed them all up and then tried to minimize that number - i.e. we tried out all possible configurations of the linear trendline until we found the one that minimizes the sum of the squared regressions.\nHow do we know how “good” or model is? Well, we can look at the R2 and p-value which tells us how much of the variance in the dependent variable is explained by the independent variable and whether or not that relationship is significant but we should also check our model to determine if a linear regression is an appropriate way to describe the relationship.\nOne way we can do this is using a diagnostic plot, called a residuals plot which can be used to determine if there is a relationship between predicted values and residuals.\nBefore we can construct this plot we need to calculate our residuals. We can do this using the function augment()\n\nlm_aug &lt;- augment(lm_fit$fit)\n\nhead(lm_aug) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday_of_year\ntemperature\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n101\n26.60\n105.70702\n-4.7070184\n0.0237040\n6.110531\n0.0074379\n-0.7827467\n\n\n93\n34.92\n92.63110\n0.3688984\n0.0415946\n6.148856\n0.0000832\n0.0619153\n\n\n99\n22.67\n111.88350\n-12.8835031\n0.0465936\n5.846741\n0.1148524\n-2.1680089\n\n\n118\n25.19\n107.92301\n10.0769910\n0.0296032\n5.969197\n0.0430927\n1.6808243\n\n\n84\n38.53\n86.95754\n-2.9575368\n0.0773935\n6.133015\n0.0107358\n-0.5059272\n\n\n83\n40.97\n83.12277\n-0.1227727\n0.1112000\n6.149068\n0.0000286\n-0.0213977\n\n\n\nTable 24.1: Fitted (predicted) values and residuals for flowering date based on observed temperature.\n\n\nWhat augmenting does is for every point in our model calculate the fitted value (.fitted) and the residuals (.resid) among other diagnostic variables.\nFrom there we can create the residuals plot.\n\nggplot(lm_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(shape = 21, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(x = \"predicted value\", y = \"residual\")\n\n\n\n\nFigure 24.2: Residuals for each predicted value.\n\n\n\n\nWhen we plot the residuals vs the predicted values we should find a completely random scatter around 0, and no visible patterns among x or y-axis indicating that there is no relationship between the predicted values and the residuals (no clusters, no correlation)2.2 The fact that the residuals have no pattern to them tells us that all the important variation has already been captured by our model, indicating we chose a good model!"
  },
  {
    "objectID": "24_phenology-modeling.html#use-the-model-to-predict",
    "href": "24_phenology-modeling.html#use-the-model-to-predict",
    "title": "24  Phenology: Inferring and Predicting patterns",
    "section": "24.2 Use the model to predict",
    "text": "24.2 Use the model to predict\nLet’s say that we had looked at a bunch of climate models predicting mean March temperatures under various emissions scenarios and based off of that information we want to know what the predicted flowering dates would be for 10 - 65F.\nFirst, we need to create a new data set with the values we want to predict flowering date for.\n\n# create data frame with temperature values from 10 - 65F\npredict_temp &lt;- expand.grid(temperature = 10:65)\n\nhead(predict_temp)\n\n\n?(caption)\n\n\n\n  temperature\n1          10\n2          11\n3          12\n4          13\n5          14\n6          15\n\n\n\n\nThen we can use the function predict(). This generates a tibble with the predicted flowering dates for each temperature.\n\npred &lt;- predict(lm_fit, new_data = predict_temp)\n\nhead(pred)\n\n\n?(caption)\n\n\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  132.\n2  130.\n3  129.\n4  127.\n5  126.\n6  124.\n\n\n\n\nWe can use the same function to calculate confidence intervals.\n\npred_CI &lt;- predict(lm_fit, new_data = predict_temp,\n                   type = \"conf_int\")\n\nhead(pred_CI)\n\n\n?(caption)\n\n\n\n# A tibble: 6 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        126.        138.\n2        124.        136.\n3        123.        134.\n4        122.        132.\n5        120.        131.\n6        119.        129.\n\n\n\n\nBecause all the outputs are tibbles, we can easily combine them all into a single data.frame using bind_cols():\n\npredict_temp &lt;- predict_temp %&gt;%\n  bind_cols(pred) %&gt;%\n  bind_cols(pred_CI)\n\nhead(predict_temp)\n\n\n?(caption)\n\n\n\n  temperature    .pred .pred_lower .pred_upper\n1          10 131.7960    125.5921    137.9999\n2          11 130.2244    124.3198    136.1289\n3          12 128.6527    123.0461    134.2594\n4          13 127.0811    121.7707    132.3915\n5          14 125.5095    120.4935    130.5255\n6          15 123.9379    119.2139    128.6618\n\n\n\n\nNow we can plot our predicted values along with the estimated confidence intervals.\n\nggplot(predict_temp, aes(x = temperature, y = .pred)) +\n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                width = .2) +\n  geom_point(shape = 21, fill = \"darkorange\", size = 2) +\n  labs(x = \"temperature\", y = \"predicted flowering date\")\n\n\n\n\nFigure 24.3: Predicted flowering date and 95% confidence intervals for temperatures ranging from 10 - 65F.\n\n\n\n\nNot unsurprisingly, our predicted points fall exactly on the calculated trendline, though it is notable that the farther we extrapolate out to temperatures lower or higher than those recorded in our data set our uncertainty increases."
  },
  {
    "objectID": "24_phenology-modeling.html#do-we-trust-our-model",
    "href": "24_phenology-modeling.html#do-we-trust-our-model",
    "title": "24  Phenology: Inferring and Predicting patterns",
    "section": "24.3 Do we trust our model?",
    "text": "24.3 Do we trust our model?\nThe process of prediction itself is straightforward. We have the equation for the linear model, so all we have to do (or tell R to do) is to plug the values of the predictor(s) (our independent variable) in to the model equation and the calculate the predicted value for our response (independent) variable.\nHowever, there still are two factors we need to consider that will determine whether or not we can trust our predictions.\n\nThere is no guarantee that the model estimates you have are correct.\nThere is no guarantee that the model will perform as well with new data as it did with your sample (observed) data.\n\nOur concern lies in the concept of overfitting vs underfitting.\n\n\n\nComparison of underfit (left), optimal (middle), or overfit (right) model.\n\n\nIn this figure you can see that a linear function is not sufficient to describe the pattern in the observed data3. This is described as underfitting, i.e. your model is too simple to capture the pattern and as a result you would expect uncertainties in our predicted values. Likely the model would be more accurate for some values (in this case lower values) while for others (high values) our predicted values would not be very trustworthy.3 Think back to us calculating the rate of change for our CO2 and temperature data … where we likely over/under/optimally fitting our data set?\nThe panel on the right illustrates what happens when we overfit the model to the data. Here, the model fits the data almost perfectly - which means that it describes the noise not the general pattern in the data.\nHow can we figure out if we’ve over- or underfit our model? Well, one method is data splitting which means we will be able to cross-validate our model."
  },
  {
    "objectID": "24_phenology-modeling.html#data-splitting",
    "href": "24_phenology-modeling.html#data-splitting",
    "title": "24  Phenology: Inferring and Predicting patterns",
    "section": "24.4 Data splitting",
    "text": "24.4 Data splitting\nPreviously, we used all the data available to use to fit a model and understand how to interpret the data. But if we want to know how good our model is at predicting things we can allocate our data to two different tasks, training/fitting the model and then testing the performance of the model.\nData splitting means that we will take our data set and split it into a training data set that we will use to to fit the model and a test data set that we can use to determine how well our model performs.\nThe rsample package which is part of the tidymodels network of packages, allows us to randomly subset our data. We will want to have most of our data in the training data set (more data generally means better model) and then keep a small number of observations for testing.\nBecause we are randomly sub-setting our data set we need to set our initial seed number which means that every time we run the analysis the same set of random numbers will be generated4.4 It is still considered a random subset, even though we get the same random subset every time. We are essentially giving the random draw the starting point to make it reproducible, i.e. anyone who runs the analysis gets the same training and test data set and therefore the same results as us.\n\n# set random seed\nset.seed(42)\n\n# assign 3/4 of data to training set\ndata_split &lt;- initial_split(pheno, prop = 3/4)\n\n# create two separate data frames\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nIf we look at the dimensions of the two subset we just created we see that they have the same number of observations but they differ in the number of rows roughly at a ratio of 4:1.\n\ndim(train_data)\n\n[1] 38  3\n\ndim(test_data)\n\n[1] 13  3"
  },
  {
    "objectID": "24_phenology-modeling.html#evaluating-models",
    "href": "24_phenology-modeling.html#evaluating-models",
    "title": "24  Phenology: Inferring and Predicting patterns",
    "section": "24.5 Evaluating models",
    "text": "24.5 Evaluating models\nOur first step is fitting the model using our training data set. This looks exactly like when we fit the model using all of our data except now we will specify train_data as the data frame containing the observations.\n\nlm_fit &lt;- linear_reg() %&gt;%                        \n  set_engine(\"lm\") %&gt;%                              \n  fit(day_of_year ~ temperature, data = train_data)\n\ntidy(lm_fit) %&gt;%\n  kable()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n145.061292\n5.5356904\n26.204733\n0\n\n\ntemperature\n-1.518128\n0.1863277\n-8.147625\n0\n\n\n\nTable 24.2: Model parameters to predict flowering date based on temperature data.\n\n\nNow we need to use our model to predict values for our training data and the combine that output with our test_data that also contains the observed flowering dates.\n\nflowering_pred &lt;- predict(lm_fit, test_data) %&gt;%\n  bind_cols(test_data) %&gt;%\n  select(year, temperature, day_of_year, .pred)\n\nLet’s take a look at how well our model performs by comparing the observed and predicted flowering dates\n\nflowering_pred %&gt;%\n  arrange(desc(.pred)) %&gt;%\n  kable()\n\n\n\n\n\n\nyear\ntemperature\nday_of_year\n.pred\n\n\n\n\n1975\n22.00\n116\n111.66247\n\n\n1970\n24.09\n114\n108.48958\n\n\n1956\n24.17\n114\n108.36813\n\n\n1962\n25.86\n112\n105.80249\n\n\n1954\n26.73\n102\n104.48172\n\n\n1982\n27.27\n108\n103.66193\n\n\n1974\n28.50\n107\n101.79463\n\n\n1949\n28.51\n102\n101.77945\n\n\n1957\n28.71\n109\n101.47583\n\n\n1961\n32.81\n103\n95.25150\n\n\n1983\n33.06\n100\n94.87197\n\n\n1963\n33.69\n90\n93.91555\n\n\n1946\n40.97\n83\n82.86357\n\n\n\nTable 24.3: Comparison of observed and predicted flowering dates for training data.\n\n\nLet’s make a plot that will allow us to compare the predicted vs observed values.\n\nggplot(flowering_pred, aes(x = day_of_year, y = .pred)) +\n  geom_smooth(method = \"lm\", color = \"darkred\") +\n  geom_abline(slope = 1, linetype = \"dashed\", color = \"black\", size = 1) +\n  geom_point(shape = 21, fill = \"darkorange\", size = 3) +\n  labs(x = \"observed flowering date\", y = \"predicted flowering date\")\n\n\n\n\nFigure 24.4: Comparison of predicted vs. observed flowering date for training data set. The dashed black line indicates perfect predictions, the red line is the linear trendline showing relationship of observed vs predicted values.\n\n\n\n\nIf we had “perfect” predictions, our linear trendline should fall on top of the dashed line indicating perfect predictions.\nOne way we can quantify this is calculating the root-mean-square deviation (RMSE) which is the standard deviation of the residuals, i.e. the prediction errors. We can calculate this using the package yardstick which contains functions to assess model performance.\n\nrmse(flowering_pred, \n     truth = day_of_year, \n     estimate = .pred) %&gt;%\n  kable()\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n5.036233\n\n\n\nTable 24.4: Root mean square deviation of residuals for the predicted model.\n\n\nIn general, a smaller RMSE indicates a better model fit but on it’s own the number does not really tell us much.\nLet’s compare that to the RMSE for our training data:\n\nlm_aug &lt;- augment(lm_fit$fit)\n\nrmse(lm_aug, \n     truth = day_of_year, \n     estimate = .fitted) %&gt;%\n  kable()\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n6.346058\n\n\n\nTable 24.5: Root mean square deviation (RMSE) for training data set.\n\n\nBy comparing these two values we can see that our model performs similarly well for the training data set used to make the model and also for the test data set that was not used to make the model.\nThe fact that the observation were not used to inform the initial model, means that our comparison of the observed vs fitted values are not biased.\nIf the model was overfit (i.e. it modeled the noise not the overall pattern) we would expect our model to perform poorly in terms of predictions for observations that were not included in the initial data set used to fit the model.\nThere are some additional processes that go into optimizing models but we’ll hold it here for now."
  }
]