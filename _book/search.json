[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for a Changing Planet",
    "section": "",
    "text": "Overview\nWelcome to our lab manual for Data Science for a Changing Planet (Fall 2023) - it will be continuously updated throughout the semester. For the most part an electronic copy (on a laptop or tablet) is sufficient, if you need to print anything for a class you will be prompted to do so on Canvas."
  },
  {
    "objectID": "index.html#course-format",
    "href": "index.html#course-format",
    "title": "Data Science for a Changing Planet",
    "section": "Course Format",
    "text": "Course Format\nThis is a lecture/lab course meeting three times a week (MWF 10:25-11:15am & 11:30am - 12:20pm). Key concepts will be introduced in reading assignments, short ‘prelab’-type tutorials, or exercises that you will be expected to complete before class.\nWhile lecture/lab time will incorporate brief refreshers and more in-depth demonstrations of some of the more complex topics (participatory code-alongs), the focus will be on applying the concepts and ‘learning by doing’ by directly applying new skills to case studies and data sets from ecology, environmental science and public health.\nBYOD: Bring your own device (to lab)\nYou are required to bring your laptop for use during class times; a tablet will not be sufficient to participate though you are welcome to bring an additional device to have an extra screen to follow along an electronic version of the lab manual. Make sure to have a power cable and/or fully charged battery! If you run into computer issues please let Dr. O’Leary know as soon as possible so we can come up with a back-up plan such as organizing a laptop to use during/outside of class time to complete activities."
  },
  {
    "objectID": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "href": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.1 Install & Set up R and Rstudio on your computer",
    "text": "2.1 Install & Set up R and Rstudio on your computer\nIf you have already installed R and Rstudio make sure your R version is up to date. Whenever you open Rstudio the version will be printed in the console (bottom left pane). In addition, you can always check what version is installed by typing sessionInfo() into your console. You should be using version 4.0.0 or later. You do not need to uninstall old version of R. If you do have to update, you will need to re-install packages (see below) for R4.0.0\n\n2.1.1 Windows\nInstall R\n\nDownload most recent version of R for Windows here.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall Rtools\n\nDownload Rtools here.\nRun the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\nInstall Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Windows XP/Vista/7/8/10.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nFinish setting up Rtools\n\nOpen Rstudio to make sure you aren’t getting any error messages.\nPut Rtools in your path by typing writeLines('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', con = \"~/.Renviron\") in the console window.\nInstall the devtools package by typing install.packages(\"devtools\") in the console.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\n\n2.1.2 Mac OS X\nDownload & install R\n\nGo to (CRAN)[http://cran.r-project.org/], select Download R for (Mac) OS X.\nDownload the .pkg file for your OS X version.\nRun the downloaded file to install R.\n\nDownload & install XQuartz (needed to run some R packages)\n\nDownload XQuartz\nRun the downloaded file to install\n\nDownload & install Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Mac OS X.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard."
  },
  {
    "objectID": "02_install-R.html#get-to-know-rstudio",
    "href": "02_install-R.html#get-to-know-rstudio",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.2 Get to know Rstudio",
    "text": "2.2 Get to know Rstudio\nRstudio is an Integrated Development Environment (IDE) that you can use to write code, navigate files, inspect objects, etc. The advantage of using an IDE is that you have access to shortcuts, visual cues, troubleshooting, navigation, and autocomplete help.\n\n2.2.1 GUI Layout\nOpen Rstudio and identify the four panes in the interface (default layout).\n\nEditor (top left): edit scripts/other documents, code can be sent directly to the console.\nR console (bottom left): Run code either by directly typing the code or sending it from the editor pane.\nEnvironment/history (top right): Contains variables/objects as you create them & full history of functions/commands that have been run.\nfiles/plots/packages/help/viewer (bottom right): Different tabs in this pane wil let you explore files on your computer, view plots, loaded packages, and read manual pages for various functions.\n\nThe panes can be customized (Rstudio -&gt; Preferences -&gt; Pane Layout) and you can move/re-size them using your mouse. We are going to switch to have the Console in our top right and the Environment in the bottom left which makes it easier to see your code output and your script/quarto document at the same time.\n\n\n2.2.2 Interacting with R in Rstudio\nThink of R as a language that allows you to give your computer precise instructions (code) to follow.\n\nCommands are the instructions we are giving the computer, usually as a series of functions.\nExecuting code or a program means you are telling the computer to run it.\n\nThere are three main ways to interact with R - directly using console, script files (*.R), or code chunks embedded in R markdown (*.Rmd) or quarto files (*.qmd). We will generally be working with the later.\nThe console is where you execute code and see the results of those commands. You can type your code directly into the console and hit Enter to execute it. You can review those commands in the history pane (or by saving the history) but if you close the session and don’t save the history to file those commands will be forgotten.\nBy contrast, writing your code in the script editor either as a standard script or as a code chunk in an quarto document allows you to have a reproducible workflow (future you and other collaborators will thank you).\nExecuting an entire script, a code chunk, or individual functions from a script will run them in the console.\n\nCtrl + Enter will execute commands directly from the script editor. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute.\nIf you are using a quarto file you can execute an entire code chunk by pressing the green arrow in the top right corner.\n\nIf the console is ready for you to execute commands you should see a &gt; prompt. If you e.g. forget a ) you will see a + prompt - R is telling you that it is expecting further code. When this happens and you don’t know what you are missing (usually it is an unmatched quotation or parenthesis), make sure your cursor is in the console and hit the Esc key.\n\nWe will run through these options, but you can always check back here while you are getting used to R.\n\n\n\n2.2.3 Customize Rstudio\nThere are several options to customize Rstudio including setting a theme, and other formatting preferences. You can access this using Tools &gt; Global Options. I recommend using a dark theme (it’s a lot easier on the eyes) and keeping the panes in the same positions outlined above because it will make troubleshooting a lot easier1.1 “You should see xx in the top left” is a lot more helpful if your top left looks like my top left!"
  },
  {
    "objectID": "02_install-R.html#installing-and-using-packages-in-r",
    "href": "02_install-R.html#installing-and-using-packages-in-r",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.3 Installing and using packages in R",
    "text": "2.3 Installing and using packages in R\n\n2.3.1 Install a package\nThink of R packages or libraries as tool kit comprising a set of functions (tools) to perform specific tasks. R comes with a set of packages already installed that gives you base R functions; you can view these and determine which have been loaded in the Packages tab in the bottom right pane. For other tasks we will need additional packages. 22 Most R packages are found in the CRAN repository and on Bioconducter, developmental packages are available on github.\nA central group of packages for data wrangling and processing form the tidyverse, described as “… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” - We are going to heavily rely on core functions from the tidyverse to wrangle, summarize, and analyze data.\nWhen you install packages they will be downloaded and installed onto your computer. Determine what your default path is using .libPaths() and change if necessary.\nThe easiest way to install packages directly in the console is to use the install.packages() function.\nUse the R console to install some libraries to get us started (we will install other libraries as needed for other labs).\n\n\nUsing # in an R script allows you to insert comments that are ignored by R when executing your code. Use comments to document your code, future you will thank you! Before submitting any of your skills tests or homework assignments you should always go through and make sure each piece of code has a descriptive comment. You do not need to add a comment for multi-line code that you are stringing together using a pipe %&gt;% but you should have one descriptive comment above the set of commands you are giving R and then make sure that you add any comments that you need to remember how the function works or which parameters might be useful to tweak/set differently if you were to reuse that code.\n\n# install the rmarkdown package\ninstall.packages(\"rmarkdown\")\n\n# install central packages in the tidyverse\ninstall.packages(\"tidyverse\")\n\n# install additional packages\ninstall.packages(\"plyr\", \"ggthemes\", \"patchwork\", \"glue\")\n\nLet’s check if you were able to successfully install those packages by ensureing you can load them. Any time you start a new R session (e.g. by closing Rstudio and restarting it), you will need to load your libraries beyond the base libraries that are automatically loaded using the library() function in order to be able to use the functions specific to that package3.3 Troubleshooting tip: if you get an error along the lines of function() cannot be found the first thing you will want to do is check if your libraries are loaded!\n\n# load library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIf you don’t see any error messages in the console along the lines of there is no package called ... you are all set. If you look in the packages tab in the lower right panel you should also see that packages such as dplyr and tidyr (two of the central tidyverse packages) now have a little check box next to them.\n\n\n2.3.2 Updating R packages\nYou should generally make sure to keep your R packages up to date as new versions include important bugfixes and additional improvements. The easiest way to update packages is to use the Update button in the Packages tab in the bottom right panel. Over the course of the semester you should not have to do this, but when you install new packages you might get message that some of your packages need to be updated which you can then either choose to do at that point or ignore.\n\n\n\n\n\n\nWarning\n\n\n\nBe aware that updating packages might break some code you have previously written. For most of what we will be doing this should not be the case. If you used R for a previous course, make sure to update you packages at the beginning of this course and we should be set for the semester."
  },
  {
    "objectID": "01_intro-data-science.html#defining-data-and-science",
    "href": "01_intro-data-science.html#defining-data-and-science",
    "title": "1  What even is Data Science?",
    "section": "1.1 Defining “data” and “science”",
    "text": "1.1 Defining “data” and “science”\nData are qualitative and quantitative observations that are measured and collected.\n\n\n\n\n\n\n Think\n\n\n\nData fall in two distinct categories, categorical and numerical data. Briefly compare and contrast these two categories by describing the data types you would expect to find in each.\n\n\nThe ‘data - information - knowledge - wisdom pyramid’ gives us a framework to consider how data can be used to inform decision making and impact the world around us. Not until data is organized and processed thus adding context can we glean information from the signal. Additional meaning is transferred as we synthesize and further contextualize information resulting in knowledge.\nThese three categories look backwards - we describe the “what” and ask about the “why” to reveal patterns and relationships. At this point we start looking forward to determine what action(s) should be taken, we know seek to reveal principles and directions that can be applied.\nWe integrate knowledge across disciplines to gain insight and wisdom to further understanding of problems and derive actionable solutions. This culminates in the decision-making process resulting in change.\n\n\n\n\n\n\n Think\n\n\n\nThe science council defines science as the pursuit and application of knowledge and understanding of the natural and social world following a systematic methodology based on evidence. Compare and contrast this definition to the DIKW framework and make an argument that all science is data science."
  },
  {
    "objectID": "01_intro-data-science.html#what-even-is-data-science",
    "href": "01_intro-data-science.html#what-even-is-data-science",
    "title": "1  What even is Data Science?",
    "section": "1.2 What even is data science?",
    "text": "1.2 What even is data science?\nData science is a fuzzy term and no single definition exists. Most definitions emphasize that it is a interdisciplinary field and that it has arisen in response to the increasingly large data sets that are produced.\nA common way of defining data science is to describe it as being the intersection of domain knowledge, statistics/mathematics, and computer science - though different definitions will ascribe more importance to certain different components.\nOne distinction to the typical scientific process as you may have learned it to be is that a large component of data science is hypothesis generation through exploratory analysis rather than hypothesis confirmation.\nThe data science process generally starts by posing an interesting question and ends with visualizing and communicating the results. The key steps to the end results are obtaining the data, processing and exploring the data, and modeling the data to understand the data set and derive conclusions."
  },
  {
    "objectID": "01_intro-data-science.html#the-data-science-process",
    "href": "01_intro-data-science.html#the-data-science-process",
    "title": "1  What even is Data Science?",
    "section": "1.3 The data science process",
    "text": "1.3 The data science process\n\n1.3.1 Ask an interesting question\nLet’s start by asking an interesting question:\n\nWill Sasquatch by impacted by climate change?\n\nSpecifically, climate change could result in a shift in habitat availability, i.e. should we expect a species extinction because of habitat loss or a range expansion or range shift?\nTo do this we need to generate a species distribution model (SDM) for the Sasquatch, a large, hairy, bipedal ape-like creature found (or is it?) throughout North America1, i.e. first we need to understand where Sasquatch are currently distributed to then assess how that might change in the future.1 I suppose, technically, since we were venturing into cryptozoology here it is not a species, but rather a cryptid distribution model\n\n\n1.3.2 Get occurrence data\nThe first thing we need for any SDM is a data set documenting species occurrence, i.e. geo-coded observations of a given species in the wild.\nFour our purposes, we turn to the Bigfoot Field Researchers Organization (BFRO), founded in 1995 as the “only scientific research organization exploring the Bigfoot/Sasquatch mystery”. You can turn to their website for answers on important FAQs, including ‘Do Bigfoot Sasquatch bury their dead?’, ‘Where is the physical evidence?’, ‘Wasn’t this all shown to be a fake?, and ’Why do you want scientists to recognize the Sasquatch as a species? Isn’t it better to just leave them alone?’. Their main focus though is on compiling reports of sightings and investigating them. In other words, they have a database full of geo-coded reported sightings2.2 And even better, it has already been downloaded and wrangled and is accessible (with sightings through 2018) right here.\nLet’s read in the data and then we can take a look at the information we can glean from this data set by looking at the column names.\n\n# read data\noccurrence &lt;- read_delim(\"data/bfro_reports_geocoded.txt\", delim = \"\\t\")\n\n\n\n1.3.3 Tidy, Transform & Explore the data\nLet’s start by taking a look at our data set to determine how we need to wrangle to get the information we need process it so we can generate our species distribution model.\nBecause we need to be able to identify the exact locations Sasquatch occur, we are going to remove any observations that do not have latitude and longitude information.\n\n# filter NAs\noccurrence &lt;- occurrence %&gt;%\n  filter(!is.na(longitude),\n         !is.na(latitude))\n\nNext, let’s consider is what geographic extent of the observations is by looking at the distributions on a map.\n\n# get minimum and maximum lat/longs\nmax.lat &lt;- ceiling(max(occurrence$latitude))\nmin.lat &lt;- floor(min(occurrence$latitude))\nmax.lon &lt;- ceiling(max(occurrence$longitude))\nmin.lon &lt;- floor(min(occurrence$longitude))\n\n# create an extent object of the range of observations\ngeo_range &lt;- extent(x = c(min.lon, max.lon, min.lat, max.lat))\n\n# get base map\ndata(wrld_simpl)\n\n# plot the base map\nplot(wrld_simpl, \n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE, \n     col = \"grey75\")\n\n# add individual occurrences\npoints(x = occurrence$longitude, \n       y = occurrence$latitude, \n       col = \"darkorange\", \n       pch = 20, \n       cex = 0.75)\n\n# draw box around figure\nbox()\n\n\n\n\nFigure 1.1: Map of Sasquatch sighting in the United States based on the BFRO database (1950 - 2021).\n\n\n\n\nHere is an example of where domain knowledge comes in - while we are tidying and exploring the data set we need to assess whether there are artifacts our outlier data points that should be removed.\n\n\n\n\n\n\n Think\n\n\n\nGive a brief descriptions of the spatial distribution of the occurrence of Sasquatch in the United States. Note areas where sightings appear to be random, clustered or more dispersed, determine if you think any points should be removed.\n\n\n\n\n\n\n\n\n Think\n\n\n\nDiscuss some possible explanations for the patterns you have observed to determine whether you think this data set is a reasonable representation of the ecological niche of the Sasquatch and can be used to create a species distribution model.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGood catch - those sightings in the middle of the ocean are probably errors in the geo-coding. Further, we don’t know how reliable the data for Alaska is as this data set is based on opportunistic sightings - at least for the most part, if you poke around the BFRO website you will find that people do plan expeditions.\n\n\n\nThe data for for observations in the lower 48 seems much more reliable, so let’s restrict our data to those observations.\n\n# load and filter data\noccurrence &lt;- read_delim(\"data/bfro_reports_geocoded.txt\", delim = \"\\t\") %&gt;%\n  filter(!is.na(longitude),\n         !is.na(latitude),\n         longitude &gt; -130,\n         latitude &lt; 55)\n\n# get minimum and maximum lat/longs\nmax.lat &lt;- ceiling(max(occurrence$latitude))\nmin.lat &lt;- floor(min(occurrence$latitude))\nmax.lon &lt;- ceiling(max(occurrence$longitude))\nmin.lon &lt;- floor(min(occurrence$longitude))\n\n# create an extent object\ngeo_range &lt;- extent(x = c(min.lon, max.lon, min.lat, max.lat))\n\n# get base map\ndata(wrld_simpl)\n\n# plot the base map\nplot(wrld_simpl, \n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE, \n     col = \"grey75\")\n\n# add individual occurrences\npoints(x = occurrence$longitude, \n       y = occurrence$latitude, \n       xlim = c(min.lon, max.lon),\n       ylim = c(min.lat, max.lat),\n       col = \"darkorange\", \n       pch = 20, \n       cex = 0.75)\n\n# draw box around figure\nbox()\n\n\n\n\nFigure 1.2: Map of Sasquatch sighting in the lower 48 states based on the BFRO database as (1950 - 2021).\n\n\n\n\nThis should work - on to the next step!\n\n\n1.3.4 Get more data\nOccurrence data along will not allow us to build a species distribution model; what we need is environmental data to go with the locations, i.e. we need a set of predictor variables.\nCombining multiple data sets is typical for data science projects, frequently the quality of a project hinges on the availability of hight quality data sets that can provide information to describe or preodict behavior if the data set you are exploring.\nClimate is complex and multidimensional, though at its core climate is determined by long-term patterns in mean and variability of temperature and precipitation.\nWe are going to use the bioclim data set from CliMond. Bioclim variables are commonly used for species distribution modeling as they are based on long-term trends (as opposed to e.g. the exact conditions when the species was observed).\n\n\n\n\n\n\n Think\n\n\n\nLook up the descriptions of the bioclim variables and give a brief description of the four abiotic parameters that are included and how they are being parameterized. Argue which you think are most important to describe a species distribution/ecological niche and whether you think overall this data set will help us understand Sasquatch species distribution.\n\n\nThe data set we are using includes a core set of 19 variables that describe temperature and precipitation, along with an additional 16 variables that describe solar radiation and soil moisture. This information is encoded in the raster files of the historical (contemporary) bioclim data sets at a resolution of 10’ (minutes) into the data folder. The “historical” data set consists of data from 1961 - 1990 centered on 1975.\nA raster file is an image file consisting of pixels with data associated with it. In this case, our “pixels” are 10’ x 10’ and depending on the layer the value associated with each pixel is the value for that bioclim value at that geographic location.\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_1975H', full.names=TRUE )\n\n# import and convert to raster stack\npredictors &lt;- stack(files)\n\nWe have now created an object that at its core consists of a list where each element is a layer (bioclim variable raster).\n\n\n1.3.5 Tidy, transform & explore the data (again)\nLet’s plot the first bioclim variable (Bio01, annual mean temperature).\n\nplot(predictors@layers[[1]])\n\n\n\n\nFigure 1.3: Global distribution of annual mean temperature (1961 - 1990, centered on 1975).\n\n\n\n\nWe see the pattern we would intuitively expect, with temperatures decreasing as you move poleward and being warmest around the poles.\nLet’s extract the values for each bioclim variable at our occurrence points (observations).\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  dplyr::select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\nLet’s take a quick look at the first few rows and columns of the matrix we just created.\n\nhead(presence[,1:3])\n\n     CM10_1975H_Bio01_V1.2 CM10_1975H_Bio02_V1.2 CM10_1975H_Bio03_V1.2\n[1,]              8.387707              15.84337             0.3534581\n[2,]             10.156170               9.49416             0.2941423\n[3,]             15.522450              13.50635             0.3474373\n[4,]             11.259700              12.40612             0.3456673\n[5,]             10.189860              11.02385             0.3073747\n[6,]              8.724987              10.20157             0.2807353\n\n\n\n\n\n\n\n\n Think\n\n\n\nBriefly state what these values represent\n\n\n\n\n1.3.6 Create species distribution model\nOur next step is to fit a bioclim model to the data set we just generated.\n\n# fit bioclim model\nmodel.fit &lt;- bioclim(presence)\n\nThe bioclim model is a classic climate-envelope-model3.3 You may remember reading about this in your reading assignments.\nBriefly, the algorithm computes the similarity of locations by comparing the value of each environmental variables being used (our bioclim data set) to a distribution of that values at all locations with known presence4. The closer that value is to the median (50th percentile), the more suitable that location is assumed to be. Suitability scores are between 0 and 1, with 1 indicating a “perfect suitability”.)4 Also called the training sites; these are our occurrence points.\nIn general, there is no distinction between the tails of the distribution (i.e. the 90th and 10th percentile are equivalent), though in some implementations you can specify those to be treated as distinct. As a result e.g. low levels of precipitation could be limiting but high levels would not be.\nFinally, we will use our suitability scores and the bioclim raster data set to generate a predictive map of the Sasquatch species distribution. This means that the algorithm will assign a suitability score to each pixel based on the model and create a new raster layer.\n\n# generate raster with predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\nLet’s plot our species distribution map.\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.4: Species distribution model for Sasquatch. Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Think\n\n\n\nGive a brief description of the habitat suitability map, including the maximum probabilities. Describe wether this result fits your expectations based on the distribution map of reports we looked at earlier. According to our map, if you were planning a Sasquatch research trip, where would you be headed?\n\n\nOur highest habitat suitability values (probability of occurring) seem pretty low. One reason for this is that we used presence-only data.\n\n\n\n\n\n\n Think\n\n\n\nThe alternative to presence-only models is to have presence-absence data. Discuss how this would improve the models. Argue why you think presence-only data sets are easier to generate.\n\n\n\n\n\n\n\n\n Think\n\n\n\nA work-around of not having absence data is to generate pseudo-absence data. This is done by generating random points within the geographic range and using those as proxies for absence data. Briefly argue the merits and limitations of such an approach.\n\n\n\n\n1.3.7 Model future climate change\nProjections of future climate change are heavily dependent on human activity and the resulting greenhouse gas emissions. Therefore the IPCC’s Assessment Reports contain scenario families that represent projected climate conditions based on emission scenarios resulting from future technological and economic development as defined by each scenario.\nLet’s look at how the species distribution map might change in response to a shift in environmental parameters.\nTo do this we will use bioclim raster files for 2100 generated using the A1B and A2 scenarios.\nThe A1 climate scenarios assume a more integrated world characterized by rapid economic growth, a global population that peaks at 9 billion (2050) and the gradually declines, rapid spread of new/efficient technologies, and a convergent world characterized by extensive worldwide social and cultural interactions. Scenario A1B further assumes a balanced emphasis on fossil and non-fossil fuels.\nBy contrast, A2 scenarios assume a more divided world consisting of independently operating and self-reliant nations and regionally-oriented economic development. The population is assumed to continuously grow. Finally, this scenario is characterized by high emissions.\nLet’s start with the A1 climate scenario to create our species distribution model.\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_2100_A1B', full.names=TRUE )\n\n# import and convert to raster stack\npredictors_A1 &lt;- stack(files)\n\nNow let’s fit our model and create predictive map.\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors_A1, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\n# fit the bioclim model\nmodel.fit &lt;- bioclim(presence)\n\n# create raster layer of predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\nFinally, let’s plot our species distribution map.\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.5: Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A1B). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Think\n\n\n\nGive a brief description of the habitat suitability map, including the maximum probabilities. Describe whether this fit your expectations based on the change in bioclim variables. Describe the shift in habitat suitability relative to our current model.\n\n\nHow about our A2 climate scenario?\n\n# get list of files\nfiles &lt;- list.files(\"data/\", pattern='^CM10_2100_A2', full.names=TRUE )\n\n# import and convert to raster stack\npredictors_A2 &lt;- stack(files)\n\n# create df with just xy coordinates\nxy &lt;- occurrence %&gt;%\n  select(longitude, latitude)\n\n# crop bioclim data to geographic range\ncropped_predictors &lt;- crop(x = predictors_A2, y = geo_range)\n\n# extract values\npresence &lt;- raster::extract(cropped_predictors, xy)\n\n# fit the bioclim model\nmodel.fit &lt;- bioclim(presence)\n\n# create raster layer of predicted distribution\nprediction &lt;- dismo::predict(x = cropped_predictors, \n                             object = model.fit,\n                             ext = geo_range)\n\n# plot model probabilities\nplot(prediction,\n     xlim = c(min.lon, max.lon),\n     ylim = c(min.lat, max.lat),\n     axes = TRUE)\n\n# add map\nplot(wrld_simpl, add = TRUE, border = \"black\")\n\n# draw box around it\nbox()\n\n\n\n\nFigure 1.6: Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A2). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.\n\n\n\n\n\n\n\n\n\n\n Think\n\n\n\nBriefly compare this model to the previous two. Comment on whether you expected to see greater or smaller difference to the other future climate prediction based on the scenarios that they are based on.\n\n\n\n\n\n\n\n\n Think\n\n\n\nDiscuss whether you think bioclim variables are good models to predict a species’ respone to climate change. In your discussion consider how future bioclim data sets are generated, as well as, whether abiotic conditions along will determine range changes."
  },
  {
    "objectID": "01_intro-data-science.html#visualize-and-communicate-your-results",
    "href": "01_intro-data-science.html#visualize-and-communicate-your-results",
    "title": "1  What even is Data Science?",
    "section": "1.4 Visualize and Communicate your results",
    "text": "1.4 Visualize and Communicate your results\nPrinting our maps side by side for better comparison would be a good way to visualize and communicate our results. We would probably include a discussion of our our approach (we should probably validate our model too which we haven’t done here) and make recommendations based on our findings."
  },
  {
    "objectID": "01_intro-data-science.html#acknowledgments",
    "href": "01_intro-data-science.html#acknowledgments",
    "title": "1  What even is Data Science?",
    "section": "1.5 Acknowledgments",
    "text": "1.5 Acknowledgments\nSince I’m not the first person to create a lab/tutorial on species distribution modeling, I drew inspiration from various educators, R and data enthusiasts to shape this tutorial, most notably:\nAnna L. Carter. November 2017, posting date. Painting turtles: an introduction to species distribution modeling in R. Teaching Issues and Experiments in Ecology, Vol. 13: Practice #1 [online]. http://tiee.esa.org/vol/v13/issues/data_sets/carter/abstract.html\nWendy L. Clement, Kathleen L. Prudic, and Jeffrey C. Oliver. 16 August 2018, posting date. Exploring how climate will impact plant-insect distributions and interactions using open data and informatics. Teaching Issues and Experiments in Ecology, Vol. 14: Experiment #1 [online]. http://tiee.esa.org/vol/v14/experiments/clement/abstract.html\nhttps://jcoliver.github.io/learn-r/011-species-distribution-models.html"
  }
]