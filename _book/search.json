[
  {
    "objectID": "13_greenhouse-gases.html#unequivocal-and-unprecedented",
    "href": "13_greenhouse-gases.html#unequivocal-and-unprecedented",
    "title": "13  Climate change: Drivers",
    "section": "13.1 Unequivocal and unprecedented?",
    "text": "13.1 Unequivocal and unprecedented?\nCurrent climate change is concerning not (only) because of absolute warmth of the earth but due to rate at which it is occurring. What this means is that over the history of our planet there have been times at which temperatures have been higher that what we are currently experiencing. The component that is “unprecedented” is the rate of increase and impact that is having on our planet22 We will look at examples of that impact using various data set describing change in the earth-climate system.\nIn this chapter, we are going to dive into a series of data sets that will allow us to explore whether we do indeed have evidence that the climate change we are currently observing is indeed unequivocal, driven by anthropogenic effects, and unprecedented in its rate.\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we get started let’s consider what data sets we need to investigate whether rates of temperature increase are indeed “unprecedented”, to identify patterns of atmospheric CO2 concentrations consistent with anthropogenically driven climate change, and what we would expect our results to look like if the IPCC assessment is indeed correct in their claims.\nTake a few minutes to write out your thoughts for the following prompts:\n\nwhat variables/measurements do we need?\nwhat comparisons do we need to be able to make to determine if the rate of temperature increase is unprecedented?\nwhat would patterns consistent with anthropogenically driven climate change look like?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe will also need to consider limitations of our approach. In this case, we will need to assess whether our approach is investigating a causal or mechanistic effect in and of itself or if we are uncovering evidence consistent with a known mechanism/process."
  },
  {
    "objectID": "13_greenhouse-gases.html#current-rates-of-air-temperature-change",
    "href": "13_greenhouse-gases.html#current-rates-of-air-temperature-change",
    "title": "13  Climate change: Drivers",
    "section": "13.2 Current rates of air temperature change",
    "text": "13.2 Current rates of air temperature change\nLet’s start by taking a look at changes in global mean air temperatures.\n\n\n\n\n\n\n Consider this\n\n\n\nSketch out what the patterns of air temperature over time would look like if the (A) earth is warming, (B) cooling, (C) not changing at all and discuss with your class mates3. Determine what variables you would plot on the x-axis, y-axis and what the slope would look like. Then practice describing figures with a 1-2 sentence description of what each scenario.\n\n\n3 You can do this on a piece of paper and snap a picture or sketch directly on a computer/mobile device in an appropriate app. Save your picture as scratch/glob-temp-patterns.jpg and you can use the markdown code below to import it. If you end up with a different filename or file format, adjust your code accordingly.\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\nExpectations of change in air patterns over time for different scenarios\n\n\n\n\nThe air temperature data we will be using is compiled by the Goddard Institute for Space Studies (NASA) and can be accessed in on their webpage which also describes how their data set was compiled and processed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nAccess the GISS Surfact Temperature Analysis Webpage and scroll down the page to the section Tables of Global and Hemispheric Monthly Means and Zonal Annual Means and download the CSV version of the Global-mean monthly seasonal, and annual means data set.\nAfter you have downloaded the data set place it in your data folder in your project directory and read it into your Global Environment.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRemember to correctly specify the delim argument for a *.csv file.\n\n# read csv file\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThis data is presented as temperature anomalies, i.e. as deviations from the corresponding 1951 - 1980 mean. Explain what this means and argue why this is a more appropriate way to present this data than to simply use the measured global temperature.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\n\nGenerally, climate patterns are long-term patterns. To describe climate patterns we generally use at least 30 years worth of data.\nAccording to Oxford Languages, the definition of an anomaly is “something that deviates from what is standard, normal, or expected”.\nIn the context of climate change is in more important to understand the actual temperature or the temperature change?\n\n\n\n\nBefore we move on, let’s check out our temperature object in the Global Environment to see if the file read in okay - spoiler alert, it did not4.4 You should create a habit of always checking that your data has read in as expected, immediately determining that something is wrong and correcting it will minimize issues with troubleshooting down the line.\nTroubleshooting Skills: Your file didn’t read in correctly. Let’s figure out why.\n\n\n\n\n\n\n Consider this\n\n\n\nWhat ideas do you have for us to track down the issue? Document the process you used to identify and fix the issue using short bullet points for future reference.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA good starting point is to open the file in a text editor to see if we missed anything. Rstudio now has a built in text editor. Use the file navigation pane to navigate to the data folder. Clicking on the temperature file will create a pop up, select View File and it will pop up in the View pane in a separate tab next to your quarto pane5.\nSure enough there seems to be an additional line at the beginning which is probably causing the issue. One way to fix this is to simply delete the extra line … before you do this, remember our first module looking at data wrangling and the cardinal rules we set in place? One key principle is “DO NOT EDIT YOUR RAW FILES”: if we want to have a reproducible workflow we should avoid manually editing our data sets.\nInstead use ?read_delim to pull up the help file for the function. You should find an argument called skip which will let us tell the function how many extra lines there are at the beginning of the file.\n\n# read csv file skipping the first line\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\", skip = 1)\n\n# check that dataframe is in order\nhead(temperature)\n\n# A tibble: 6 × 19\n   Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug Sep   Oct   Nov   Dec  \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1  1880 -0.18 -0.24 -0.09 -0.16 -0.1  -0.21 -0.18 -0.1  -.15  -.23  -.22  -.17 \n2  1881 -0.19 -0.14  0.03  0.05  0.06 -0.18  0    -0.04 -.15  -.22  -.18  -.07 \n3  1882  0.16  0.14  0.04 -0.17 -0.15 -0.23 -0.16 -0.07 -.14  -.24  -.17  -.36 \n4  1883 -0.29 -0.37 -0.12 -0.18 -0.17 -0.08 -0.06 -0.14 -.21  -.11  -.23  -.11 \n5  1884 -0.13 -0.08 -0.36 -0.4  -0.34 -0.36 -0.3  -0.27 -.27  -.25  -.33  -.31 \n6  1885 -0.58 -0.34 -0.27 -0.42 -0.45 -0.43 -0.34 -0.31 -.29  -.24  -.24  -.11 \n# ℹ 6 more variables: `J-D` &lt;chr&gt;, `D-N` &lt;chr&gt;, DJF &lt;chr&gt;, MAM &lt;dbl&gt;,\n#   JJA &lt;dbl&gt;, SON &lt;chr&gt;\n\n\nThat seems to have done the trick! Put that in your bag of tricks for future reference.\n\n\n\n5 Be really careful not to accidentally edit the raw data file!Now that that is resolve, let’s take a slightly more detailed look at our data set to make sure there aren’t any additional changes we need to make. For example we need to determine if there are NA values and if they are properly formatted and we need to make sure all the columns are numeric.\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your data frame, and make a few notes to document what information each column contains, if anything is out of order, and ideas on how to clean up any issues you have identified.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNA values have been specified as ***, which has caused some columns to be formatted as character instead of numeric.\nWe can use replace() to search every column (we specify this using . instead of a specific column name) and mutate_if() which tells R to check every column and if it is a character data type (is.character) to convert it to numeric (as.numeric).\n\ntemperature &lt;- temperature %&gt;%\n  replace(. == \"***\", NA) %&gt;%\n  mutate_if(is.character, as.numeric)\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nNow that we are all set, let’s create a plot with the yearly mean global temperature anomaly across time. Briefly conceptually describe what parameters we need to plot on the x and y-axis to accomplish this.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are going to use ggplot2 which is part of the tidyverse to plot our figure. This will introduce you to some of the standard syntax. Fear not, we will work through the details of the framework on which ggplot2 relies in the next chapter so think of this as a sneak peak to get used to the syntax.\n\nggplot(temperature, aes(x = Year, y = `J-D`)) +             # define data set and columns to plot on x and y axis\n  geom_line(color = \"blue\", size = 1) +                     # plot line plot\n  labs(x = \"year\", y = \"temperature anomaly [C]\",           # determine labels\n       title = \"Annual mean global surface temperature relative to 1951-1980 average.\",\n       caption = \"source: NASA Goddard Institute for Space Studies\")\n\nHere’s what your figure should look like based on that code.\n\n\n\n\n\nFigure 13.1: Change in annual mean global surface temperature from 1880 - 2021 relative to 1951-1980 average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe & interpret the figure: Describe the change in the deviation of global temperature to the 1950-1980 mean. Include and explanation of what it means for values to be negative or positive.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThis figure gives us a qualitative view of changing global temperatures.\nFor a quantitative assessment we would want to determine the rate of change. For this case we would define the rate of change as the change in temperature divided by the change in time for a certain time period. A more general definition would be that you are calculating the slope of the line you have fit through the data as the change-in-y divided by the change-in-x6.6 If you compare the two figure you should see that fitting a linear regression is an oversimplification but it will allow us to make a quantitative comparison\nWe can visualize this by adding a layer to our figure using geom_smooth() and setting the method to lm (linear regression).\n\nggplot(temperature, aes(x = Year, y = as.numeric(`J-D`))) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", size = 1, se = FALSE) +\n  labs(x = \"year\", y = \"temperature anomaly [C]\",\n       title = \"Annual mean global surface temperature relative to 1951-1980 average.\",\n       caption = \"source: NASA Goddard Institute for Space Studies\")\n\nHere is what your figure will look like with that added layer.\n\n\n\n\n\nFigure 13.2: Linear regression (red) for change in annual mean global surface anomaly from 1880-2021 (blue). Temperature anomaly measured relative to 1951-1980.\n\n\n\n\nThat’s helpful in terms of a visualization but for a to really quantitatively assess the rate of change, we need to fit a linear regression as y = mx + b. With m as the slope and b as the intercept to determine the rate of change; with that equation we can then determine the rate of change by extracting m. The larger the slope m, the steeper the fitted line and the more rapid the change in temperature.\nWe can fit the linear regression using the function lm().\n\n# fit linear regression\nscore_model &lt;- lm(`J-D` ~ Year, data = temperature)\n\n# view summary of results\nsummary(score_model)\n\n\nCall:\nlm(formula = `J-D` ~ Year, data = temperature)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36171 -0.13297 -0.02646  0.12807  0.45284 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.515e+01  7.106e-01  -21.32   &lt;2e-16 ***\nYear         7.797e-03  3.641e-04   21.41   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1798 on 141 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7648,    Adjusted R-squared:  0.7631 \nF-statistic: 458.5 on 1 and 141 DF,  p-value: &lt; 2.2e-16\n\n\nWe are interested in the equation of the regression line, how well the data fits the line, and whether or not the regression is significant. We can use the output of the linear regression to determine that. The estimate columns shows the values for the intercept b and the slope m for the variable (in this case year).\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your output from the linear regression model and determine what the equation for our line of best fit looks like and use this to determine the rate of change, including what the units would be.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nThe adjusted R2 value describes the proportion of variance of the dependent value explained by the independent variable. In our figure what is the dependent and the independent value? How well does the regression fit our data?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nInterpret the results of your analysis to determine whether you have evidence of a warming earth.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\n\nIdentify the time period where you observe the steepest rate of increase.\nCreate a subset of the data set to contain only that time frame.\nPlot you subset of data with a linear regression.\nPerform a statistical analysis to determine the rate of change.\nSummarize your results and compare them to your results above.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nThe time period with steep(est) rate of increase I selected is XXX - XXX.\n\n# create subset of data\n\n\n# Plot subset of data with a linear regression\n\n\n# linear regression\n\n[Summarize your results and compare them to your results above.]"
  },
  {
    "objectID": "13_greenhouse-gases.html#rate-of-change-of-atmospheric-co2",
    "href": "13_greenhouse-gases.html#rate-of-change-of-atmospheric-co2",
    "title": "13  Climate change: Drivers",
    "section": "13.3 Rate of change of atmospheric CO2",
    "text": "13.3 Rate of change of atmospheric CO2\nNext, let’s determine the rate of change of atmospheric CO2.\nDr. Charles David Keeling (1928 - 2005) began collecting atmospheric CO2 concentration data at the Mauna Observatory (Hawaii); this data set comprises the longest measurement of atmospheric CO2 concentrations7. This data set has been fundamental to our understand the role of human activities such as fossil fuel burning in driving climate change.7 Longest measurement using instruments - we will see later that we can use proxy data to indirectly measure CO2 levels for much longer time periods\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly argue why Hawaii is a good location for a long-term monitoring station for CO2.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWe can access the data set directly from the Global Monitoring Laboratory. Select the Data tab, then download the csv data set Mauna Loa CO2 annual mean data.\nOnce you have downloaded the data set, make sure to move it to the data folder of your project directory, then read it into Rstudio using the follow code8.8 If you look at the raw data using a text editor you will quickly see why we need to include the skip = 55 argument\n\nCO2 &lt;- read_delim(\"data/co2_annmean_mlo.csv\", delim = \",\", skip = 55)\n\nhead(CO2)\n\n# A tibble: 6 × 3\n   year  mean   unc\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1959  316.  0.12\n2  1960  317.  0.12\n3  1961  318.  0.12\n4  1962  318.  0.12\n5  1963  319.  0.12\n6  1964  320.  0.12\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nExplore your data set to determine if it is correctly formatted and briefly describe what information it contains - bullet points are fine.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPlot the change in CO2 over time and use a linear regression to determine the rate of change in atmospheric CO2 over the entire data set based on your analysis. Briefly summarize your results and argue how confident you are in these results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n# plot the change over time \n\n\n# perform linea regression\n\n[Your results here]\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nCompare your results of rates of change of average global temperature and atmospheric CO2 and describe the phenomenon that can be used to explain this.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\n\n\n\n\n\nFigure 13.3: Comparison of increase in mean global temperature anomaly relative to 1951-1980 (left) and increase in mean atmospheric CO2 concenttrations (right panel) measured a Mauna Loa Observatory. Regression lines indicating temporal trend included in red."
  },
  {
    "objectID": "13_greenhouse-gases.html#comparison-of-current-and-pre-historic-rates-of-change",
    "href": "13_greenhouse-gases.html#comparison-of-current-and-pre-historic-rates-of-change",
    "title": "13  Climate change: Drivers",
    "section": "13.4 Comparison of current and pre-historic rates of change",
    "text": "13.4 Comparison of current and pre-historic rates of change\nThe most recent IPCC assessment has labeled the increase in temperatures driving contemporary climate change as “unprecedented”. However, temperatures have changed in earth’s past, and temperatures have even been higher than what we are experiencing now. What is unprecedented is the rate at which this is occurring, at least according to the IPCC.\nShall we investigate?\nTo do this we will need to look at past climate change. The two data sets we just took a look at are measurements of temperature and CO2 using instrumentation, i.e. we have directly measured values for the parameters we are interested in at different points in time. Dr. Keeling was one of the first scientists to consider the importance of long-term monitoring sites. The rapid changes taking place in our environment have created a push to generate long-term data sets with a focus on making the accessible. We will take a look at some of these data sets later in this semester and you will likely use at least one of them for your own data science project.\nHowever, we frequently have questions that might extend beyond data sets like the two we used above.\nHow can we access data from before we had instrumentation? One way to do this we have to use so-called proxy data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nIn general, a proxy is an intermediary or subsistute, i.e. it is a parameter that can be used to represent the value of something in a calculation. Paleoclimatologists use preserved physical characteristics of the environment to stand in for direct measurements using instruments, typical examples would be ice cores, tree rings, ocean sediments, or fossil pollen. Briefly discuss the pros and cons of proxy data compared to direct measurements using specialized instruments.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nFor a deep dive on proxy data sets and climate archives, you can check out a detailed interactive brief on proxy data.\nFor our assessment we are going to explore proxy data derived from ice cores which next to ocean sediments give us some of the longest records of past climate conditions.\nHundreds of ice cores extracted from polar ice have proven valuable to understanding changes in atmospheric chemistry over pre-historic time. Here, we can make use of the fact that as the ice is formed, air bubbles are trapped. Because these air bubbles have remained frozen, they still have the same composition of gases as at the time they were trapped. The depth of an ice core is correlated to time, deeper ice is older. In other words, ice cores form an archive of atmospheric conditions over time. We can directly measure CO2 from the air bubbles trapped in the ice and we can measure isotopic ratios of oxygen in the water molecules of the core to derive temperature.\nVostok Ice core data set has been constructed using ice cores from the Vostock research station in the Antarctica and can be access through the Carbon Dioxide Information Analysis Center.\nLet’s start by taking a look at the temperature data. Use the code below to read the data set that has been downloaded and placed in the data folder for you into R as a data frame9.9 We are using a slightly different method from before which allows us to directly download the data into our data folder. We are also using read_table2() from the readr package due to the fact that our text file is formatted using neither white space nor tabs.\n\n# load dataset\nvostok_temp &lt;- read_table2(\"data/vostok_temp.txt\",\n                     skip = 60,\n                     col_names = c(\"depth\", \"age_ice\", \"deuterium\", \"temperature\"))\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at the data set and briefly describe what data is contained in the data set (you may want to take a peak at the original text file to get a better understanding).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow we can create a plot of the temperature data over time. The age is recorded as years before present. For better visualization we will convert this to “thousand years ago” by dividing that number by 1,000.\n\nggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature variation during glacial/interglacial periods\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n\n\n\n\n\nFigure 13.4: Temperature variation during glacial/interglacial periods derived from air bubbles in Vostock ice cores.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we move at looking at rates of change, we first need to determine how to properly read this plot. Think through the following aspects:\n\nWhat does it mean that time (on the x-axis) is represented as “thousand years ago” or “time before present”? How does this differ from the other time series we have plotted today?\nTemperature is being measured by proxy by looking at differences in isotope ratios; the data file lists this information as “Temperature variation”. What does 0C mean on this plot?\nConsider how long glacial and interglacial periods typically last - are we currently in a glacial or interglacial period?\nIn what parts of the figure is temperature increasing/decreasing (consider slope)?\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow add a trend line to our ice core temperature data and run a linear regression model. Argue whether or not you think this trend line is a good representation of long-term temperature change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nPlot temperature data derived from vostok ice cores.\n\nggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature derived from Vostok ice core, Antarctica.\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n\n\n\n\n\nFigure 13.5: Change in temperature during glacial/interglacial periods derived from Vostok ice cores (blue) with fitted linear regression (red).\n\n\n\n\nAnd then we still need to fit a linear regression.\n\n# fit linear regression\nscore_model &lt;- lm(temperature ~ age_ice, data = vostok_temp)\n\n# view summary of results\nsummary(score_model)\n\n\nCall:\nlm(formula = temperature ~ age_ice, data = vostok_temp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7781 -2.3667 -0.5821  1.9830  7.7603 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.631e+00  8.462e-02 -54.725   &lt;2e-16 ***\nage_ice      7.850e-07  4.987e-07   1.574    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.896 on 3309 degrees of freedom\nMultiple R-squared:  0.0007483, Adjusted R-squared:  0.0004463 \nF-statistic: 2.478 on 1 and 3309 DF,  p-value: 0.1155\n\n\nRemember, when you “argue” an answer you need to state your conclusion and then support that statement.\n\n\n\nYou’re right - to determine if the currently observed rate of change is “unprecedented” or not, we need to identify past time periods with the fastest rate of change and calculate them.\n\n\nYou’ve already used plotly in a previous chapter to plot an interactive graph. This will make it a lot easier to identify specific time periods because as you hover over any part of the line graph the pop up will give you the data points. Previously we wrapped an entire function in the ggplotly() function. In this case, it is easier to first create an object that holds the ggplot() output and then use that as the argument for ggplotly().\nTo do this I will show you a little trick to plot an interactive figure:\n\np1 &lt;- ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = .75) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\nggplotly(p1)\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow you can use your cursor to identify individual points on the plot to select the subset you want plot. The plot that subset with a linear best line of fit and run a linear regression to get the slope to compare to current rates of change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere’s what your code could look like, we’ve previously used the technique where you create and object to hold your variables to make it easier to resue the code.\n\n# define time range\nmin_year_vostok &lt;- \nmax_year_vostok &lt;- \n\n# filter data set + plot\nvostok_temp_subset &lt;- vostok_temp %&gt;%\n  filter(age_ice &gt;= min_year_vostok & age_ice &lt;= max_year_vostok)\n\nggplot(vostok_temp_subset, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature change recorded in Vostok ice core (128357 - 138193 years ago).\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n# fit linear regression\nscore_model &lt;- lm(temperature ~ age_ice, data = vostok_temp_subset)\n\n# view summary of results\nsummary(score_model)\n\n\n\n\n\n\nFigure 13.6: Temperature trends (linear regression, red) recorded in Vostok ice core for time period from approx. 138 - 128 kyrs before present.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIdentify two more ranges with increasing temperatures and determine the rate of change during that time period.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow, let’s take a look at past patterns of changes in atmospheric CO2 over time.\n\n# load dataset\nvostok_ice &lt;- read_delim(\"data/vostok_ice.txt\", delim = \"\\t\",\n                     skip = 21,\n                     col_names = c(\"depth\", \"age_ice\", \"age_air\", \"CO2\"))\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSelect one time period with rapid increase in CO2 concentrations and apply what you have learned identifying past periods of rapid temperature increase to calculating the rate of change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "13_greenhouse-gases.html#solution-4",
    "href": "13_greenhouse-gases.html#solution-4",
    "title": "13  Climate change: Drivers",
    "section": "13.5 Solution",
    "text": "13.5 Solution\nHere’s what your code could look like, we’ve previously used the technique where you create and object to hold your variables to make it easier to resue the code.\n\n# define time range\nmin_year_vostok &lt;- \nmax_year_vostok &lt;- \n\n# filter data set + plot\nvostok_temp_subset &lt;- vostok_temp %&gt;%\n  filter(age_ice &gt;= min_year_vostok & age_ice &lt;= max_year_vostok)\n\nggplot(vostok_temp_subset, aes(x = age_ice/1000, y = temperature)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"thousand years before present\", y = \"Temperature variation [C]\",\n       title = \"Temperature change recorded in Vostok ice core (128357 - 138193 years ago).\",\n       caption = \"Data: Carbon Dioxide Information Analysis Center (CDIAC)\")\n\n# fit linear regression\nscore_model &lt;- lm(temperature ~ age_ice, data = vostok_temp_subset)\n\n# view summary of results\nsummary(score_model)\n\n\n\n\n\n\nFigure 13.6: Temperature trends (linear regression, red) recorded in Vostok ice core for time period from approx. 138 - 128 kyrs before present.\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIdentify two more ranges with increasing temperatures and determine the rate of change during that time period.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNow, let’s take a look at past patterns of changes in atmospheric CO2 over time.\n\n# load dataset\nvostok_ice &lt;- read_delim(\"data/vostok_ice.txt\", delim = \"\\t\",\n                     skip = 21,\n                     col_names = c(\"depth\", \"age_ice\", \"age_air\", \"CO2\"))\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSelect one time period with rapid increase in CO2 concentrations and apply what you have learned identifying past periods of rapid temperature increase to calculating the rate of change.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "13_greenhouse-gases.html#final-conclusions",
    "href": "13_greenhouse-gases.html#final-conclusions",
    "title": "13  Climate change: Drivers",
    "section": "13.5 Final Conclusions",
    "text": "13.5 Final Conclusions\nNow it’s time to put everything together. You might want to refer back to the beginning of this section when we looked at some background information about the IPCC report, the atmospheric energy budget, formulated our central questions, thought about what data sets when can use to answer our question, and the limitations of our approach.\n\n\n\n\n\n\n Consider this\n\n\n\nBefore we summarize and then interpret our results, let’s re-orient ourselves to what we’ve done with this analysis.\n\nWrite out the central question(s) we are asking.\nList the data sets you used to investigate and what metric you calculated for each.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nWell-written papers frequently end their introduction section with a paragraph that summarizes what their study is investigating and how they are investigating that (set of) questions - it forms a “bridge” between the introduction that sets up relevant background information (why is my question important?) and the methods section which is a very detailed account of how data was acquired (experimental design), processed, and analyzed.\nIf you read several of these paragraphs you will realize that they all contain a statement that follows a general formula along these lines:\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF THE TYPE OF DATA SET GENERATED] to [METRICS THAT WERE CALCULATED].\n\nYou should always be able to make a 2-3 sentence statement summarizing what you are investigating and how you did it, it’s a good self-check that you know what you’re trying to accomplish.\n\n\n\n\n\n\n Consider this\n\n\n\nUse your results to compare current changes over the last approx. 200 years in atmospheric and CO2 concentrations and global temperature to pre-historic changes.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nBe strategic in how you structure your answer to put together all the parts of our analysis. For example, start with temperature and compare the rate of change you calculate for recent times to the rate of change you calculated during the more distant past (glacials/interglacials)10. In include your own rate of change + the answers your classmates posted in the slack channel. Then make a statement about whether or not we are currently observing an “unprecedent” change. Then do the same for CO2 concentrations. At this point you are just summarizing and describing your results, you can make clear statements of whether or not rates are positive/negative (your variables are increasing/decreasing), and whether you see the same/opposite trends but you should not yet interpret what those results mean. This would be equivalent to the results section of a lab report or research paper.\n[Your answer here]\n\n\n10 Remember to include units!\n\n\n\n\n\n Consider this\n\n\n\nInterpret your results to assess whether recent changes in temperature are due to natural vs anthropogenic factors. For your answer consider both arguments that attribute the change to anthropogenic factors and natural fluctuations.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nNow you are entering the interpretation/discussion component of your analysis. At this point your now what your key results are and you need to figure out what they mean; essentially you are finally getting around to answering those key questions you asked before designing your experiment/study. You are going to use your results as evidence for/support of your conclusion. This means that you need to demonstrate that your results are consistent with your conclusion. You also want to consider alternative explanations, limitations of your data etc. Of course, ideally you can exclude those based on e.g. evidence from other studies/data sets.\n[Your answer here]"
  },
  {
    "objectID": "13_greenhouse-gases.html#acknowledgments",
    "href": "13_greenhouse-gases.html#acknowledgments",
    "title": "13  Climate change: Drivers",
    "section": "13.6 Acknowledgments",
    "text": "13.6 Acknowledgments\nThese activities are based on the EDDIE Climate Change Module1111 O’Reilly, C.M., D.C. Richardson, and R.D. Gougis. 15 March 2017. Project EDDIE: Climate Change. Project EDDIE Module 8, Version 1."
  },
  {
    "objectID": "D_clim-change.html#investigating-climate-change",
    "href": "D_clim-change.html#investigating-climate-change",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Investigating Climate Change",
    "text": "Investigating Climate Change\nThe most recent IPCC Asessement has made two very clear statements regarding climate change:\n\nClimate change is “unequivocally” caused by humans.\nThe impact of anthropogenic climate change is “unprecedented”.\n\nBased on their assessment of studies having looked at global warming and climate change to date, the global mean warming is estimated at 1.1°C compared to the pre-industrial record. Additionally, under every emission scenario and average warming of 1.5 °C is likely to be reached in in next 20 years. Finally, some effects such as melting glaciers, ice sheets and the permafrost have been deemed irreversible.\nIn this module we are going to learn about data visualization using ggplot2 as we explore drivers and impacts of climate change. For our exploration of the drivers of climate change we will pull data sets describing temperature and greenhouse gas concentrations past and present. As we explore the impacts, we will consider whether the effects of climate change are something we will observe in the future if no action is taken or if we are already experiences widespread impacts changing the earth-climate system in ways that we can currently observe. And while we’re on the topic of visualizations and climate change we will then also explore how visualizations are used for misinformation."
  },
  {
    "objectID": "D_clim-change.html#the-earth-climate-system",
    "href": "D_clim-change.html#the-earth-climate-system",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The earth-climate system",
    "text": "The earth-climate system\nThe Earth-climate system, also known as the Earth’s climate system, refers to the complex and interconnected set of physical, chemical, biological, and atmospheric processes that govern the Earth’s climate. It describes all the components and interactions that determine the planet’s climate patterns and conditions over time.\nThe climate system is comprise of five major components: The hydrosphere, cryosphere, atmosphere, lithosphere, and biosphere.\n\nThe atmosphere consists of a mixture of gases, primarily nitrogen and oxygen, along with trace gases like carbon dioxide (CO2), methane (CH4), and water vapor (H2O). These gases play a critical role in regulating the planet’s temperature through the greenhouse effect.\nThe hydrosphere encompasses all of Earth’s water, including oceans, lakes, rivers, glaciers, and groundwater. The movement and distribution of water in the hydrosphere have a significant impact on climate patterns and weather events.\nThe cryosphere includes all of Earth’s frozen water, such as polar ice caps, glaciers, and permafrost. Changes in the cryosphere, such as melting ice, can have profound effects on sea levels and regional climate.\nThe lithosphere refers to Earth’s solid outer layer, including the continents, ocean floors, and the Earth’s crust. It plays a role in the redistribution of heat and the formation of land forms that can influence climate patterns.\nThe biosphere consists of all living organisms on Earth, including plants, animals, and microorganisms. Biological processes, such as photosynthesis and respiration, influence the composition of greenhouse gases in the atmosphere and affect climate.\n\nYou can think of the biosphere as the global ecosystem composed of all living organisms and the abiotics factors they derive energy and nutrients from. Another way to think of it is all the regions of the lithosphere, atmosphere, hydrosphere, and cryosphere occupied by living organisms. The fact that it is comprised of living organisms (biotic factors) sets it apart from the other components of the earth climate system.\nOverall, the climate system is an interactive system acted on by internal and external forcing mechanisms.\nThe components of the climate system are open systems with the freedom to exchange mass, heat, and momentum. For example the ocean and atmosphere exchange gases like carbon dioxide as the ocean acts a large carbon sink. Similarly, we observe an exchange of mass through the water cycle that links the atmosphere and hydrosphere through processes such as evaporation and condensation/precipitation. Even here in New England you can observe the exchange of heat as summer (air) temperatures cause the ocean to warm (minimally). Finally, surface waves are the result of the exchange of momentum as the wind causes the surface waters to move.\nNext to these internal mechanisms of the different components of the earth climate system impacting each other from within, the earth climate system is additionally impacted by external factors, primarily solar radiation. For example, the output of the sun heats the hydropshere and atmosphere. As the lithosphere encompasses the rigid outer part of the earth consisting of the crust and upper mantle, plate tectonics and volcanic eruptions are frequently also considered external mechanisms."
  },
  {
    "objectID": "D_clim-change.html#climate-regimes",
    "href": "D_clim-change.html#climate-regimes",
    "title": "Climate Change: Drivers & Impacts",
    "section": "Climate regimes",
    "text": "Climate regimes\nWeather is the condition of the atmosphere for a specific time & place – climate is a long-term statistical portrait of a specific place, region, or the entire planet.\nWeather is a snapshot of atmopsheric conditions at a specific time for a specific place. It is directly observable and can be broken down into readily measureable, discrete characteristics including temperature and precipitation but also extending to include among others wind speed and direction, cloud cover and type, visibility, or air pressure.\nBy contrast, climate comprises the statistical averages of weather of long-term timescales & involves behavior of entire complex earth system. Generally, climate refers to the long-term patterns and average weather conditions along with extremes in a specific place, region or on Earth as a whole based on a single or multiple stations (locations). It represents the statistical summary of weather patterns over an extended period, typically 30 years or more.\n\n\n\nChanges in weather, climate variability, and climate change occur on very different time scales.\n\n\nBoth weather and climate do vary over time for natural reasons but on very different time scales. While weather can change at a moment’s notice2, climate variability describes (natural) shifts in climate conditions on decadal time scales. Finally, climate change describes long-term changes on scales of centuries to millenia.2 And as we all know the length of the “moment’s notice” is inversely proportional to the probability of you wearing an umbrella/having a jacket with you."
  },
  {
    "objectID": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "href": "D_clim-change.html#the-energy-budget-and-global-temperatures",
    "title": "Climate Change: Drivers & Impacts",
    "section": "The energy budget and global temperatures",
    "text": "The energy budget and global temperatures\nTemperature is a primary determinant of climate. Overall, earth maintains a stable average temperature (climate) by balancing energy received from the sun with energy emitted by earth back into space. Global temperature is a function of how much energy the earth receives and stores which in turn is influenced by three major factors:\n\nThe amount of energy received from the sun.\nReflection of energy by earth’s surface.\nAtmospheric composition (greenhouse gas effect).\n\nThe Earth’s energy budget is a concept that describes the balance between the incoming energy from the Sun and the outgoing energy radiated back into space from the Earth. It provides a framework for understanding how energy flows into and out of the Earth’s climate system. The Earth’s energy budget is essential for maintaining the planet’s temperature and climate.\n\n\n\nThe atmospheric energy budget source: weather.gov\n\n\nA material may transmit, reflect, emit or absorb radiation, and generally does more than one of these at a time. Earth’s energy budget consists of two different form of radiation\n\nIncoming shortwave radiation from the sun (Insolation): This is the energy received from the Sun. Sunlight, or solar radiation, is the primary source of energy for the Earth’s climate system. It includes both wavelengths in the visible and non-visible range, primarily UV.\nOutgoing longwave radiation: As the Earth’s surface and atmosphere absorb solar energy, they emit heat in the form of infrared radiation. This outgoing longwave radiation is a crucial part of the Earth’s energy budget.\n\nWhile some gases such as Ozone absorb shortwave (UV) radiation, Greenhouse gases such as water vapor, CO2, and Methane are defined by their property that they transmit short-wave radiation but absorb longwave radiation. This means that the greenhouse gases let through the incoming solar radiation but absorb large parts of the longwave radiation being emitted from earth’s surface. This so called greenhouse effect is a crucial component of the Earth’s climate system as it turns the atmosphere into a “warm blanket”. Ultimately, the energy absorption by the atmosphere stores more energy near the earth’s surface than if there was no atmosphere, making life on the planet possible in the first place.\nHowever, human activities have led to an enhance greenhouse effect. The reason why the IPCC describes current climate change as anthropogenic is that increasing levels of atmospheric CO2 and other greenhouse gases since the Industrial Revolution are driving the rapid increase in temperatures. Earth absorbs incoming solar radiation at its surface and emits long-wave radiation to maintain the energy balance at the surface. Only as small portion of that emitted radiation goes directly into space, most of it is absorbed by greenhouse gases (e.g. CO2) in the atmosphere. For the atmosphere to maintain its energy balance it emits radiation to space and back to earth. With increasing concentrations of GHG, the atmosphere absorbs and re-emits increasingly more energy. This creates an imbalance at earth’s surface and as a response earth continues to emit more energy to re-balance the budget and as a result global temperatures increase.\nIn this module we will look at several data sets that support the fact that the currently observed climate change is indeed unprecedented in the rate of change, that it is correlated to rapidly increasing greenhouse gases and therefore consistent with the description of being unequivocally human-caused and that the impacts of climate change are currently being observed across the earth climate system in a manner consistent with rapidly increasing global temperatures."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "href": "C_bioinformatics-eDNA.html#genetic-barcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Genetic barcoding",
    "text": "Genetic barcoding\nGenetic barcoding, also known as DNA barcoding, is a molecular biology technique used to identify and distinguish between different species of organisms by analyzing a short and standardized DNA sequence. This method is based on the idea that each species has a unique DNA barcode or genetic signature within a particular region of its genome and is particularly useful for rapid species identification, especially when dealing with complex or difficult-to-identify specimens even from small, damaged, or industrial processed material.\nGenetic barcoding relies on the ability to amplify a standard gene (locus) across a wide range of taxonomic groups using universal primers. A good barcoding locus is typically a short, conserved section of DNA that contains enough genetic variation to differentiate between species (intraspecific variance) but remains relatively constant within a species (little to no intraspecific variance). The mitochondrial cytochrome c oxidase subunit 1 (COI) gene is a commonly used barcode region in animals, while other genes or regions may be used for plants, fungi, and microorganisms. The data set we will look at is fungi for which ITS2 is commonly used.\nRegardless of which genetic region is actually implemented the key steps remain the same:\n\nDNA Extraction: Genetic material (usually DNA) is extracted from the biological sample of interest, this can be tissue, cells, or even environmental DNA (eDNA) extracted from water, soil, or other sources.\nPCR Amplification: Polymerase chain reaction (PCR) is employed to selectively amplify the barcode region from the extracted DNA. Generally, a set of universal primers are used that will amplify in a wide range of taxonomic groups.\nSequencing: The PCR-amplified DNA fragments (Amplicons) are subjected to DNA sequencing, typically using Sanger sequencing. The resulting sequence data contain the barcode information.\nComparison to reference database: The obtained DNA barcode sequence is compared to a reference database containing sequences from known species. Bioinformatics tools and algorithms are used to search for matches or close matches in the database.\nSpecies Identification: Based on the comparison results, researchers can identify the species of the specimen. If the sequence closely matches a known barcode sequence in the database, the specimen can be confidently identified."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#metabarcoding",
    "href": "C_bioinformatics-eDNA.html#metabarcoding",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Metabarcoding",
    "text": "Metabarcoding\nMetabarcoding of samples with mixed DNA templates allows us to characterize biological communities. Barcoding generally required there to be only one species present in the extracted DNA in order to get a clean sequence for comparison and taxonomic assignment. However, for many applications it would be useful to amplify and sequence DNA that potentially contains DNA from multiple species. Advances in sequencing technology (high throughput sequencing and next generation sequencing) has allowed us to perform metabarcoding studies for which the same steps apply except that during sequencing a large number of reads are produced. These sequences then need to be analyzed to identify the unique sequences present in the data set and then those are matched to a database."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#edna",
    "href": "C_bioinformatics-eDNA.html#edna",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "eDNA",
    "text": "eDNA\nEnvironmental DNA is DNA captured from an environmental sample without the need for pre-isolating specific targets. Macroorganisms shed DNA as cellular or extracellular material into the environment. Typical sources include mucous, the excretion of bodily fluids (feces, urine), and the sloughing off of skin cells, scales or other tissue. This means that we can capture DNA from an environmental sample without pre-isolating specific target organisms. For example, we take a water sample (1 – 5l) and then use a nitrocellulose filter to trap the DNA and then extract that DNA using very straightforward protocols.\n\n\n\n\n\n\n Consider this\n\n\n\nEnvironmental samples cover a wide range of ecosystems and habitats and spatiotemporal scales. Use examples to describe this variety of samples that can be used.\n\n\nGenerally, we refer to environmental DNA as DNA that is a “trace” of an organism in the environment, not the organisms itself. However, in some cases the same methods used to characterize biological communities using environmental DNA are applied to community DNA.\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast eDNA and community DNA and argue whether you would consider a gut content analysis to be eDNA or community DNA.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nDNA can be isolated from bulk-extracted from mixtures of organisms isolated from an environmental sample (Community DNA).\n\n\n\neDNA has high potential as a complementary method to characterize and monitor biological communities but users must be aware of biases and caveats to analyze the resulting data sets in a meaningful way.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the potential of eDNA in terms of applications and contrast this with some of the challenges that still need to be overcome."
  },
  {
    "objectID": "C_bioinformatics-eDNA.html#our-data-set",
    "href": "C_bioinformatics-eDNA.html#our-data-set",
    "title": "Bioinformatics and Assessment of Biological Communities using eDNA",
    "section": "Our Data set",
    "text": "Our Data set\nThe data set that we will be exploring used metabarcoding to characterize fungi communities in soil samples taken from different depths (soil horizons) in forests dominated by different trees and their mutualistic mycorrhizal fungi to explore how this affects resource competition with free-living saptrotrophs.\nHere is the abstract from (Carteron et al. 2021).\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7.\n\nIn temperate and boreal forests, competition for soil resources between free-living saprotrophs and ectomycorrhizal (EcM) fungi has been suggested to restrict saprotrophic fungal dominance to the most superficial organic soil horizons in forests dominated by EcM trees. By contrast, lower niche overlap with arbuscular mycorrhizal (AM) fungi could allow fungal saprotrophs to maintain this dominance into deeper soil horizons in AM-dominated forests. Here we used a natural gradient of adjacent forest patches that were dominated by either AM or EcM trees, or a mixture of both to determine how fungal communities characterized with highthroughput amplicon sequencing change across organic and mineral soil horizons. We found a general shift from saprotrophic to mycorrhizal fungal dominance with increasing soil depth in all forest mycorrhizal types, especially in organic horizons. Vertical changes in soil chemistry, including pH, organic matter, exchangeable cations, and extractable phosphorus, coincided with shifts in fungal community composition. Although fungal communities and soil chemistry differed among adjacent forest mycorrhizal types, variations were stronger within a given soil profile, pointing to the importance of considering horizons when characterizing soil fungal communities. Our results also suggest that in temperate forests, vertical shifts from saprotrophic to mycorrhizal fungi within organic and mineral horizons occur similarly in both ectomycorrhizal and arbuscular mycorrhizal forests."
  },
  {
    "objectID": "02_install-R.html#learning-objectives",
    "href": "02_install-R.html#learning-objectives",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nAfter completing this activity you should\n\nbe able to download and install R and Rstudio on your laptop\nbe able to install Rtools & devtools to be able to compile R packages from source (Windows).\nunderstand the main use for each of the four main panes in the Rstudio GUI.\nunderstand what a package is in R and how to install them."
  },
  {
    "objectID": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "href": "02_install-R.html#install-set-up-r-and-rstudio-on-your-computer",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.2 Install & Set up R and Rstudio on your computer",
    "text": "2.2 Install & Set up R and Rstudio on your computer\nIf you have already installed R and Rstudio make sure your R version is up to date. Whenever you open Rstudio the version will be printed in the console (bottom left pane). In addition, you can always check what version is installed by typing sessionInfo() into your console. You should be using version 4.0.0 or later. You do not need to uninstall old version of R. If you do have to update, you will need to re-install packages (see below) for R4.0.0\n\n2.2.1 Windows\nInstall R\n\nDownload most recent version of R for Windows here.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall Rtools\n\nDownload Rtools here.\nRun the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\nInstall Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Windows XP/Vista/7/8/10.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nFinish setting up Rtools\n\nOpen Rstudio to make sure you aren’t getting any error messages.\nPut Rtools in your path by typing writeLines('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', con = \"~/.Renviron\") in the console window.\nInstall the devtools package by typing install.packages(\"devtools\") in the console.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard.\n\n\n2.2.2 Mac OS X\nDownload & install R\n\nGo to (CRAN)[http://cran.r-project.org/], select Download R for (Mac) OS X.\nDownload the .pkg file for your OS X version.\nRun the downloaded file to install R.\n\nDownload & install XQuartz (needed to run some R packages)\n\nDownload XQuartz\nRun the downloaded file to install\n\nDownload & install Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Mac OS X.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall quarto\nDownload quarto using this link. Pick the file according to your operating system Run the downloaded .exe file that was download and follow the instructions in the set-up wizard."
  },
  {
    "objectID": "02_install-R.html#get-to-know-rstudio",
    "href": "02_install-R.html#get-to-know-rstudio",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.3 Get to know Rstudio",
    "text": "2.3 Get to know Rstudio\nRstudio is an Integrated Development Environment (IDE) that you can use to write code, navigate files, inspect objects, etc. The advantage of using an IDE is that you have access to shortcuts, visual cues, troubleshooting, navigation, and autocomplete help.\n\n2.3.1 GUI Layout\nGUI stands for graphic user interface and refers to a type of user interface that allows users to interact with software applications and electronic devices through visual elements such as icons, buttons, windows, and menus, rather than using text-based command-line interfaces.\nYou have probably mostly interacted with computer programs through a GUI, where you interact with the system by manipulating graphical elements using a pointing device like a mouse, touch screen, or stylus. GUIs provide a more intuitive and user-friendly way for individuals to interact with computers and software because you can “see” what the effect of what you are doing is having. Graphical User Interfaces are a major departure from earlier text-based interfaces like command-line interfaces. They have contributed significantly to the widespread adoption of computers and software by making them more accessible to a broader range of users. GUIs are used in various types of software, from operating systems to applications like web browsers, image editors, word processors, and more.\nNot too long ago, if you had wanted to learn R or another programming language you would have been working directly on a console instead of an IDE like Rstudio which has made coding a lot more accessible to beginners because you can more easily use scripts, interactively run code and visualize data.\n\n\n\n\n\n\nNote\n\n\n\nUse this link to access an Rstudio IDE Cheatsheet pointing out the key features using annotated impages of the different panes. You can also download a pdf version and keep a printout handy as you get used to the GUI.\n\n\nOpen Rstudio and identify the four panes in the interface (default layout).\n\nEditor (top left): edit scripts/other documents, code can be sent directly to the console.\nR console (bottom left): Run code either by directly typing the code or sending it from the editor pane.\nEnvironment/history (top right): Contains variables/objects as you create them & full history of functions/commands that have been run.\nfiles/plots/packages/help/viewer (bottom right): Different tabs in this pane wil let you explore files on your computer, view plots, loaded packages, and read manual pages for various functions.\n\nThe panes can be customized (Rstudio -&gt; Preferences -&gt; Pane Layout) and you can move/re-size them using your mouse.\n\n\n\n\n\n\nNote\n\n\n\nWe are going to switch to have the Console in our top right and the Environment in the bottom left which makes it easier to see your code output and your script/quarto document at the same time.\n\n\n\n\n2.3.2 Interacting with R in Rstudio\nThink of R as a language that allows you to give your computer precise instructions (code) to follow.\n\nCommands are the instructions we are giving the computer, usually as a series of functions.\nExecuting code or a program means you are telling the computer to run it.\n\nThere are three main ways to interact with R - directly using console, script files (*.R), or code chunks embedded in R markdown (*.Rmd) or quarto files (*.qmd). We will generally be working with the later.\nThe console is where you execute code and see the results of those commands. You can type your code directly into the console and hit Enter to execute it. You can review those commands in the history pane (or by saving the history) but if you close the session and don’t save the history to file those commands will be forgotten.\nBy contrast, writing your code in the script editor either as a standard script or as a code chunk in an quarto document allows you to have a reproducible workflow (future you and other collaborators will thank you).\nExecuting an entire script, a code chunk, or individual functions from a script will run them in the console.\n\nCtrl + Enter will execute commands directly from the script editor. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute.\nIf you are using a quarto file you can execute an entire code chunk by pressing the green arrow in the top right corner.\n\nIf the console is ready for you to execute commands you should see a &gt; prompt. If you e.g. forget a ) you will see a + prompt - R is telling you that it is expecting further code. When this happens and you don’t know what you are missing (usually it is an unmatched quotation or parenthesis), make sure your cursor is in the console and hit the Esc key.\n\nWe will run through these options, but you can always check back here while you are getting used to R.\n\n\n\n2.3.3 Customize Rstudio\nThere are several options to customize Rstudio including setting a theme, and other formatting preferences. You can access this using Tools &gt; Global Options. I recommend using a dark theme (it’s a lot easier on the eyes) and keeping the panes in the same positions outlined above because it will make troubleshooting a lot easier1.1 “You should see xx in the top left” is a lot more helpful if your top left looks like my top left!"
  },
  {
    "objectID": "02_install-R.html#installing-and-using-packages-in-r",
    "href": "02_install-R.html#installing-and-using-packages-in-r",
    "title": "2  Getting set up with R and Rstudio",
    "section": "2.4 Installing and using packages in R",
    "text": "2.4 Installing and using packages in R\n\n2.4.1 Install a package\nThink of R packages or libraries as tool kit comprising a set of functions (tools) to perform specific tasks. R comes with a set of packages already installed that gives you base R functions; you can view these and determine which have been loaded in the Packages tab in the bottom right pane. For other tasks we will need additional packages. 22 Most R packages are found in the CRAN repository and on Bioconducter, developmental packages are available on github.\nA central group of packages for data wrangling and processing form the tidyverse, described as “… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” - We are going to heavily rely on core functions from the tidyverse to wrangle, summarize, and analyze data.\nWhen you install packages they will be downloaded and installed onto your computer. Determine what your default path is using .libPaths() and change if necessary.\nThe easiest way to install packages directly in the console is to use the install.packages() function.\nUse the R console to install some libraries to get us started (we will install other libraries as needed for other labs).\n\n\nUsing # in an R script allows you to insert comments that are ignored by R when executing your code. Use comments to document your code, future you will thank you! Before submitting any of your skills tests or homework assignments you should always go through and make sure each piece of code has a descriptive comment. You do not need to add a comment for multi-line code that you are stringing together using a pipe %&gt;% but you should have one descriptive comment above the set of commands you are giving R and then make sure that you add any comments that you need to remember how the function works or which parameters might be useful to tweak/set differently if you were to reuse that code.\n\n# install central packages in the tidyverse\ninstall.packages(\"tidyverse\")\n\n# install additional packages\ninstall.packages(\"plyr\", \"ggthemes\", \"patchwork\", \"glue\")\n\nLet’s check if you were able to successfully install those packages by ensureing you can load them. Any time you start a new R session (e.g. by closing Rstudio and restarting it), you will need to load your libraries beyond the base libraries that are automatically loaded using the library() function in order to be able to use the functions specific to that package3.3 Troubleshooting tip: if you get an error along the lines of function() cannot be found the first thing you will want to do is check if your libraries are loaded!\n\n# load library\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIf you don’t see any error messages in the console along the lines of there is no package called ... you are all set. If you look in the packages tab in the lower right panel you should also see that packages such as dplyr and tidyr (two of the central tidyverse packages) now have a little check box next to them.\n\n\n2.4.2 Updating R packages\nYou should generally make sure to keep your R packages up to date as new versions include important bugfixes and additional improvements. The easiest way to update packages is to use the Update button in the Packages tab in the bottom right panel. Over the course of the semester you should not have to do this, but when you install new packages you might get message that some of your packages need to be updated which you can then either choose to do at that point or ignore.\n\n\n\n\n\n\nWarning\n\n\n\nBe aware that updating packages might break some code you have previously written. For most of what we will be doing this should not be the case. If you used R for a previous course, make sure to update you packages at the beginning of this course and we should be set for the semester."
  },
  {
    "objectID": "03_Rbasics.html#learning-objectives",
    "href": "03_Rbasics.html#learning-objectives",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\nAfter completing this tutorial you should be able to\n\nname, create, and assign values to objects.\nsave a series of commands/code as an R script.\nuse comments to describe your code/scripts.\ncall functions and modify the default options using their arguments.\nunderstand what a vector is and distinguish between the main data types.\nto inspect, subset, and extract their content from a vector.\nunderstand how data.frames and vectors relate."
  },
  {
    "objectID": "03_Rbasics.html#r-is-all-about-objects",
    "href": "03_Rbasics.html#r-is-all-about-objects",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.2 R is all about Objects",
    "text": "3.2 R is all about Objects\nYou can think of the R console as a super-powerful calculator. You can get output from R by simply typing math directly into the console.\n\n13 + 29\n\n[1] 42\n\n\nor\n\n546 / 13\n\n[1] 42\n\n\nWell that’s fun - but not super helpful in our context.\nIn the R programming language, an object is a fundamental concept used to store, manipulate, and represent data. Everything in R is treated as an object, whether it’s a number (numeric), a text string (character), a dataset (data.frame), or even more complex data structures.\nObjects in R can be created, modified, and used to perform various operations. Objects are assigned names that you can then use to reference them in your code. When you create an object, you’re essentially creating a container that holds a value or data.\nCreating an object is straightforward. First, we give it a name, then we use the assignment operator to assign it a value.\nThe assignment operator (&lt;-) assigns the value on the right of the &lt;- to the object on the left1.1 Start building good habits starting now in terms of your coding style. For example, your code is a lot more readable if you use white space to your advantage. For example, make sure you have a space before and after your &lt;-\n\nfork_length_mm &lt;- 344\n\nType that into the console and execute the command using Enter. If you look at your Global Environment (bottom left panel) you should now see forklength and the value you assigned it.\nNotice, how when you assigned a value to your new object nothing was printed in the console compared to when you were typing in math.\nTo print the value of an object you can type the name of the object into the console.\n\n# print value\nfork_length_mm\n\n[1] 344\n\n\nNow that fork_length_mm is in your environment we can use it to compute instead of the value itself.\nFor example, we might need to convert our fork length from millimeters (mm) to centimeters (cm).\n\nfork_length_mm / 10 \n\n[1] 34.4\n\n\nWe can change the value of an object any time by assigning it a new one. Changing the value of one object does not change the values of other objects.\n\nfork_length_mm &lt;- 567\n\n\n\n\n\n\n\n Give it a whirl!\n\n\n\nCreate a new object with the fork length in centimeters. Then change then change the value of our fork length in millimeters object to 50. What do you think the value of fork_length_mm will be now?\n\n\nSome initial thoughts on naming things22 You will soon discover that coding is 90% naming things.\nTheoretically, we can name objects anything we want - but before that gets out of hand let’s think about some guidelines for naming objects.\n\nMake them simple, specific, and not too long (otherwise you will end up with a lot of typing to do and difficulties remembering which object is which).\nObject names cannot start with a number.\nR is case sensitive, fork_length is not the same as Fork_Length.\nAvoid using dots (.) in names. Typically dots are used in function names and also have special meaning (methods) in R.\nSome names are already taken by fundamental functions (e.g. if, else, for) and cannot be used as names for objects; in general avoid using names that have already been used by other function names.\nRule of thumb: nouns for object names, verbs for function names.\n\nUsing a consistent style for naming your objects is part of adopting a consistent styling of your code; this includes things like spacing, how you name objects, and upper/lower case. Clean, consistent code will make following your code a lot easier for yourself and others3.3 Remember, future you is your most important collaborator.\n\n\n\n\n\n\nNote\n\n\n\nOne of the criteria for your homework assignments and skills tests is your code style. Next to imitating the style of coding presented in this manual, you can refer to r4ds (2e) Ch 5 for some initial pointers, you can also access a short style guide here and a more detailed, tidyverse specific style guide here."
  },
  {
    "objectID": "03_Rbasics.html#saving-your-work",
    "href": "03_Rbasics.html#saving-your-work",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.3 Saving your work",
    "text": "3.3 Saving your work\nSo far, we have inputed all of our code directly into the console. If you scroll up in the console you will find that all the commands and results from your current R session are still in the console. Using Cmd/Ctrl + L will clear the entire console.\nUh-oh - what if we need to go back over the code we just cleared?\nWell, for one if you check the History tab in the top right panel you will see that all your commands have been recorded. If you highlight one of them and either click on To Console or hit Enter it will send it directly to the console.\nUsually your history will be saved automatically when you close R/end an R session (unless you have changed the settings) and it will be restored when you open R again. You can use the broom icon to clear your entire history.\nUh-oh - now what do we do?\nIn general, you should only be typing code directly into the console for quick queries or troubleshooting but since usually we want to be able to revisit and share our work you will want to be able to save your work in an R script (*.R) or include it in a quarto document (*.qmd) as a code chunk. For this course we will mostly be operating with quarto files (more on that in the next chapter).\nYou can open a new R script using Ctrl + Shift + C or using File &gt; New File &gt; R Script. This will open an R script in a new tab in the top left pane.\nSave your R script using Cmd/Ctrl + S or File &gt; Save As - this will open a dialogue box for you to save your R script with the file extension .R.\nCtrl + Enter will execute commands directly from the script editor by sending them through to the console. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute. You can also run all the code in a script by clicking on the Run button.\nCreate a new R script to keep track of the rest of the things we will learn today."
  },
  {
    "objectID": "03_Rbasics.html#using-comments",
    "href": "03_Rbasics.html#using-comments",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.4 Using comments",
    "text": "3.4 Using comments\nYou can add comments to your R scripts using #. Essentially, once you type an # in a line anything to the right of it will be ignored.\nThis is really helpful as it will allow you to comment your script, i.e. you can leave notes and explanations as to what your code is doing for future you and for other collaborators. This is especially helpful if you come back to some of your code after a period of time, if you are sharing your code with others, and when you are debugging code. You will find that as you become more experienced your comments will become shorter and more concise and you might even be tempted to leave them out completely - don’t4!4 To help you build a habit of good commenting practice, commenting your code is a requirement for your homework assignment and skills tests.\nFor example you might find a comment like this more helpful at the moment:\n\n# assign value to new object total length\nfork_length &lt;- 436\n\nBut soon you’ll find this just as helpful:\n\n# total length fish\nfork_length &lt;- 436\n\n\n\n\n\n\n\n Consider this.\n\n\n\nPredict what value of the object total_length will be after executing this command.\n\n\n\nFL &lt;- 436  # total length fish\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can comment/uncomment multiple lines at once by highlighting the lines you want to comment (or uncomment) and hitting Ctrl + Shift + C. This can be useful if you are playing around with code and don’t want to delete something but don’t want it to be run either."
  },
  {
    "objectID": "03_Rbasics.html#functions",
    "href": "03_Rbasics.html#functions",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.5 Functions",
    "text": "3.5 Functions\nWhen we installed R packages earlier we mentioned that they are sets of predefined functions. These are essentially mini-scripts that automate using specific sets of commands. So instead of having to run multiple lines of code (this can be 10s - 100s of lines code) you call the function instead.\nEach function usually requires multiple inputs (arguments) and once executed return a value (though this is not always the case).\nFor example the function round() can be used to round a number5.5 This is an excellent example of naming things well!\n\nfork_length_cm &lt;- round(34.8821)\n\nIf we print the value of our object we see the following value is returned.\n\nfork_length_cm\n\n[1] 35\n\n\nFor this function the input (argument) is a number and the returned value is also a number. This is not always the case, arguments can be numbers, objects, file paths …\nMany functions have set of arguments that alter the way a function operates - these are called options. Generally, they have a default value which are used unless specified otherwise by the user.\nYou can determine the arguments as function by calling the function args().\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nOr you can call up the help page using ?round or by typing it into the search box in the help tab in the lower right panel.\nFor example, our round() function has an argument called digits, we can use this to specify the number of significant digits we want our rounded value to have.\n\nround(34.8821, digits = 2)\n\n[1] 34.88\n\n\nIf you provide the arguments in the exact same order as they are defined you do not have to specify them.\n\nround(34.8821, 2)\n\n[1] 34.88\n\n\nHowever, if you specify the arguments, you can switch their order.\n\nround(digits = 2, x = 34.8821)\n\n[1] 34.88\n\n\n\n\n\n\n\n\nProtip\n\n\n\nGood code style is to put the non-optional arguments (frequently the object, file path or value you are using) first and then specify the names of all the optional arguments you are specifying. This provides clarity and makes it easier for yourself and others to follow your code.\n\n\nOccasionally you might even want to use comments to further specify what each argument is doing or why you are choosing a specific option.\n\nround(34.8821,     # number to round\n      digits = 2)  # specify number of significant digits\n\n[1] 34.88"
  },
  {
    "objectID": "03_Rbasics.html#vectors-data-types-i",
    "href": "03_Rbasics.html#vectors-data-types-i",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.6 Vectors (data types I)",
    "text": "3.6 Vectors (data types I)\nNow that we’ve figured out what objects and functions are let’s get to know the two data types we will be spending the most time with this semester - vectors and data frames (data.frame)6.6 Other data types include lists (list), factors (factor) matrices (matrix), and arrays (array); we’ll introduce those later on.\nThe most simple data type in R is the (atomic) vector which is a linear vector of a single type. There are six main types -\n\ncharacter: strings or words.\nnumeric or double: numbers.\ninteger: integer numbers (usually indicated as 2L to distinguish from numeric).\nlogical: TRUE or FALSE (i.e. boolean data type).\ncomplex: complex numbers with real and imaginary parts (we’ll leave it at that).\nraw: bitstreams (we won’t use those either).\n\nYou can check the data type of any object using class().\n\nclass(fork_length)\n\n[1] \"numeric\"\n\n\nCurrently, our fork_length object consists of a single value. The function c() (concatenate) will allow us to assign a series of values to an object.\n\nfork_length &lt;- c(454, 234, 948, 201)\n\nfork_length\n\n[1] 454 234 948 201\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nPredict what data type you expect this vector to be.\n\n\nWe call the same function to create a character vector.\n\nsharks &lt;- c(\"bullshark\", \"blacktip\", \"scallopedhammerhead\")\n\nclass(sharks)\n\n[1] \"character\"\n\n\nThe quotes around \"bullshark\" etc. are essential because they indicate that this is a character.\n\n\n\n\n\n\nProtip\n\n\n\nIf we do not use quotes, R will assume that we are trying to call an object and you will get an error code along the lines of “! object 'bullshark' not found”.\n\n\nYou can use c() to combine an existing object with additional elements (assuming they are the same data type).\n\nspecies &lt;- c(sharks, \"gafftop\")\n\nspecies\n\n[1] \"bullshark\"           \"blacktip\"            \"scallopedhammerhead\"\n[4] \"gafftop\"            \n\n\nNext to class() there are other helpful functions to inspect the content of a vector. For example length() will tell you how many elements are in a particular vector.\n\nlength(fork_length)\n\n[1] 4\n\n\nThe function str() will give you an overview of the structure of any object and its elements.\n\nstr(fork_length)\n\n num [1:4] 454 234 948 201\n\n\nRecall, that an atomic vector is a linear vector of a single type. Let’s explore what that means by taking a look at what happens if we create atomic vectors where we mix the data types.\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe what happens when data types are mixed in a single atomic vector based on the messages generated by the code chunk below to figure out what the rules are in terms of which data type is convered to match the others when they are mixed.\n\n\n\nnumeric_character &lt;- c(1, 2, 3, \"a\")\nnumeric_logical &lt;- c(1, 2, 3, TRUE)\ncharacter_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\nwtf &lt;- c(1, 2, 3, \"4\")\n\nWe already discovered that we can combine vectors - but can we extract certain components from vectors? Indeed, there are a variety of ways that we can subset vectors.\nThe most simple way is using square brackets to indicate which element (or elements) we can’t extract. In R, indices start at 1.77 This is not the case for all programming languages, e.g. Perl, Python, or C++ start with 0.\n\n# extract second element\nspecies[2]\n\n[1] \"blacktip\"\n\n# extract fourth and second element\nspecies[c(4, 2)]\n\n[1] \"gafftop\"  \"blacktip\"\n\n\nYou can also repeat indices to create a new object with additional elements.\n\nspecies_longer &lt;- species[c(2, 2, 4, 3, 4, 4, 1, 1)]\n\nspecies_longer\n\n[1] \"blacktip\"            \"blacktip\"            \"gafftop\"            \n[4] \"scallopedhammerhead\" \"gafftop\"             \"gafftop\"            \n[7] \"bullshark\"           \"bullshark\"          \n\n\nMore frequently, we will want to extract certain elements based on a specific condition (conditional subsetting).\nThis is done using a logical vector, here TRUE select the element with the same index and FALSE will not.\n\nfork_length &lt;- c(454, 234, 948, 201)\n\n# use logical vector to subset\nfork_length[c(TRUE, FALSE, TRUE, FALSE)]\n\n[1] 454 948\n\n\nThis seems like a very impractical option. However, normally we would not create the logical vector by hand as we have done here, rather it will be the output of a function or logical test. For example, we might want to identify fish with a fork length &gt; 300mm.\n\n# identify fish with fork length &gt; threshold\nfork_length &gt; 300\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nThis creates an output the same length as the vector we looked at (fork_length) consisting of TRUE/FALSE statements for each element by comparing the each element of the vector to the condition (&gt;300) and determining if the condition is met (the statement is true) or not.\nInstead of first creating a vector of TRUE/FALSE statements can use this condition to subset our vector directly.\n\n#  identify true/false of fish with fork length &gt; threshold\nfork_length[c(fork_length &gt; 300)]\n\n[1] 454 948\n\n\nThere are a series of boolean expressions8 we can use for subsetting vectors.8 Boolean expressions are logical statements that are either true or false; most of them you are probably already familiar with because math\n\n&gt; and &lt; (greater/less than)\n=&gt; and =&lt; (equal to or greater/less than)\n== (equal to) and != (is not equal to)\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can combine to boolean expressions using &, (both conditions must be met) and | (at least one condition must be met).\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset the fork_length vector to\n\ncontain only values equal to 234\ncontain all values but 948\ncontain all values larger than 230 but smaller than 900\ncontain all values smaller than 250 or larger that 900\n\n\n\nR is set apart from other programming languages because it was designed to analyze data9 it has straightforward ways to deal with missing data (NA or na values) because those are quite common in real world data sets.9 some people will argue that it is a ‘statistical language’ rather than a true programming language … don’t listen to them, they are just jealous of your R skillz!\nLet’s create a vector with a missing value.\n\ntotal_length &lt;- c(560, NA, 1021, 250)\n\nLet’s say we want to calculate the mean value.\n\nmean(total_length)\n\n[1] NA\n\n\nMost functions will return NA when doing operations on objects with missing values. As such, many functions include an argument to omit missing values.\n\nmean(total_length, na.rm = TRUE)\n\n[1] 610.3333\n\n\nOther functions that are helpful to deal with missing values are is.na(), na.omit(), and complete.cases().\n\n\n\n\n\n\n Give this a whirl.\n\n\n\nSubset the fork_length vector to\nRun each of these functions on our total_length vector and describe what they do.”"
  },
  {
    "objectID": "03_Rbasics.html#data-frames-data-types-ii",
    "href": "03_Rbasics.html#data-frames-data-types-ii",
    "title": "3  R: Functions, Objects, Vectors - oh my!",
    "section": "3.7 Data frames (data types II)",
    "text": "3.7 Data frames (data types II)\nRecall that atomic vectors are linear vectors of a simple type, essentially they are one dimensional. Frequently we will be using data frames (data.frame) which you can think of as consisting of several vectors of the same length where each vector becomes a column and the elements are the rows.\nLet’s create a new object that is a dataframe with three columns containing information on species, fork length, and total length.\n\n# combine vectors into data frame\ncatch &lt;- data.frame(species, fork_length, total_length)\n\nYou should now see a new object in your Global Environment and you will now also see that there are two categories of objects Data and Values. You will see that the data.frame is described as having 4 obs (observations, those are your rows) of 3 variables (those are your columns). If you click on the little blue arrow it will give you additional information on each column - note that because each column is essentially a vector, each one must consist of a single data type which is also indicated.\nCalling the str() will give you the same information.\n\nstr(catch)\n\n'data.frame':   4 obs. of  3 variables:\n $ species     : chr  \"bullshark\" \"blacktip\" \"scallopedhammerhead\" \"gafftop\"\n $ fork_length : num  454 234 948 201\n $ total_length: num  560 NA 1021 250\n\n\nYou can further inspect the data.frame by clicking on the little white box on the right which will open a tab in the top left panel next to your R script. You can also always view a data.frame by calling the View() function.\n\nView(catch)\n\nThis can be a helpful way to explore your data.frame, for example, clicking on the headers will sort the data frame by that column. Usually we won’t build or data.frames by hand, rather we will read them in from e.g. a tab-delimited text file - but more on that later."
  },
  {
    "objectID": "04_Rproj.html#learning-objectives",
    "href": "04_Rproj.html#learning-objectives",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nAfter completing this tutorial you should\n\nbe able to set up a well structured research compendium1.\nunderstand what a working directory is and how to set up an R project.\nunderstand the value of using Rmarkdown and quarto documents to document your work.\nunderstand the core components of the markdown format.\nbe able to use basic markdown syntax to format a document (headers, bold/italics).\nbe able to add a code chunk to a .qmd file and add options.\nunderstand how to modify code chunk options to determine what is/is not rendered in the knitted document.\nknow how to render a document to produce an *.html file from an *.qmd2.\n\n1 We will at a later point in this semester revisit and determine if this truly is a well-structured folder structure2 the recent implementation of quarto has made exporting to other formats such as pdf a lot more straightforwardThe goal of open science and reproducible research is to make scientific methods, data, and results more transparent, available, and reproducible. Quarto documents written in markdown are a useful tool to be able to generate reports documenting your data, methods (code used to process data), and results. Recently quarto has been implemented as a authoring framework for data science that extends past previous use of Rmarkdown focused on R and makes it easy to include python and other coding languages.\nSimilar to R markdown (*.Rmd) files, quarto documents will let you document your workflow, share how you processed/analyzed your data and the resulting output along with any visualizations. Quarto unifies and extends the functionality of several packages that were devoloped around using R markdown unto a consistent system that supports several coding languages beyond R including python and Julia. This is a text based file format that consists of standard text, code chunks, and the resulting output using a very simple syntax (henc markdown as opposed to markup languages like html or LateX which have much more complicated syntax). When you render your document, the code is executed and the resulting output with be included in the rendered document (common formats are html or pdf). Advantages to a workflow centered around using quarto and markdown to document your work include:\n\nthe simple syntax makes it easy to learn the basics (but some of the more advance options will let you create some sophisticated reports)3.\nresulting files have a simple, standardized formatting that looks professional and is easy to read and understand (code, documentation, figures all in one place).\nfuture you will be thankful when you don’t have to remember your assumptions, individual steps, and modifications.\neasy modification to extend/refine analysis or re-run with updated data.\n\n3 Rstudio now also has a visual editor so you can really get away with knowing very little markdown, though the basics can’t hurt and knowing a few tricks like inline code will let you do some pretty cool stuff with your documents\n\n\n\n\n\nNote\n\n\n\nUse this link to access a quarto cheatsheet for a quick overview on publishing and sharing with quarto. You can also download a pdf summarizing the core quarto functionalities to keep handy as you get used to setting up quarto documents.\nSimilarly, Use this link to access a rmarkdown cheatsheet. We won’t be using Rmarkdown documents but the syntax of writing in markdown is the same. You can also download and print a pdf for easy access."
  },
  {
    "objectID": "04_Rproj.html#project-organization-101",
    "href": "04_Rproj.html#project-organization-101",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.2 Project organization 101",
    "text": "4.2 Project organization 101\nA key component to doing data analysis is organizing your working directory which will contain your data, scripts, quarto-documents, results, figures, and other output. Keeping this well organized will help you establish a reproducible workflow, collaborate well, and share your results.\n\n4.2.1 Organizing your files and directories\nFor each project/lab we will set up a project directory4 with the following set of sub-directories:4 We’ll use this term interchangeably with working directory and research compendium\n\ndata\nresults\nscr\nscratch\n\nYou will want to set up a folder (directory) locally5 on your computer called bi349 that you will use throughout this semester for all the project directories, a good place is your Documents directory or in a pinch your desktop. You will frequently download an entire research compendium with data and quarto documents “preloaded”. Macs will automatically unzip those folders on a Windows computer you will need to do this by hand (right click &gt; extract all), then move that folder into your bi329 directory. Make sure you are running your Rprojects out of the correct folder - this is one of the most common issues we run into when things aren’t working as they should.5 Local means that it is physically on your computer hard drive. If you have an automatic integration with a cloud storage service like OneDrive past experience has shown that you can run into difficulties, so yes, cloud backup is good but make sure that you are running your projects locally off your computer\n\n\n4.2.2 A note on Naming things\nNaming conventions for files, sub-directories etc. should conform to the following key principles that form the holy trinity of file naming6:6 see Jenny Bryan’s excellent summary of these principles\n\nHuman readable: keep it short but self-explanatory.\nMachine readable: don’t use special characters or spaces.\nSortable: standardize components of the file names to make it possible to sort files and find what you are looking for.\n\nApplying these principles includes conventions include sticking to all lowercase7, consistent use of _ and - instead of spaces, writing your dates as year-month-day, using leading zeros (e.g. 001, 002, … etc instead of 1, 2, ... 10, 11, 12... which will sort as 1, 11, 12, ... 2, 21, ... etc once you go into double digits).7 alternatives include uppercase or CamelCase, but since R is case sensitive this leads to mroe typos\n\n\n4.2.3 Set up your project directory using Rprojects\nIf you have not already, create a directory called bi328 locally on your computer. Make sure you know where it is, you will be adding to this directory throughout the semester.\nCreate a project directory 8 zz_skills9) within your bi328 folder, and within that directory create sub-directories data, results, scr, and scratch. Throughout the semester you will add quarto documents to this directory as you complete your weekly skills tests.8 Yes, a directory is essentially a folder, however when using the term directory we are considering the relationship between a folder and it’s full path.9 addin the zz before the folder name means that it will sort to the bottom of this directory, this is an example of sortable. Naming it skills is descriptive in terms that you know it refers to your skills tests (human readable) and using the _ makes it machine-readable, as you are avoiding issues with white spaces\nNow, we are going to create an R project within this directory.\n\nin the top right hand corner of Rstudio click on the project icon\nselect New Project and Create in existing directory\nfollow the prompts to navigate to your zz_skills directory to create a new Rproject.\n\nThis should create a new R project and open it (the R project name should be in the top right corner next to the icon).\nIf you look in the bottom left hand pane in the Files tab, the bread crumbs should lead to your project folder which has now become your working directory, i.e. all paths are relative to this location. 10 If you navigate away from your working directory (project directory) you can quickly get back to your project directory by clicking on the project icon in the Files pane or by clicking the cog icon (More) and selecting Go to Working Directory.10 If you weren’t working with an R project, you can set your working directory by navigating to your new working director and selecting More &gt; Set as working directory."
  },
  {
    "objectID": "04_Rproj.html#structure-of-an-quarto-document",
    "href": "04_Rproj.html#structure-of-an-quarto-document",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.3 Structure of an quarto document",
    "text": "4.3 Structure of an quarto document\nFor each skills test you will either be creating a quarto document your solutions or you may be asked to download a quarto document to work in. Let’s make sure you know how to create a new document and what the different component of that document are.\nCreate a new .qmd file using File -&gt; New File -&gt; Quarto Document and save that file in your project directory as Lastname_first-quarto-document.qmd.\nAn qmd-file consists of three components:\n\nHeader: written in YAML format the header contains all the information on how to render the .qmd file.\nMarkdown Sections: written in Rmarkdown syntax.\nCode chunks: Chunks of R code (or other code such as bash, python, …). These can be run interactively while generating your document and will be rendered when knitting the document."
  },
  {
    "objectID": "04_Rproj.html#yaml-header",
    "href": "04_Rproj.html#yaml-header",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.4 YAML header",
    "text": "4.4 YAML header\nThe header is written in YAML syntax, it begins and ends with ---. It will include a few default parameters. You will find that there is a wide range of parameters that you can use to customize the look of your document but for now we will add these four.\n\n---\ntitle: \"title\"\nauthor: \"name\"\ndate: \"Date\"\nformat: html\n----\n\nCustomize your .qmd by changing the title and add your name in the author line11. Changing the date to `r Sys.Date()` will automatically include the current date when you render the document instead of having to update that yourself.11 You can always do this when you start a new file, for a lot of case studies this semester you will download quarto documents where you will want to change those"
  },
  {
    "objectID": "04_Rproj.html#markdown-sections",
    "href": "04_Rproj.html#markdown-sections",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.5 Markdown sections",
    "text": "4.5 Markdown sections\nYour markdown sections can contain any text you want using the markdown syntax; once you render the .qmd the resulting (html) file will appear as text.\nMost of your text (without syntax) will appear as paragraph text but you can add additional syntax to format it in different ways.\nHere are the basics that are fairly consistent across a range of markdown flavors:\nText formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\nHeadings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n    \n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n    \nLinks and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](quarto.png){fig-alt=\"Quarto logo and the word quarto spelled in small case letters\"}\n\nTables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\n\nNote\n\n\n\nCurrent Rstudio versions do have a visual editor that is WYSIWYG12 and will allow you to format your document similar to the way you would in Google Docs or Microsoft Word. However, it is helpful to know the basics because de-bugging can be more straightforward using the Source Editor, and if you are comfortable with the syntax it is also a lot faster to format.\n\n\n12 What you see is what you get"
  },
  {
    "objectID": "04_Rproj.html#code-chunks",
    "href": "04_Rproj.html#code-chunks",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.6 Code chunks",
    "text": "4.6 Code chunks\nCode chunks contain your R code and start and end with three back ticks; {r} determines that the code chunk should be interpreted as R code.\n\n\n\nYou can type it in manually but it is a lot quicker to add a code chunk using the shortcut Ctrl + Alt + I on a Windows computer or Command + Option + I for you Mac users.\n\n\n\n\n\n\nNote\n\n\n\nYou can also insert a code chunk using Code -&gt; Insert Chunk in the tool bar, the insert button in the tab bar (little green box with C and +) or if you are in the visual editor, your can use insert button in the editor bar.\nIn short, there is no excuse not to be adding code chunks and writing code!\n\n\nYou can run (execute) entire code chunks entire chunk by clicking the Run button in the tab bar or the little green arrow in the top right corner of an R chunk.\nIt is a lot faster to use shortcuts. You can execute and entire code chunk using Cmd/Ctrl + Shift + Enter. Or if you only want to execute a certain piece of code, using Cmd/Ctrl + Enter while your cursor is placed within that code, or highlight the code you want to execute and then hit Cmd/Ctrl + Enter.\nRemember to use # to comment your code, any lines following a # will not be run by R, you can use them to describe what your code is doing. Use comments liberally to document your code, future you will thank you!\n\n\n\n\n\n\nNote\n\n\n\nBefore submitting any of your skills tests or homework assignments you should always go through and make sure each piece of code has a descriptive comment. You do not need to add a comment for multi-line code that you are stringing together using a pipe %&gt;% but you should have one descriptive comment above the set of commands you are giving R and then make sure that you add any comments that you need to remember how the function works or which parameters might be useful to tweak/set differently if you were to reuse that code.\n\n\nOptionally you can add a label to your code chunks that can be used to navigate directly to code chunks using the drop-down menu in the bottom left of the script editor.\n\n```{r}\n#| label: code-label-1\n\n1 + 1\n```\n\n[1] 2\n\n\nIf you do this for figures or tables you can start your labels with fig- or tbl- which will allow them to be automatically numbered and you can link to them later in your document. Labels cannot be repeated (i.e. they all must be unique) and cannot have spaces, best practice here would be using dashes (-) to separate words.\nYou can add options to each code chunk to customize how/if a chunk is executed and appears in the rendered output. These options are added to within the curly brackets. For example, eval: false: results in code chunk not being evaluated or run though it will still be rendered in the knitted document13. You can apply multiple options to the same chunk.13 This can be useful for you if e.g. for one of your skill tests you cannot solve one of the challenges and the document will not render because your code won’t run, this way you can show you attempt but also run the document\n\n```{r}\n#| label: code-label-2\n#| eval: false\n\n1 + 1\n```\n\nYou do have options to add figure and table captions, you can also e.g. control figure width and height. See section in r4ds (2e) Chapter 29 for a list of commonly used code options and you can find additional options here."
  },
  {
    "objectID": "04_Rproj.html#render-your-document",
    "href": "04_Rproj.html#render-your-document",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.7 Render your document",
    "text": "4.7 Render your document\nknitr is an R package used to render quarto documents to another format (usually html or pdf). In Rstudio the most straightforward way of knitting a document is using the render button in the editor toolbar. This will open a new tab in your console titled Background Jobs that will show the knitting process; any errors that occur with show up here along with a line number so you can determine where the error is occurring in your .qmd file to troubleshoot the issue. The output will automatically be saved in your working directory."
  },
  {
    "objectID": "04_Rproj.html#some-advanced-options",
    "href": "04_Rproj.html#some-advanced-options",
    "title": "4  Project management and Rmarkdown Basics",
    "section": "4.8 Some advanced options",
    "text": "4.8 Some advanced options\nYou can stylize your rendered document by modifying the YAML header to include a table of contents like this14:14 the option of toc-depth determines how many levels are included in the table of contents, e.g. here headers at level 1 and 2 will be included\n---\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n---\nIf you really want to jazz things up, you can change the theme15.15 you can choose from various options here\n---\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 2\n---\nYou may be noticing some similarities between this lab manual and the documents you are producing in terms of layout … for exactly the reasons you are suspecting!"
  },
  {
    "objectID": "05_data-management.html#wait-spreadsheets",
    "href": "05_data-management.html#wait-spreadsheets",
    "title": "5  Data management using spreadsheets",
    "section": "5.1 Wait… spreadsheets?",
    "text": "5.1 Wait… spreadsheets?\nThe foundation of any research project is good data organization. This not only includes your actual data points (observations) but also things like keeping track of specimen and other samples along with all the meta-data2. Additionally, you should keep good records of how the data was produced (your methods). Thinking through ahead of time what measurements are important, i.e. what data you will record and how you will store your data is really important to make sure you are keeping track of the entire process. Good data management and clean data sets will make sharing and analyzing data a lot more straightforward.2 Metadata is data about your data. It helps describe, categorize, organize, and provide context to the main content or primary data it is associated with. It’s commonly used to provide additional information that helps users, systems, or processes understand and interpret the main data.\nWhile we are going to use R to wrangle, manipulate, analyze, and visualize data in R for a more efficient and reproducible approach compared to what can be using spreadsheets, spreadsheets are the better tool for data entry, data management/organization, and simple quality control (QC) and quality assurance (QA). Thinking ahead to how you want to organize and format your data in spreadsheets will prevent a lot of extra work and headaches down the line, especially when we plan ahead as to how we should organize it to make it more efficient to use with command-line computational tools such as R.\nSo, before we dive deep into manipulating data with R, we’ll take a small step back and think through a few fundamental rules for managing data in spreadsheets before learning how to do these and more advanced data wrangling and manipulation using R.\nWhile you can do some statistics and plotting using spreadsheet programs we will not be learning how to do this in this class. Data analysis in spreadsheets requires a lot of manual work and generally any time you want to change one parameter or if you have to update your spreadsheet with new entries or you need to apply the same analysis to another data set you end up having to redo everything by hand. The more things you do by hand, the more likely you are to make a mistake. Even if you do apply some sophisticated coding in spreadsheets and/or use it for analysis or plotting3 it is very difficult to track the exact steps or document them in a way that makes it fully reproducible for another person.3 It can be helpful to know the fundamentals for simple plots in spreadsheets for quick and dirty plotting to get a quick look at your data to get an idea of what it looks like and spot potential mistakes during data entry without having to export data and fire up R or a another command-line program."
  },
  {
    "objectID": "05_data-management.html#best-practices-for-formatting-data-in-spreadsheets",
    "href": "05_data-management.html#best-practices-for-formatting-data-in-spreadsheets",
    "title": "5  Data management using spreadsheets",
    "section": "5.2 Best practices for formatting data in spreadsheets",
    "text": "5.2 Best practices for formatting data in spreadsheets\n\n5.2.1 Format your data set for the tool you will use to analyze it with\nOur brains don’t work the same way as computers. Your spreadsheet is not a lab notebook and while a layout where there are notes in the margin, context of the experiment, or a specific layout of data might be something that you can interpret, it will more difficult for another person to forllow your through process and understand your records/notes. Another person might have the opportunity to ask follow up questions and get the clarifications they need, but a computer cannot.\nOccasionally, you might use a spreadsheet instead of a lab notebook where it is a way of keeping track of an experiment and various people interacting with samples, completing different steps etc. However, if you are using a spreadsheet for data entry and management, then you want to ensure that you have set up your spreadsheet in a way where a computer is going to be able to correctly interpret it as intended. This means that we need to think through how we want to set up spreadsheets. There are generally a few different ways you can set things up and some of them will limit how easy it is for you and/or a future collaborator to work with it down the line4.4 the optimum software/interface for data input and layout/formatting may differ depending on your intended analysis, so keep in mind how you want to be able to analyze your data and whether that will require specific formats. In general, try to pick a format that will give you the advantage of being able to easily convert it between different formats - which is something we will learn to do with R specifically in the tidyverse which centers on a specific concept of what makes data tidy.\n\n\n5.2.2 Never touch your raw data\nRaw data is the original, unaltered data that is collected or generated before any manipulation, transformation, or analysis takes place. It’s the most fundamental form of data, directly obtained from observations, measurements, or data sources. Raw data is often in its most unstructured and basic state, and it serves as the foundation for subsequent data processing and analysis.\nIn the biological and environmental sciences typical sources include direct observations made in experiments, field studies, or natural phenomena or measurments from sensors or other instruments measuring physical and chemical parameters such as temperature, GPS coordinate, pH etc.\n\n\n\n\n\n\nBe mindful\n\n\n\nNever touch your raw data. Always keep a copy of your raw data that you never modify directly.\n\n\nFor any kind of data related work it is important that you preserve the original, unaltered version of your data when conducting data analysis. Avoid making changes directly to the original data files or data set. Instead, You should work with copies of the data or use a structured workflow that ensures the integrity and reproducibility of your analysis.\nKeeping your raw data as a separate file that is never altered is important for\n\nData Integrity: Modifying the raw data directly could lead to unintended changes or loss of information. By keeping the raw data untouched, you ensure that you have a reliable source of truth to refer back to if needed.\nReproducibility: If you or others need to replicate your analysis in the future, having access to the exact original data is crucial. Changes made to the raw data could make it difficult or impossible to reproduce your results accurately.\nError Prevention: Working with copies of the raw data minimizes the risk of accidental changes or mistakes that could affect your analysis. If errors occur, you can always go back to the untouched raw data.\nData Auditing: In some cases, you might need to show the authenticity and accuracy of your data. Keeping the original data untouched allows you to demonstrate the reliability of your work.\nMultiple Analyses: If you’re working on different analyses, projects, or collaborations using the same data, maintaining the integrity of the raw data enables consistency across these efforts.\n\nBest practices of maintaining the integrity of your raw data include * making copies: Always work with copies of the original data files or datasets. This ensures that any changes you make are isolated from the raw data. * implementing a structured workflow: Develop a structured workflow that includes data cleaning, transformation, and analysis steps. Document each step thoroughly to ensure transparency and repeatability. * using version vontrol: Use version control systems like Git to track changes to your code and analysis scripts. This allows you to see how your analysis evolves over time. * creating backups: Regularly back up your data, including both the raw data and any processed versions, to prevent data loss. * creating documentation: Maintain clear and detailed documentation about the steps you’ve taken, the rationale behind your decisions, and any changes you’ve made to the data.\n\n\n5.2.3 Keep track of your formatting steps\nBy working with copies and following a structured workflow, you can ensure the accuracy, reproducibility, and integrity of your data analysis work. While you shouldn’t modify the raw data directly, it’s also important to apply necessary data preprocessing steps (like cleaning and transforming) as part of your analysis process. This means that you should do two things\n\nAny time you need to process or analyze your data make a copy instead of operating directly in your raw data5 and then create a new file with your cleaned or analyzed data.\nKeep track of the exact steps you took to clean or analyze your data6; this is just as important as keeping a detailed record of the steps you took in an experiment. Good practice would be to keep a plain text file or similar in the same folder as your data set where you record any steps you take.\n\n5 In our next lesson you are going to see that this is a key advantage of command line programs like R where you can read a raw data set into the program and then apply specific data wrangling, manipulation and analysis steps without altering the raw data.6 The second advantage of command-line programs like R is that because you are using a series of commands to wrangle and analyze your data your are automatically creating a very detailed, reproducible record of your your workflow\n\n5.2.4 Put variables in columns and observations in rows\nObservations and variables are two fundamental concepts that describe different aspects of data.\n\n\n\n\n\n\n Consider this\n\n\n\nCompare and contrast what an observation is compared to a variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAn observation is a single data point or unit within a data set, while a variable is a characteristic that is being measured or observed for each of those data points.\nTogether, observations and variables make up the structure of a data set, where each observation has values for the various variables being measured.\n\n\n\nHere is the key rule for structuring data in spreadsheet:\nEvery variable gets its own column, every observation gets its own rule, do not combine multiple pieces of information in one cell7.7 This is what we will refer to as tidy data. We will explore this concept more in depth down the line and will play “Is it tidy?” regularly this semester.\n\n\n5.2.5 Export data as text-based formats\nWhile it is a lot easier to enter and look at data in a spreadsheet you should always export your raw and cleaned data set as a text-based format such as CSV, TSV, JSON, XML, etc.).\nThis offers several advantages compared to proprietary binary formats8:8 an example would be Excel’s .xlsx\n\nInteroperability: Text-based formats are widely supported by various software and programming languages. This means that data can be easily shared and integrated into different applications and systems, regardless of the software being used. Especially if program have proprietary formats having a format that is platform independent is really important.\nSimplicity: Text-based formats have a simple, human-readable structure. This makes it easier to understand the data’s content, and it allows manual inspection and editing using basic text editors.\nVersion Control: Text-based formats work well with version control systems (e.g., Git). Since changes can be easily tracked in plain text, it’s easier to collaborate, review, and manage changes made to the data. & Data Integrity: Text-based formats are less prone to corruption and data loss. Proprietary binary formats can sometimes become corrupted, making data recovery difficult.\nPlatform Independence: Text-based formats are platform-independent. They can be used on different operating systems without compatibility issues\nReduced File Size: Text-based formats generally have smaller file sizes compared to their binary counterparts. This can be advantageous for sharing and storage, especially when dealing with large datasets.\nAutomation and Scripting: Text-based formats are well-suited for automation and scripting tasks. Many programming languages have libraries and tools to read and write data from these formats easily.\nData Analysis: Text-based formats can be directly used in data analysis workflows. Data scientists and analysts often use tools like Python, R, and SQL to work with text-based data formats.\nData Accessibility: When sharing data with others, especially outside your organization, text-based formats offer a universal way to provide data that can be imported into various tools without compatibility issues."
  },
  {
    "objectID": "05_data-management.html#common-spreadsheet-formatting-issues",
    "href": "05_data-management.html#common-spreadsheet-formatting-issues",
    "title": "5  Data management using spreadsheets",
    "section": "5.3 Common spreadsheet formatting issues",
    "text": "5.3 Common spreadsheet formatting issues\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe common formatting mistakes formatting data in spreadsheets, explain why it might be tempting to format data in this way and why it might cause downstream issues for data analysis.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere are the key points you will want to discuss:\n\nmultiple tables in one tab\ndata spread across multiple tabs\nnot filling in zeros\nusing problematic null values for missing data\nusing formatting to convey information\nusing formatting to make the data sheet look pretty\nplacing comments or units in cells\nentering more than one piece of information per cell\nusing problematic field/column names\nusing special characters in data\nincluding metadata in the data table\ndate formatting"
  },
  {
    "objectID": "05_data-management.html#dates-are-data-but-like-the-worst-kind",
    "href": "05_data-management.html#dates-are-data-but-like-the-worst-kind",
    "title": "5  Data management using spreadsheets",
    "section": "5.4 Dates are data (but like, the worst kind)",
    "text": "5.4 Dates are data (but like, the worst kind)\nProbably the most intuitive way to store dates in a spreadsheet would be to create a column called date and then just store your dates in there.\n\n\n\n\n\n\n Consider this\n\n\n\nQuick. Off the top of your head come up with 10 ways that you could format a date.\n\n\nSo that’s the first problem - what should a date even look like? Problem two is that while to you the human this would be the most natural way to do this, the spreadsheet might be displaying it in a way that makes sense to you but is actually storing it in a very different format. Additionally, different spreadsheet programs (Microsoft Excel, Google Sheets, LibreOffice, OpenOffice) might be storing and handling dates slightly different from each other. In this case the date functions valid for one might be only somewhat-ish compatible with each other. Additionally, spreadsheet programs generally are trying to automatically recognize dates so e.g. gene/protein names like MAR1, OCT4 would be interpreted to dates and getting the original identifier back might be tricky.\nAdditionally spreadsheets sometimes try to be especially helpful by autocompleting information.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOpen the exercise-dates Google Sheets document in the scratch folder of the 01_SharkNurseries folder. Label cell A1 as location and cell B1 as date_sample-1. In cell A2-16 type in A, B, … . Then in cell B2 type a date as just month/day. Hit enter, then click back on the cell and look at the value bar at the top. Describe what you observed and how this “helpful” behavior could lead to data entry problems.\n\n\nYou can switch between different data formats by customizing the format of the cell.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn your Google Sheets document type a few dates in cells B3 - B6. Next, highlight column A. and click on the 123 button in the tool bar. Select Custom Date and Time and describe what type of formatting options you have. Pick one, and see how it changes how the content of your cells is displayed.\n\n\nFor some of the more elaborate formats if you look at the value bar you will notice that even though the content of your cell has changed in terms of how it is formatted, that value might not necessarily match the cell content. How does the spreadsheet program so easily convert between all the different formats?\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s assume that you revisited every site 15 days after the initial visit. In your Google Sheets document type date_sample2 in cell C1. Now in cell C2 type B2 + 15. Describe what happens.\n\n\nWait? Since when can you just add an integer and a date? Aren’t those completely different data formats?\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHighlight column B. Now Click on the 123 button in the toolbar and select Automatic. Describe what you see. Speculate what this means about how spreadsheet programs actually store dates and what implications this could have if you export spreadsheets as text files.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGoogle Sheets is actually storing dates as integers from a default day of December 31, 1899. This can be useful, spreadsheet programs were initially developed for and most heavily used for accounting, so the option of being able to in a straightfoward way add days, months, or years to a given date is quite practical.\nHowever, if you export as a text file you can run into the issue that you text file now has a column with an integer where you expected a date.\nAs a side note it also becomes problematic if you are using dates before December 31, 1899 because it cannot parse them correctly.\n\n\n\nIt is a lot safer to store dates in spreadsheets not as date but rather in three columns as year, month, day. Another option is to use Julian Day or day of the year. Or you can store as a single string as YYYYMMDD you can do the same for time stamps as YYYYMMDDhhmmss. This has advantages in terms of sorting by assending and descending order and you do not have to worry about converting to text.\nIf you were to read this format into R it would initially think that it is an integer, however there are functions that we can use to tell R that it is actually a date and what format it is, and then we would be able to apply functions to extract year, month etc into other programs.\nIn sum, treat dates as multiple pieces of data to make them easier to handle downstream."
  },
  {
    "objectID": "05_data-management.html#quality-assurance-and-control",
    "href": "05_data-management.html#quality-assurance-and-control",
    "title": "5  Data management using spreadsheets",
    "section": "5.5 Quality assurance and control",
    "text": "5.5 Quality assurance and control\nYou will frequently hear people say something along the lines of “oh we still have to QC the data” or “we need to complete QA/QC before we can analyze the data. QA stands for quality assurance and QC stands for quality control and both processes are critical to ensure that data being used moving forward is accurate, reliable and valid.\nQuality assurance focuses on preventing errors and ensuring that the proceses used to generate and enter the data are effective and efficient and minimize error. It involves establishing guidelines, standards, and best practices to be followed during the processes. The goal is to identify and address potential issues before they can occur or at least as early as possible in the process.\nBy contrast, quality control focuses on identifying errors that may have occured during the process of generating and entering data by performing checks and tests at various stages of the process to verify that the final output meets the predefined quality standards.\nEnsuring that we have accurate and consistent data collection methods, checking for and removing or correcting data entry errors, and validating data against predefined criteria is a critical step in (data) science. It is important that you keep a good record of the steps you took, rules you apply to discern “good” vs “bad” errors, and which data was removed to ensure transparency and repeatability.\nOne important component of quality assurance is stopping from bad data being entered in the first place by creating a list of valid values which will then prohibit false values from being entered. For example, we might be working with different types of gear to catch sharks at each location, longlines, gillnets, and hook-and-line. It would be easy to accidentally mistype one of these gear types or forget whether we are entering everything lowercase or using capitalization.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn your Google Sheet in column D type gear. Then place your cursor in cell D2 and navigate to Data &gt; Data Validation which will bring up a dialog on the right sight of your screen. Click on Add rule.\nIn the Apply to range box it will currently say Sheet1!D2, you can extend this to include all cells from D2 to D6 by modifying this to Sheet1!D2:D6 (or by marking the area in the spreadsheet). Click on the Criteria Dropdown menu to see how many different types of options you have to set rules in terms of what is allowed to be entered into the cells to which you are applying this rule.\nWe are going to use a Dropdown. By default you will have two fields. Fill those out as longline and gillnet. Then click on add another item and add hook-and-line.\nCheck out the advance options which allows you to change whether you just get a warning or the input is rejected if your entry is invalid, you can also play with the display style to see how that affects the formatting and ease of use. Then click Done.\nWe have a short list of options so you you can easily see all three and select the correct one. For longer lists it is more helpful that you can start typing in you data and then select it.\n\n\nYou can see how this option is helpful for categorical data where typos are an issue. But you can also restrict dates to certain time periods or numbers to certain values.\nUsing these types of rules help minimize errors, however it is almost inevitable that something will sneak in eventually which is where quality control comes in.\n\n\n\n\n\n\nBe mindful\n\n\n\nRemember, before you implement any quality control measure you want to make sure that you make a copy of your data and save the original data as your raw, unaltered data set. You will want to make sure that the file name reflects that it is your raw data.\nCreate a separate file that you will then clean, make sure that your filename includes some sort of versioning and/or a date so you have a good record of when you processed a data set. Then you want to make sure that your data are all values and not formulas which refer to specific cells. Once you start moving cells around this can screw with your data.\nYou will also want to create a text-file (a typical filename would be README) that keeps track of all your files and manipulations so that future you or a collaborator can easily understand and replicate any steps that you take.\n\n\nThe goal of QC is to find erroneous data. This means that it is generally going to stick out from the rest of the values in a specific column9.9 Errors are not the same as outliers. Sometimes you know that e.g. certain values cannot be true, for example if all your sample locations where in the northern hemisphere then you cannot have latitudes from the southern hemisphere.\nWe can generally make the assumption that the vast majority of your data is correct. This means that if we sort the values in a column if there are a few errors they will stick out and in many cases they will sort at the very top or very bottom. For example, if your column is numeric anything that is a character will pop out or if you have null values or empty cells they will generally sort to the bottom of a column.\n\n\n\n\n\n\nBe mindful\n\n\n\nAny time you are going to sort data, make sure that you are sorting the entire dataset not just a single column or you will corrupt your data set and everything will end up scrambled.\nGenerally, if you don’t have any empty columns or too much missing data if you place the cursor in a cell with a value you can use the shortcut Ctrl/Cmd + A to select all.\nAlways double check that you have expanded your sort to the entire data set\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn the scratch folder, open up the catchdata_messy Google Sheet. Then sort each column and see which errors you can spot.\nMake the entire data set is highlighted. Then go select Data &gt; Sort range &gt; Advanced sorting options. Make sure you check the Data has header row option. Then use the sort by drop down menu to select the column you want to sort. Once you are ready, cleock sort.\nThen inspect your column to determine if there are unexpected values and describe your observations. Discuss what you will do with you findings - consider whether it is better (more ethical/responsible) to remove them or correct them.\n\n\nSimilarly, we can use conditional formatting which allows you to apply specific rules for automatically color coding to a column based on specific rules. This makes it easier for unusual entries or entries outside the possible boundaries to stand out.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn the catchdata_messy Google Sheet, highlight the STL column. The select Format &gt; Conditional formatting from the toolbar. This will pull up a dialog on the right hand of your window.\nSimilar to the Data Validation dialog, you can select the range you want to apply this rule to either by typing it in or selecting it directly in the spreadsheet.\nClick on the Format rules dropdown menus and look through the available options. Let’s say that we know that the sharks cannot be smaller than 50cm or larger than 2m. Set up rules for conditional highlighting that will allow you to quickly pull out invalid entries.\nClick Done once you are all set and evaluate your results.\n\n\nEspecially for smaller data sets being able to quickly scan for errors can be really helpful, however, down the line we will also learn how to use similar principles to identify errors using R.\n\n\n\n\n\n\n Consider this\n\n\n\nArgue the pros and cons of doing QA/QC directly in the spreadsheet compared to using a command-line program like R."
  },
  {
    "objectID": "05_data-management.html#exporting-data",
    "href": "05_data-management.html#exporting-data",
    "title": "5  Data management using spreadsheets",
    "section": "5.6 Exporting data",
    "text": "5.6 Exporting data\nGenerally, want to make sure that we are storing our data in a universally accessible, open, and static format rather than e.g. the default Excel file format (*.xls or *.xlsx).\n\nExcel files have a proprietary format and it is possible that in the future technology will change and you will no longer be able to access your files.\nother program may not be able to read Excel formatted files.\ndifferent version of Excel may handle data differently which can lead to inconsistencies.\nfrequently journals or grant agencies require you to deposit your data in a data repository that only accepts certain formats which may not include Excel.\n\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether you think Google Sheets has the same problems or if it is an acceptable format to avoid these issues.\n\n\nText-based formats such as comma-delimited (*.csv) or tab-delimited (*.txt or *.txv) files overcome these issues. In CSV files, columnes are separated by commas and in tab-delimited files by tabs10. The advantage of text files is that they can be opened in any plain text editors11 but you can also import them into spreadsheet programs or command-line programs like R.10 This will look like whitespace if you look at it in a text editor, but tabs, but using whitespace can cause issues when command-line files parse them, tabs are less ambiguous11 Your operating system will have a built in plain text editors such as notepad. However, you are regularly operating with textfiles it can be helpful to have a more powerful program like Notepad++ or Atom.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOpen the Excel spreadsheet longline_catchdata.xlsx in the data folder of the project directory you downloaded.\nSelect File &gt; Save as from the ribbon, then select Comma Separated Values (*.csv) from the list. Double check the file location and name then click Save.\nNow, repeat the same process to export as a tab-delimited file. You will have multiple options to export as text file, make sure that it says tab-delimited.\nOpen both files in a text editor and compare them. If you double click on a *.csv file your computer will typically open it in excel, you may need to right click and then select open with to open in a text editor.\n\n\nYou will find that I habitually use tab-delimited files because in Germany we use a , instead of a . for our decimals which means that data values can include commas and therefore exporting as *.csv files can cause a bit of chaos. However, as you pick a data set for your course project you will more likely end up with *.csv files.\n\n\n\n\n\n\nTip\n\n\n\nGoogle Sheets now make it a lot easier to export and download copies of spreadsheets in different formats including *.csv by selecting File &gt; Download from the main toolbar and the choosing comma-delimited or tab-delimited from the drop down menu."
  },
  {
    "objectID": "05_data-management.html#acknowledgments",
    "href": "05_data-management.html#acknowledgments",
    "title": "5  Data management using spreadsheets",
    "section": "5.7 Acknowledgments",
    "text": "5.7 Acknowledgments\nThis chapter is adapted from data carpentries “Data Organization in Spreadsheets for Ecologists lesson."
  },
  {
    "objectID": "06_data-frames.html#reading-data-into-r",
    "href": "06_data-frames.html#reading-data-into-r",
    "title": "6  Intro to dataframes",
    "section": "6.1 Reading data into R",
    "text": "6.1 Reading data into R\nlibrary(tidyverse) is actually loading a set of packages used for data science that share a common design philosophy, and “grammar”. One of the packages we loaded is called readr which contains functions for reading in and parsing files.\nAt the end of Chapter 5 when we were exploring the usefulness of spreadsheets for data entry and management. You would have export the excel file with the catch data as *.csv (comma delimited) and as a tab-delimited text file (*.txt or *.tsv if exporting from google sheets).\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse ?read_delim to pull up the help page for the function we will using and explore the arguments. How do you think we read in our csv file?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nread_delim has two required arguments, the path (data/longline_catchdata.csv) which tells R where your file is located and the delimiter in this case a comma (,) tells R how columns are separated from each other.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.csv\", delim = \",\")\n\nAs we access data sets that are not as “clean” as the one we have here, you will find that some of the other arguments apart from specifying the delimiter will come in handy - but don’t worry about those for now.\n\n\n\nExecute the code. If you look over in your environment pane you should now see the object catch. This is your dataframe. Click on it, you should see the command View(catch) in your console and a tab catch appear in your top left pane.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nBased on how you read in the csv file how would you read in the tab-delimited version?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to change delimiter to indicate that it is tab-delimited. In this case, we would specify it as \\t is “computer” for tab.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\nAnother delimiter you might encounter are white space (\" \") but technically it could be anything.\n\n\n\nWhen you loaded your data set you should have seem an message along the lines of parsed with column specification and information on the number of columns and their data type. What this means is that read_delim() looks through the first 1,000 rows for each column and guesses the data type - usually this works pretty well though occasionally we will have to either specify the data types manually using the col_types argument or convert the data type later on.\nLet’s use class() to figure out what type of object we are dealing with.\n\nclass(catch)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nYou can see that this object actually has multiple classes attached to it. The last one in the list is data.frame which is the standard format for (rectangular) tabular data.\nRecall from our tutorial on vectors that each column in a data.frame is an atomic vector, they must all have the same length (hence, “rectangular”) and each must contain the same data type (characters, integers, …).\nThe other three have the same basic properties as a data.frame along with some additional features. The tbl (pronounced tibble) was designed to be at the center of the tidyverse which means that when you use readr functions it will automatically be read in as a tibble and data.frame. If you do some exploring and/or troubleshooting on the web you will likely run into tibbles but for our intents of purposes we will use data.frame when talking about data in a rectangual, tabular shape."
  },
  {
    "objectID": "06_data-frames.html#inspecting-your-data.frame",
    "href": "06_data-frames.html#inspecting-your-data.frame",
    "title": "6  Intro to dataframes",
    "section": "6.2 Inspecting your data.frame",
    "text": "6.2 Inspecting your data.frame\nYou have of course already peaked at the data when you opened it in excel to export it in a text-based format. But not infrequently, you may access data from a public database or a collaborator might share a text-based formatted data set with you and the first thing that you are going to want to do is figure out what information is contained in the data set.\n\n\n\n\n\n\n Consider this\n\n\n\nYou know that this data set is the result from a long-lining survey and you’re now basically an expert in formatting data - what information do you expect to find in this data set? How would you expect it to be formatted if this is a ‘tidy data set’.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThere are several functions that you can use for a preliminary inspection of your data, including figuring out what dimensions it has and what information is contained in your data set.\nCall the following functions on your object and describe what each function does, what information you can learn about your data set from executing them, and when these could be helpful.\n\ndim(), nrow(), ncol()\nstr(), summary()\nhead(), tail()\ncolnames(), rownames()\nView()"
  },
  {
    "objectID": "06_data-frames.html#subsetting-your-dataframe",
    "href": "06_data-frames.html#subsetting-your-dataframe",
    "title": "6  Intro to dataframes",
    "section": "6.3 Subsetting your dataframe",
    "text": "6.3 Subsetting your dataframe\nSimilar to the way we were able to subset vectors, we can do the same things with our data.frames using rows and columns as our “coordinates” in the format data_frame[row_index, column_index].\n\n6.3.1 Using coordinates\nSo for example we can extract the first row and column from our catch object as\n\ncatch[1, 1]\n\n# A tibble: 1 × 1\n  Site       \n  &lt;chr&gt;      \n1 Aransas_Bay\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you extract the 5th column from the third row?\n\n\nYou can select multiple rows or columns by specifying them using a vector.\n\ncatch[c(1, 20, 40), c(2, 5)]\n\n# A tibble: 3 × 2\n  Species         PCL\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Bagre_marinus    NA\n2 Bagre_marinus    NA\n3 Bagre_marinus    NA\n\n\nYou can also select a set of adjacent rows (columns) using : as so\n\ncatch[500:505, 2:5]\n\n# A tibble: 6 × 4\n  Species       Sex   Observed_Stage   PCL\n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Bagre_marinus U     &lt;NA&gt;              NA\n2 Bagre_marinus U     &lt;NA&gt;              NA\n3 Bagre_marinus U     &lt;NA&gt;              NA\n4 Bagre_marinus U     &lt;NA&gt;              NA\n5 Bagre_marinus U     &lt;NA&gt;              NA\n6 Bagre_marinus U     &lt;NA&gt;              NA\n\n\nYou can exclude indices using -\n\ncatch[1:5, -1]\n\n# A tibble: 5 × 11\n  Species     Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day Month\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bagre_mari… U     &lt;NA&gt;              NA   287   353        10     1    28     7\n2 Bagre_mari… U     &lt;NA&gt;              NA   425   495        10     1    28     7\n3 Bagre_mari… U     &lt;NA&gt;              NA   416   502        15     1    28     7\n4 Bagre_mari… U     &lt;NA&gt;              NA   416   507        10     1    28     7\n5 Bagre_mari… U     &lt;NA&gt;              NA   418   510        15     1    28     7\n# ℹ 1 more variable: Year &lt;dbl&gt;\n\n\nYou can select all columns of a given row by leaving the column index blank; for example if we want to extract the first row.\n\ncatch[1, ]\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you extract the entire 5th column?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are quite a few ways to do this:\n\nyou can use indices by number (as we have done up until this point)\ninstead of index numbers you can use the column name\n\nHere are two options using indices:\n\ncatch[, 1]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\ncatch[1]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\n\nInstead of using indices you can also call their column names directly - both of these options will return a data.frame.\n\ncatch[\"Site\"]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\ncatch[, \"Site\"]\n\n# A tibble: 2,325 × 1\n   Site       \n   &lt;chr&gt;      \n 1 Aransas_Bay\n 2 Aransas_Bay\n 3 Aransas_Bay\n 4 Aransas_Bay\n 5 Aransas_Bay\n 6 Aransas_Bay\n 7 Aransas_Bay\n 8 Aransas_Bay\n 9 Aransas_Bay\n10 Aransas_Bay\n# ℹ 2,315 more rows\n\n\n\n\n\n\n\n6.3.2 Extracting columns as vectors\nUsing [] will always return a subset of your dataframe as a data frame. Occassionally, we might want to extract the column as a vector. You can do this using square brackets [[]] or $.\n\ncatch[[\"Site\"]]\n\n   [1] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [4] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [7] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [10] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [13] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [16] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [19] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [22] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [25] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [28] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [31] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [34] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [37] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [40] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [43] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [46] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [49] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [52] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [55] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [58] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [61] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [64] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [67] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [70] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [73] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [76] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [79] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [82] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [85] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [88] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [91] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [94] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [97] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [100] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [103] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [106] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [109] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [112] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [115] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [118] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [121] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [124] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [127] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [130] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [133] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [136] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [139] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [142] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [145] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [148] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [151] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [154] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [157] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [160] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [163] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [166] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [169] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [172] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [175] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [178] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [181] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [184] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [187] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [190] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [193] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [196] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [199] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [202] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [205] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [208] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [211] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [214] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [217] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [220] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [223] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [226] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [229] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [232] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [235] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [238] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [241] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [244] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [247] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [250] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [253] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [256] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [259] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [262] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [265] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [268] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [271] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [274] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [277] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [280] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [283] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [286] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [289] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [292] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [295] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [298] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [301] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [304] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [307] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [310] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [313] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [316] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [319] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [322] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [325] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [328] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [331] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [334] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [337] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [340] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [343] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [346] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [349] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [352] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [355] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [358] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [361] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [364] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [367] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [370] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [373] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [376] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [379] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [382] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [385] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [388] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [391] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [394] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [397] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [400] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [403] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [406] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [409] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [412] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [415] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [418] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [421] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [424] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [427] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [430] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [433] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [436] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [439] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [442] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [445] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [448] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [451] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [454] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [457] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [460] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [463] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [466] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [469] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [472] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [475] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [478] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [481] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [484] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [487] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [490] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [493] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [496] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [499] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [502] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [505] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [508] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [511] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [514] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [517] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [520] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [523] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [526] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [529] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [532] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [535] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [538] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [541] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [544] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [547] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [550] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [553] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [556] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [559] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [562] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [565] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [568] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [571] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [574] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [577] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [580] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [583] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [586] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [589] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [592] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [595] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [598] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [601] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [604] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [607] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [610] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [613] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [616] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [619] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [622] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [625] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [628] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [631] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [634] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [637] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [640] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [643] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [646] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [649] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [652] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [655] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [658] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [661] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [664] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [667] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [670] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [673] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [676] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [679] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [682] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [685] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [688] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [691] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [694] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [697] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [700] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [703] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [706] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [709] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [712] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [715] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [718] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [721] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [724] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [727] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [730] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [733] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [736] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [739] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [742] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [745] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [748] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [751] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [754] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [757] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [760] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [763] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [766] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [769] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [772] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [775] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [778] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [781] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [784] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [787] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [790] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [793] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [796] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [799] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [802] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [805] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [808] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [811] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [814] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [817] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [820] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [823] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [826] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [829] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [832] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n [835] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [838] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [841] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [844] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [847] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [850] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [853] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [856] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [859] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [862] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [865] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [868] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [871] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [874] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [877] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [880] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [883] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [886] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [889] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [892] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [895] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [898] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [901] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [904] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [907] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [910] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [913] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [916] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [919] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [922] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [925] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [928] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [931] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [934] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [937] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [940] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [943] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [946] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [949] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [952] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [955] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [958] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [961] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [964] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [967] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [970] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [973] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [976] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [979] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [982] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [985] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [988] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [991] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [994] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [997] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1000] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1003] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1006] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1009] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1012] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1015] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1018] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1021] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1024] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1027] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1030] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1033] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1036] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1039] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1042] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1045] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1048] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1051] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1054] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1057] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1060] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1063] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1066] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1069] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1072] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1075] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1078] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1081] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1084] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1087] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1090] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1093] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1096] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1099] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1102] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1105] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1108] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1111] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1114] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1117] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1120] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1123] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1126] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1129] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1132] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1135] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1138] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1141] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1144] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1147] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1150] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1153] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1156] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1159] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1162] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1165] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1168] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1171] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1174] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1177] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1180] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1183] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1186] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1189] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1192] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1195] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1198] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1201] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1204] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1207] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1210] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1213] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1216] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1219] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1222] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1225] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1228] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1231] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1234] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1237] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1240] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1243] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1246] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1249] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1252] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1255] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1258] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1261] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1264] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1267] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1270] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1273] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1276] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1279] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1282] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1285] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1288] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1291] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1294] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1297] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1300] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1303] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1306] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1309] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1312] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1315] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1318] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1321] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1324] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1327] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1330] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1333] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1336] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1339] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1342] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1345] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1348] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1351] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1354] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1357] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[1360] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1363] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1366] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1369] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1372] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1375] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1378] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1381] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1384] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1387] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1390] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1393] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1396] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1399] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1402] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1405] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1408] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1411] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1414] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1417] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1420] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1423] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1426] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1429] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1432] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1435] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1438] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1441] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1444] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1447] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1450] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1453] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1456] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1459] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1462] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1465] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1468] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1471] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1474] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1477] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1480] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1483] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1486] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1489] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1492] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1495] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1498] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1501] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1504] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1507] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1510] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1513] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1516] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1519] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1522] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1525] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1528] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1531] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1534] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1537] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1540] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1543] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1546] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1549] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1552] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1555] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1558] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1561] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1564] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1567] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1570] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1573] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1576] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1579] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1582] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1585] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1588] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1591] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1594] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1597] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1600] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1603] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1606] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1609] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1612] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1615] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1618] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1621] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1624] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1627] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1630] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1633] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1636] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1639] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1642] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1645] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1648] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1651] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1654] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1657] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1660] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1663] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1666] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1669] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1672] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1675] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1678] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1681] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1684] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1687] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1690] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1693] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1696] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1699] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1702] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1705] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1708] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1711] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1714] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1717] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1720] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1723] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1726] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1729] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1732] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1735] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1738] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1741] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1744] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1747] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1750] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1753] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1756] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1759] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1762] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1765] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1768] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1771] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1774] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1777] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1780] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1783] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1786] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1789] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1792] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1795] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1798] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1801] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1804] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1807] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1810] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1813] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1816] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1819] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1822] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1825] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1828] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1831] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1834] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1837] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1840] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1843] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1846] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1849] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1852] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1855] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1858] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1861] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1864] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1867] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1870] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1873] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1876] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1879] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1882] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1885] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1888] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1891] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1894] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1897] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1900] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1903] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1906] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1909] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1912] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1915] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1918] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1921] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1924] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1927] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1930] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1933] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1936] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1939] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1942] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1945] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1948] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1951] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1954] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1957] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1960] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1963] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1966] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1969] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1972] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1975] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1978] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1981] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1984] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1987] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1990] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1993] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1996] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1999] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2002] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2005] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2008] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2011] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2014] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2017] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2020] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2023] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2026] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2029] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2032] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2035] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2038] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2041] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2044] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2047] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2050] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2053] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2056] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2059] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2062] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2065] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2068] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2071] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2074] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2077] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2080] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2083] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2086] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2089] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2092] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2095] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2098] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2101] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2104] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2107] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2110] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2113] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2116] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2119] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[2122] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2125] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2128] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2131] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2134] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2137] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2140] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2143] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2146] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2149] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2152] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[2155] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2158] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2161] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2164] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2167] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2170] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2173] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2176] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2179] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2182] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2185] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2188] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2191] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2194] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2197] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2200] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2203] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2206] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2209] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2212] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2215] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2218] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2221] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2224] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2227] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2230] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2233] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2236] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2239] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2242] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2245] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2248] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2251] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2254] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2257] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2260] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2263] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2266] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2269] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2272] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2275] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2278] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2281] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2284] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2287] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2290] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2293] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2296] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2299] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2302] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2305] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2308] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2311] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2314] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[2317] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2320] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2323] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n\ncatch$Site\n\n   [1] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [4] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n   [7] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [10] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [13] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [16] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [19] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [22] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [25] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [28] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [31] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [34] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [37] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n  [40] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [43] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [46] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [49] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [52] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [55] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [58] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [61] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [64] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [67] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [70] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [73] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [76] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [79] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [82] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [85] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [88] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n  [91] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [94] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n  [97] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [100] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [103] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [106] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [109] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [112] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [115] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [118] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [121] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [124] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [127] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [130] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [133] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [136] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [139] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [142] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [145] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [148] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [151] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [154] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [157] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [160] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [163] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [166] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [169] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [172] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [175] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [178] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [181] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [184] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [187] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [190] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [193] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [196] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [199] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [202] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [205] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [208] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [211] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [214] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [217] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [220] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [223] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [226] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [229] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [232] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [235] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [238] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [241] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [244] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [247] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [250] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [253] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [256] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [259] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [262] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [265] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [268] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [271] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [274] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [277] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [280] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [283] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [286] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [289] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [292] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [295] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [298] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [301] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [304] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [307] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [310] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [313] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [316] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [319] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [322] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [325] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [328] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [331] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [334] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [337] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [340] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [343] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [346] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [349] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [352] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [355] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [358] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [361] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [364] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [367] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [370] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [373] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [376] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [379] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [382] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [385] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [388] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [391] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [394] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [397] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [400] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [403] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [406] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [409] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [412] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [415] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [418] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [421] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [424] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [427] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [430] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [433] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [436] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [439] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [442] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [445] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [448] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [451] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [454] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [457] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [460] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [463] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [466] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [469] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [472] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [475] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [478] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [481] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [484] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [487] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [490] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [493] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [496] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [499] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [502] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [505] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [508] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [511] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [514] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [517] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [520] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [523] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [526] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [529] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [532] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [535] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [538] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [541] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [544] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [547] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [550] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [553] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [556] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [559] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [562] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [565] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [568] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [571] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [574] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [577] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [580] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [583] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [586] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [589] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [592] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [595] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [598] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [601] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [604] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [607] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [610] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [613] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [616] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [619] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [622] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [625] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [628] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [631] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [634] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [637] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [640] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [643] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [646] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [649] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [652] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [655] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [658] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [661] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [664] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [667] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [670] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [673] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [676] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [679] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [682] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [685] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [688] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [691] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [694] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [697] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [700] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [703] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [706] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [709] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [712] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [715] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [718] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [721] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [724] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [727] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [730] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [733] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [736] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [739] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [742] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [745] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [748] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [751] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [754] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [757] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [760] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [763] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [766] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [769] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [772] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [775] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [778] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [781] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [784] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [787] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [790] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [793] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [796] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [799] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [802] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [805] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [808] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n [811] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [814] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [817] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [820] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [823] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [826] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [829] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [832] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n [835] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [838] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [841] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [844] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [847] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [850] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [853] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [856] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [859] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [862] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [865] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [868] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [871] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [874] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [877] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [880] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [883] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [886] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [889] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [892] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n [895] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [898] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [901] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [904] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [907] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [910] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [913] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [916] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [919] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [922] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [925] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [928] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [931] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [934] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [937] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [940] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [943] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [946] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [949] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n [952] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [955] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [958] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [961] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [964] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [967] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [970] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [973] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n [976] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n [979] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [982] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [985] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [988] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [991] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n [994] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n [997] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1000] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1003] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1006] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1009] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1012] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1015] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1018] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1021] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1024] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1027] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1030] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1033] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1036] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1039] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1042] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1045] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1048] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1051] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1054] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1057] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1060] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1063] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1066] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1069] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1072] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1075] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1078] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1081] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1084] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1087] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1090] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1093] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1096] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1099] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1102] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1105] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1108] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1111] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1114] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1117] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1120] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1123] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1126] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1129] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1132] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1135] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1138] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1141] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1144] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1147] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1150] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1153] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1156] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1159] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1162] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1165] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1168] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1171] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1174] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1177] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1180] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1183] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1186] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1189] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1192] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1195] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1198] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1201] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1204] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1207] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1210] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1213] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1216] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1219] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1222] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1225] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1228] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1231] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1234] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1237] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1240] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1243] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1246] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1249] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1252] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1255] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1258] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1261] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1264] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1267] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1270] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1273] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1276] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1279] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1282] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1285] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1288] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1291] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1294] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1297] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1300] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1303] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1306] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1309] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1312] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1315] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1318] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1321] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1324] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1327] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1330] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1333] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1336] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1339] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1342] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1345] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1348] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1351] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1354] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1357] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[1360] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1363] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1366] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1369] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1372] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1375] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1378] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1381] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1384] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1387] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1390] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1393] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1396] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1399] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1402] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1405] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1408] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1411] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1414] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1417] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1420] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1423] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1426] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1429] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1432] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1435] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1438] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1441] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1444] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1447] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1450] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1453] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1456] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1459] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1462] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1465] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1468] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1471] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1474] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1477] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1480] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1483] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1486] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1489] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1492] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1495] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1498] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1501] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1504] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1507] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1510] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1513] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1516] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1519] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1522] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1525] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1528] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1531] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1534] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1537] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1540] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1543] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1546] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1549] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1552] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1555] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1558] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1561] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1564] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1567] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1570] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1573] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1576] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1579] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1582] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1585] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1588] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1591] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1594] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1597] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1600] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1603] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1606] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1609] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1612] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1615] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1618] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1621] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1624] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1627] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1630] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1633] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1636] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1639] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1642] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1645] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1648] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1651] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1654] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1657] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1660] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1663] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1666] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1669] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1672] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1675] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1678] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1681] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1684] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1687] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1690] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1693] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1696] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1699] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1702] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1705] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1708] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1711] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1714] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1717] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1720] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1723] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1726] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1729] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1732] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1735] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1738] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1741] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1744] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1747] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1750] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1753] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1756] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1759] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1762] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1765] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1768] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1771] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1774] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[1777] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1780] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1783] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1786] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1789] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1792] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1795] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1798] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1801] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1804] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1807] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1810] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1813] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1816] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1819] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1822] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1825] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1828] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1831] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1834] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1837] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1840] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1843] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1846] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1849] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1852] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1855] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1858] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1861] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1864] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1867] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1870] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1873] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1876] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[1879] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1882] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1885] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1888] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1891] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1894] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[1897] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1900] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1903] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1906] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1909] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1912] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1915] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1918] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1921] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1924] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1927] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1930] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1933] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1936] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1939] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[1942] \"Aransas_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1945] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1948] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1951] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1954] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1957] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[1960] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1963] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1966] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1969] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[1972] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[1975] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1978] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1981] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1984] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1987] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1990] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1993] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1996] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[1999] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2002] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2005] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2008] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2011] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2014] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2017] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2020] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2023] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2026] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2029] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2032] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2035] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2038] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2041] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2044] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2047] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2050] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2053] \"Redfish_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2056] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2059] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2062] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2065] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2068] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2071] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2074] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2077] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2080] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2083] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2086] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2089] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2092] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2095] \"Aransas_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2098] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2101] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2104] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2107] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2110] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2113] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2116] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2119] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Redfish_Bay\"       \n[2122] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2125] \"Redfish_Bay\"        \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2128] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2131] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2134] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2137] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2140] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Redfish_Bay\"       \n[2143] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2146] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2149] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2152] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Corpus_Christi_Bay\"\n[2155] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2158] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2161] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2164] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2167] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2170] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2173] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2176] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2179] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2182] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2185] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Aransas_Bay\"       \n[2188] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2191] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2194] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2197] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2200] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2203] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2206] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2209] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2212] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2215] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2218] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2221] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2224] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2227] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2230] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2233] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2236] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2239] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2242] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2245] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2248] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2251] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2254] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2257] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2260] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2263] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2266] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2269] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2272] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2275] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2278] \"Corpus_Christi_Bay\" \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2281] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2284] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2287] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2290] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2293] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Corpus_Christi_Bay\"\n[2296] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2299] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2302] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2305] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2308] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n[2311] \"Corpus_Christi_Bay\" \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2314] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Aransas_Bay\"       \n[2317] \"Aransas_Bay\"        \"Aransas_Bay\"        \"Aransas_Bay\"       \n[2320] \"Redfish_Bay\"        \"Redfish_Bay\"        \"Redfish_Bay\"       \n[2323] \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\" \"Corpus_Christi_Bay\"\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a vector of all the species in the data set called species.\n\n\nIf you call the vector by typing its name (species) in the console you will notice that it repeats the species names.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nExecute the following code chunk and describe what this function does:\n\nunique(species)\n\n\n\nUsing indices might seem a little bit clunky, e.g. you have to know which column and row is which by position but it has its practical applications and is computationally very fast. For most of our data wrangling we will be using functions from the tidyverse packages dplyr and tidyr which work a little bit more intuitively."
  },
  {
    "objectID": "06_data-frames.html#write-data.frame-to-file",
    "href": "06_data-frames.html#write-data.frame-to-file",
    "title": "6  Intro to dataframes",
    "section": "6.4 Write data.frame to file",
    "text": "6.4 Write data.frame to file\nFrequently, we will process raw data sets and then need to write intermediate or final results to file, for example to share them with collaborators. Here, the readr packages comes in handy.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a subset of our data set consisting of the first 100 rows and containing information on the species, sex and fork length. Then use the help page for the function write_delim() to figure out how to write out a tab-delimited file into your data folder."
  },
  {
    "objectID": "07_data-transformation-i.html#data-wrangling",
    "href": "07_data-transformation-i.html#data-wrangling",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.1 Data Wrangling",
    "text": "7.1 Data Wrangling\nNow that we have a data frame to play with, let’s explore some data wrangling options using dplyr. The d stands for data and the plyr stands for plyers - this package is designed to manipulate data frames3. We are going to focus on the central actions (verbs) that will allow you to manipulate the data frame.3 This is also known as data wrangling or data munging, manipulating is not a negative thing in this case it means you can clean up and format the data in appropriate way to fit the questions you are asking and allow to to transform the information in a helpful way so that you can apply analysis and modeling as needed in the next step\nThe main advantages to using a command line program like R/code compared to a spreadsheet program such as Excel or Google sheets are:\n\nYou aren’t manipulating the raw data set - if you make a mistake or accidentally overwrite something you haven’t made any permanent damage.\nYou can manipulate data sets too large to easily handle in a spreadsheet\nIf you update your data set or have a second identically formatted data set you just have to re-run the code.\n\nBe sure to record all the steps (code chunks) in your quarto document - both the examples given here and the applications you will be asked to make. You can copy and paste, but you will find that writing out the code will help you get more used to syntax, how auto complete etc. works. Be sure to annotate/comment your code as reminders while we go through new functions in class, and that you take the time to go over your comments before submitting your knotted *.html document.\nThese are central concepts that you will use and reuse throughout the semester so you will likely want to refer back to this document. A good way to create a “cheatsheet” would be to for example for each function write a short description of what it does in general before each code chunk, then make your comment in the code specific to your example. Similarly use normal text to refer to the question numbers in this manual as you work through the problem sets."
  },
  {
    "objectID": "07_data-transformation-i.html#selecting-and-organizing-columns",
    "href": "07_data-transformation-i.html#selecting-and-organizing-columns",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.2 Selecting and organizing columns",
    "text": "7.2 Selecting and organizing columns\nLet’s start by loading our data set.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\nBe mindful\n\n\n\nWe will make heavy use of the magrittr pipe %&gt;% this smester which allows you to link commands together; think of it as “and now do this”. R for Data Science (2e) implements the native R pipe |&gt;. For our intents and purposes they are identical.\nBecause we are mostly interested in what the individual functions (verbs) do we will not always assign a new object, but just having it print to the console/below the code chunk we will be able to immediately assess the affect. By piping our function to head() it will print just the first 6 lines.\n\n\nThe function select() is used to select a subset of columns from a data set.\nFor example, you can select just the Site and Species columns4.4 Remember, the function head() allows you to just print the first few lines of the dataframe to the console, otherwise you can end up with several thousand lines!\n\ncatch %&gt;%\n  select(Site, Species) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Site        Species      \n  &lt;chr&gt;       &lt;chr&gt;        \n1 Aransas_Bay Bagre_marinus\n2 Aransas_Bay Bagre_marinus\n3 Aransas_Bay Bagre_marinus\n4 Aransas_Bay Bagre_marinus\n5 Aransas_Bay Bagre_marinus\n6 Aransas_Bay Bagre_marinus\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select just Day, Month, and Year columns?\n\n\nYou can also specify individual columns to eliminate by name. For example, the PCL column doesn’t contain any information (all NAs).\n\ncatch %&gt;%\n  select(-PCL) %&gt;%\n  head()\n\n# A tibble: 6 × 11\n  Site      Species Sex   Observed_Stage    FL   STL Hook_Size   Set   Day Month\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;             287   353        10     1    28     7\n2 Aransas_… Bagre_… U     &lt;NA&gt;             425   495        10     1    28     7\n3 Aransas_… Bagre_… U     &lt;NA&gt;             416   502        15     1    28     7\n4 Aransas_… Bagre_… U     &lt;NA&gt;             416   507        10     1    28     7\n5 Aransas_… Bagre_… U     &lt;NA&gt;             418   510        15     1    28     7\n6 Aransas_… Bagre_… U     &lt;NA&gt;             434   515        10     1    28     7\n# ℹ 1 more variable: Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you eliminate hook size from the data set?\n\n\nYou can also eliminate multiple columns by name, for example you would remove Day, Month and Year like this:\n\ncatch %&gt;%\n  select(-Day, -Month, -Year) %&gt;%\n  head()\n\n# A tibble: 6 × 9\n  Site        Species     Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   287   353        10     1\n2 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   425   495        10     1\n3 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   416   502        15     1\n4 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   416   507        10     1\n5 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   418   510        15     1\n6 Aransas_Bay Bagre_mari… U     &lt;NA&gt;              NA   434   515        10     1\n\n\nIf you want to re-arrange columns in your data frame, you would also use select().\n\ncatch %&gt;%\n  select(FL, Sex, Day) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n     FL Sex     Day\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1   287 U        28\n2   425 U        28\n3   416 U        28\n4   416 U        28\n5   418 U        28\n6   434 U        28\n\n\n\n\n\n\n\n\nProtip\n\n\n\nIf you wanted to move a set of columns to the front, but not not want to have to type in all the other column names you can use everything().\n\ncatch %&gt;%\n  select(Day, Month, Year, everything()) %&gt;%\n  head()\n\n# A tibble: 6 × 12\n    Day Month  Year Site        Species   Sex   Observed_Stage   PCL    FL   STL\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   287   353\n2    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   425   495\n3    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   416   502\n4    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   416   507\n5    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   418   510\n6    28     7  2015 Aransas_Bay Bagre_ma… U     &lt;NA&gt;              NA   434   515\n# ℹ 2 more variables: Hook_Size &lt;dbl&gt;, Set &lt;dbl&gt;\n\n\n\n\nThere you go, creating subsets of columns: Simple as that."
  },
  {
    "objectID": "07_data-transformation-i.html#separating-uniting-columns",
    "href": "07_data-transformation-i.html#separating-uniting-columns",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.3 Separating & uniting columns",
    "text": "7.3 Separating & uniting columns\nOccasionally you will find that you want to combine the contents of two columns into a single column (e.g. first name, last name) or at other times you may want to separate the contents of a column over multiple columns (e.g. dates).\nFor example, you may have noticed that the Species is entered as genus_species - what if you wanted to have two separate columns with that information?\nThe function separate() will split the contents from one column across two or more columns. To do this you need to specify the new column names (into = c(\"column1\", \"column2\")), and what pattern should be used to determine where the content should be split (sep = \"pattern\").\n\ncatch %&gt;%\n  separate(Species, into = c(\"species\", \"genus\"), sep = \"_\", remove = FALSE) %&gt;%\n  head()\n\n# A tibble: 6 × 14\n  Site    Species species genus Sex   Observed_Stage   PCL    FL   STL Hook_Size\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   287   353        10\n2 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   425   495        10\n3 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   416   502        15\n4 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   416   507        10\n5 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   418   510        15\n6 Aransa… Bagre_… Bagre   mari… U     &lt;NA&gt;              NA   434   515        10\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIn some cases, there might not be a distinct pattern that you can use to identify where to split the column content. In this case it may be more helpful to use the position (e.g. “split at”third character from the left”) Look up the separate() function in the help tab and determine how you could split the Year column so you get two new columns by splitting off the last two digits (i.e. 2021 would be 20 and 21). Then eliminate the column containing the first two digits.\n\n\nIn other cases you might have information in two columns that you want to combine into a single column. This can be accomplished using the function unite().\nFor example, if we wanted to create a column called date that had the day, month, and year of each sampling trip separated by an _.\n\ncatch %&gt;%\n  unite(Date, Day, Month, Year, sep = \"_\", remove = FALSE) %&gt;%\n  head()\n\n# A tibble: 6 × 13\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set Date \n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1 28_7…\n2 Aransas_… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1 28_7…\n3 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1 28_7…\n4 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1 28_7…\n5 Aransas_… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1 28_7…\n6 Aransas_… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1 28_7…\n# ℹ 3 more variables: Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a data set with the following columns in this sequence and print the first few rows to the console.\n\nSet_ID (combining day, month, year, and set)\nGenus\nSpecies\nFL\nSTL"
  },
  {
    "objectID": "07_data-transformation-i.html#sorting-dataframes-by-a-specific-column-content",
    "href": "07_data-transformation-i.html#sorting-dataframes-by-a-specific-column-content",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.4 Sorting dataframes by a specific column content",
    "text": "7.4 Sorting dataframes by a specific column content\nUntil you want to visualize a table how the rows are arranged is not really important. However, for example, when generating reports you might want values to be listed in a specific way. This can be done using the function arrange().\nFor example, if we wanted to sort our dataframe based on the Observed_Stage column we could do the following:\n\ncatch %&gt;%\n  arrange(Observed_Stage)\n\n# A tibble: 2,325 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Sphyrn… M     MAT              622   668   850        10     1     8\n 2 Redfish… Sphyrn… M     MAT              656   710   869        10     1     8\n 3 Redfish… Sphyrn… F     MAT              708   770   979        15     2     8\n 4 Corpus_… Sphyrn… M     MAT              695   757   954        10     2    12\n 5 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 6 Corpus_… Sphyrn… M     MAT              621   689   856        10     2    27\n 7 Redfish… Sphyrn… F     MAT              781   853  1020        10     4    29\n 8 Redfish… Sphyrn… M     MAT              721   783   980        10     3    11\n 9 Redfish… Carcha… U     UND               NA    NA    NA        15     2    16\n10 Corpus_… Sphyrn… U     UND               NA    NA    NA        10     1    27\n# ℹ 2,315 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would sort your table by Sex?\n\n\nBy default, characters are sorted alphabetically, numeric columns from smallest to largest value. If you want to order your values from largest to smallest, you can specify that using desc()\n\ncatch %&gt;%\n  arrange(desc(FL))\n\n# A tibble: 2,325 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n 2 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 3 Redfish… Carcha… M     &lt;NA&gt;             792   882  1092        15     2    16\n 4 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 5 Redfish… Sphyrn… F     MAT              781   853  1020        10     4    29\n 6 Corpus_… Sciaen… U     &lt;NA&gt;              NA   841   950        10     3    25\n 7 Redfish… Carcha… M     &lt;NA&gt;             740   840  1010        15     3    29\n 8 Redfish… Carcha… M     &lt;NA&gt;             740   820  1020        10     4     1\n 9 Aransas… Carcha… M     &lt;NA&gt;             720   812   912        15     4    22\n10 Redfish… Sphyrn… M     MAT              721   783   980        10     3    11\n# ℹ 2,315 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would sort your site column from Z to A?"
  },
  {
    "objectID": "07_data-transformation-i.html#filtering-subsetting-rows",
    "href": "07_data-transformation-i.html#filtering-subsetting-rows",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.5 Filtering (subsetting) rows",
    "text": "7.5 Filtering (subsetting) rows\nFrequently, we are less interested in being able to sort columns by content, rather, we want to extract a subset of rows based on specific content.\nThe function filter() is used to subset a data frame by row based on regular expressions and the boolean operators we previously encounter to describe the content of sets of rows.\nFor example, we might a data.frame with only Gafftop sail catfish (Bagre marinus)5.5 Remember for exact matches we use == not =\n\ncatch %&gt;%\n  filter(Species == \"Bagre_marinus\")\n\n# A tibble: 1,511 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 1,501 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select only rows containing Scalloped Hammerheads (Sphyrna lewini)?”\n\n\nIf we want all rows but Gafftop sailfish you can use a ! to say “not that” instead of having to list all the species that you do want to keep6.6 This is frequently called “blacklisting”, while creating a list of content that you do want to keep would be referred to as “whitelisting”.\n\ncatch %&gt;%\n  filter(!Species == \"Bagre_marinus\")\n\n# A tibble: 814 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Rhizop… M     &lt;NA&gt;             351   378   433        15     1    29\n 2 Redfish… Sphyrn… F     &lt;NA&gt;             470   430   600        10     3    29\n 3 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   670        15     3    29\n 4 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   340        10     3    29\n 5 Redfish… Hypanu… M     &lt;NA&gt;              NA    NA   810        10     4    29\n 6 Corpus_… Carcha… F     &lt;NA&gt;             609   670   820        15     1    30\n 7 Corpus_… Sphyrn… M     &lt;NA&gt;             495   485   615        10     2    24\n 8 Corpus_… Sphyrn… F     &lt;NA&gt;             550   370   720        10     2    24\n 9 Corpus_… Sphyrn… M     &lt;NA&gt;             470   505   645        10     3    24\n10 Corpus_… Sphyrn… F     &lt;NA&gt;             540   565   720        10     3    24\n# ℹ 804 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a dataframe without Scalloped Hammerheads (Sphyrna lewini) entries?\n\n\nSometimes you might want to select rows that match one of a set of values7. In this case we would use %in% to indicate “keep any of these”.7 Recall, the function c() (concatenate) creates a vector\n\ncatch %&gt;%\n  filter(Species %in% c(\"Sciades_felis\", \"Bagre_marinus\", \"Synodus_foetens\"))\n\n# A tibble: 2,166 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 2,156 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nAgain, if you wanted everything but rows containing those values you would preface it with a !.\n\ncatch %&gt;%\n  filter(!Species %in% c(\"Sciades_felis\", \"Bagre_marinus\", \"Synodus_foetens\"))\n\n# A tibble: 159 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Redfish… Rhizop… M     &lt;NA&gt;             351   378   433        15     1    29\n 2 Redfish… Sphyrn… F     &lt;NA&gt;             470   430   600        10     3    29\n 3 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   670        15     3    29\n 4 Redfish… Hypanu… F     &lt;NA&gt;              NA    NA   340        10     3    29\n 5 Redfish… Hypanu… M     &lt;NA&gt;              NA    NA   810        10     4    29\n 6 Corpus_… Carcha… F     &lt;NA&gt;             609   670   820        15     1    30\n 7 Corpus_… Sphyrn… M     &lt;NA&gt;             495   485   615        10     2    24\n 8 Corpus_… Sphyrn… F     &lt;NA&gt;             550   370   720        10     2    24\n 9 Corpus_… Sphyrn… M     &lt;NA&gt;             470   505   645        10     3    24\n10 Corpus_… Sphyrn… F     &lt;NA&gt;             540   565   720        10     3    24\n# ℹ 149 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you subset a data frame to contain only entries for different species in the genus Carcharhinus aka the sharky-sharks? There are four species in the data set - bullsharks (Carcharhinus leucas), spinner sharks (Carcharhinus brevipinna), blacktip sharks (Carcharhinus limbatus), and smalltail sharks (Carcharhinus porosus).\n\n\nFor numbers you likely aren’t just searching for exact matches, you also want to be able to set threshold values and select everything above or below. For example, you can select all rows with values greater than a certain value using &gt;.\n\ncatch %&gt;%\n  filter(FL &gt; 440)\n\n# A tibble: 907 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   496   565        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   476   569        10     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   495   570        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   490   575        10     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   486   581        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   503   589        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   489   590        10     1    28\n# ℹ 897 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCreate a data frame containing only entries with a forklength shorter than 300mm, then create a data frame that contains only entries with a forklength equal to or smaller than 300mm.\n\n\nIn this second piece of code you used a single boolean operator to include two conditions, “smaller than” and “equal two”. That is a special case of wanting to retain data than fulfills one of either of two conditions and we have a specific boolean operator that can combine the two.\nThis is not always the case, for example, you might want to retain data that fulfills conditions in two different columns. In this case you can combine expressions using & to indicate that it must fulfill all conditions indicated or | to indicate that it must retain at least one of the.\nFor example to select only scalloped hammerheads that are also smaller than 300 cm you would use\n\ncatch %&gt;%\n  filter(Species == \"Sphyrna_lewini\" & FL &lt; 300)\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Corpus_C… Sphyrn… F     &lt;NA&gt;             192   210   280        15     3     6\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you subset a data frame to contain only entries for different species in the genus Carcharhinus that have a forklength larger than 500 cm?\n\n\nBy contrast, if you wanted all entries that are either gafftops or a fork length smaller than 300 cm you could use the following code:\n\ncatch %&gt;%\n  filter(Species == \"Sphyrna_lewini\" | FL &lt; 300)\n\n# A tibble: 409 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   285   314        10     2    28\n 3 Corpus_… Bagre_… U     &lt;NA&gt;              NA   299   348        10     1    30\n 4 Corpus_… Bagre_… U     &lt;NA&gt;              NA   297   367        10     2    30\n 5 Corpus_… Bagre_… U     &lt;NA&gt;              NA   298   362        10     3    30\n 6 Corpus_… Bagre_… U     &lt;NA&gt;              NA   290   350        10     2    24\n 7 Redfish… Bagre_… U     &lt;NA&gt;              NA   254   284        10     4     8\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA    50   574        10     1    25\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   280   340         3     3    25\n10 Redfish… Bagre_… U     &lt;NA&gt;              NA   294   353        10     4    16\n# ℹ 399 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you select fish that have a forklength either smaller than 200 cm or larger than 300cm?"
  },
  {
    "objectID": "07_data-transformation-i.html#sneak-peak-grouping-rows-for-specific-wrangling-actions",
    "href": "07_data-transformation-i.html#sneak-peak-grouping-rows-for-specific-wrangling-actions",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.6 Sneak peak: grouping rows for specific wrangling actions",
    "text": "7.6 Sneak peak: grouping rows for specific wrangling actions\nWe have already see that it can be helpful to subset rows based on conditions that are met by the content of more than one column. In those cases, we were creating conditions based on Boolean operators.\nIn many cases we might be interested in subsetting a dataframe in a way where our conditions cannot be expressed by a TRUE/FALSE scenario using Boolean operators.\nFor example, we might want to extract the data entry for the longest fish in the data set based on forklength.\nThe function max() can be used to get the maximum value for a vector of numbers. In this case, the vector we are looking at is the FL column of the catch dataframe.\n\ncatch %&gt;%\n  filter(FL == max(FL, na.rm = TRUE))\n\n# A tibble: 1 × 12\n  Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aransas_… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nThat’s great, now we now what the largest fish is that we caught.\nWhat about if we wanted to subset the dataframe to retain the largest fish based on forklength for each species?\n\n\n\n\n\n\n Consider this\n\n\n\nConceptually lay out the individual steps that you would need to complete to do this (don’t worry about whether or not you actually know how to code this).\n\n\nThe tidyverse has a central concept call “split-apply-combine”, which means that occasionally we want to group entries in a dataframe (split), do some sort of manipulation (apply), but end up with a single data frame (combine). We will look at how useful this is in the next chapter but let’s take a quick sneak peak at how this is implemented in dplyr using group_by().\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  filter(FL == max(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# A tibble: 12 × 12\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Corpus_… Bagre_… U     &lt;NA&gt;              NA   575   640        10     2    24\n 2 Aransas… Rhizop… F     &lt;NA&gt;             580   637   790        10     2    25\n 3 Aransas… Carcha… F     &lt;NA&gt;            1042  1140  1410        15     1    25\n 4 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 5 Corpus_… Sphyrn… F     MAT              760   861  1090        10     2    12\n 6 Aransas… Carcha… F     &lt;NA&gt;             690   757   940        10     2    22\n 7 Corpus_… Sphyrn… F     &lt;NA&gt;             520   578   770        10     1    21\n 8 Corpus_… Carcha… U     &lt;NA&gt;             335   415   475        10     1     3\n 9 Aransas… Sciade… U     &lt;NA&gt;              NA   480   548        10     2    18\n10 Aransas… Sciade… U     &lt;NA&gt;              NA   480   580        10     2    18\n11 Corpus_… Sciaen… U     &lt;NA&gt;              NA   841   950        10     3    25\n12 Corpus_… Synodu… U     &lt;NA&gt;              NA   173   185        10     3    30\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nThis is also an example of how we can use the pipe (%&gt;%) to string a bunch of commands, in this example we are saying “take the object catch, and then group rows by Species and then for each group retain only the maximum forklength value for that group and then ungroup them again.”\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you group rows by Species and then retain the individual caught on the largest hook size for each species?\n\n\n\n\n\n\n\n\nProtip\n\n\n\nSpecifically for cases where we want to retain the largest or smallest values, we can use family of of functions called slice() which allow us to subset rows based on their position.\nFor example, we can retain the largest 5 individuals per species based on forklength using slice_max()\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  slice_max(order_by = FL, n = 5)\n\n# A tibble: 66 × 12\n# Groups:   Species [14]\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Corpus_… Bagre_… U     &lt;NA&gt;              NA   575   640        10     2    24\n 2 Corpus_… Bagre_… U     &lt;NA&gt;              NA   574   676        10     2    21\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   565    NA        10     1    17\n 4 Redfish… Bagre_… U     &lt;NA&gt;              NA   564   651        15     1    29\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   555   541        10     3    13\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   555    NA        15     1    17\n 7 Redfish… Carcha… F     &lt;NA&gt;             812   900  1090        15     1    16\n 8 Redfish… Carcha… M     &lt;NA&gt;             792   882  1092        15     2    16\n 9 Redfish… Carcha… M     &lt;NA&gt;             740   820  1020        10     4     1\n10 Redfish… Carcha… M     &lt;NA&gt;             660   722   880        10     1    20\n# ℹ 56 more rows\n# ℹ 2 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;"
  },
  {
    "objectID": "07_data-transformation-i.html#create-a-subset-with-only-unique-entries",
    "href": "07_data-transformation-i.html#create-a-subset-with-only-unique-entries",
    "title": "7  Data Transformation: Organizing rows & Columns",
    "section": "7.7 Create a subset with only unique entries",
    "text": "7.7 Create a subset with only unique entries\nOccasionally, you might want to create a subset of the data set that shows only the unique (distinct) entries for a specific column; this is especially common during an exploratory analysis of a data set that you are getting an overview of. This can be achieved using the function distinct().\nFor example, we might want to know which years the survey took place.\n\ncatch %&gt;%\n  distinct(Year)\n\n# A tibble: 4 × 1\n   Year\n  &lt;dbl&gt;\n1  2015\n2  2016\n3  2017\n4  2018\n\n\nNotice how that dropped all the other columns. You can switch that off using .keep_all = FALSE.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you produce a table with only one representative per species?\n\n\nYou can also combine columns. For example if we wanted to determine the individual sets of the data set we could use\n\ncatch %&gt;%\n  distinct(Day, Month, Year, Set)\n\n# A tibble: 197 × 4\n     Day Month  Year   Set\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    28     7  2015     1\n 2    28     7  2015     2\n 3    28     7  2015     3\n 4    28     7  2015     4\n 5    29     7  2015     1\n 6    29     7  2015     2\n 7    29     7  2015     4\n 8    30     7  2015     1\n 9    30     7  2015     2\n10    30     7  2015     3\n# ℹ 187 more rows\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you produce a table showing all the species caught per station using distinct(), presented as arranged alphabetically by Site and Species within Site?"
  },
  {
    "objectID": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "href": "B_shark-wrangling.html#essential-fish-habitat-shark-nurseries",
    "title": "Data/Shark wrangling",
    "section": "Essential fish habitat: Shark Nurseries",
    "text": "Essential fish habitat: Shark Nurseries\nThe Magnuson-Stevens Act (1996) defined essential fish habitat as “those waters and substrate necessary to fish for spawning, breeding, feeding or growth to maturity”, i.e. they are habitats necessary for an organism to complete their life cycle. Identifying essential fish habitats is critical for management and conservation plans because it enables policy makers to prioritize certain ecosystems.\nWhile some elasmobranchs (sharks, rays, skates) inhabit estuaries year round, many use the estuaries for specific purposes such as feeding, mating, gestation, parturition or as nurseries and only inhabit them during specific life history stages. Estuaries are heavily impacted by humans - overfishing, pollution, habitat destruction and altered flow regimes all affect the biological communities they support.\nBroadly, shark nurseries are areas where young are born and/or reside in during maturation. Typically, these would areas that provide additional protection (e.g. mangroves for hiding) and plenty of food.\nShark Nurseries have three defining criteria(Heupel et al. 2018; Heupel, Carlson, and Simpfendorfer 2007):\n\nHeupel, Michelle R., Shiori Kanno, Ana P. B. Martins, Colin A. Simpfendorfer, Michelle R. Heupel, Shiori Kanno, Ana P. B. Martins, and Colin A. Simpfendorfer. 2018. “Advances in Understanding the Roles and Benefits of Nursery Areas for Elasmobranch Populations.” Marine and Freshwater Research 70 (7): 897–907. https://doi.org/10.1071/MF18081.\n\nHeupel, Michelle R., John K. Carlson, and Colin A. Simpfendorfer. 2007. “Shark Nursery Areas: Concepts, Definition, Characterization and Assumptions.” Marine Ecology Progress Series 337 (May): 287–97. https://doi.org/10.3354/meps337287.\n\nan area where sharks are more commonly encountered within compared to outside of.\nan area in which Young-of-the-year (YOY)/juveniles remain in or return to for extended periods of time.\nan area that is repeatedly used across years.\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe how you could design a study to identify estuaries that are shark nurseries."
  },
  {
    "objectID": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "href": "B_shark-wrangling.html#identifying-shark-nurseries-on-the-texas-coast",
    "title": "Data/Shark wrangling",
    "section": "Identifying shark nurseries on the Texas Coast",
    "text": "Identifying shark nurseries on the Texas Coast\nTexas Parks and Wildlife (TPWD) defines eight major estuaries along the Texas coastline and performs regular shore-based gill net surveys for 10 week periods in April - June and September to November.\n\n\n\nFigure 1: Map of major estuaries located along the Texas coast in the northwest Gulf of Mexico (Plumlee et al. 2018).\n\n\nAnalysis of this survey has identify eight elasmobranch species present in these ecosystems (Plumlee et al. 2018):\n\nPlumlee, Jeffrey D., Kaylan M. Dance, Philip Matich, John A. Mohan, Travis M. Richards, Thomas C. TinHan, Mark R. Fisher, and R. J. David Wells. 2018. “Community Structure of Elasmobranchs in Estuaries Along the Northwest Gulf of Mexico.” Estuarine, Coastal and Shelf Science 204 (May): 103–13. https://doi.org/10.1016/j.ecss.2018.02.023.\n\nBull shark\nBonnethead\nCownose ray\nBlacktip shark\nAtlantic stingray\nAtlantic sharpnose shark\nSpinner shark\nScalloped hammerhead\nFinetooth shark\nLemon shark\n\nGill nets generally exclude individuals &gt; 2m.\nMore recently, a multi-year open water long-lining study targeting elasmobranchs was performed in three estuarine locations near Corpus Christi, TX that are considered putative shark nurseries. Here, the sampling period lasted from May to November (Swift and Portnoy 2021).\n\nSwift, Dominic G., and David S. Portnoy. 2021. “Identification and Delineation of Essential Habitat for Elasmobranchs in Estuaries on the Texas Coast.” Estuaries and Coasts 44 (3): 788–800. https://doi.org/10.1007/s12237-020-00797-y.\n\n\n\n\n\n\n Consider this\n\n\n\nDiscuss whether or not you would expect to get similar results from both studies and what factors could result in differences.\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere are some things to consider:\n\ngear-bias (hook size, net-size)\nseasonality (peak use for many coastal elasmobranchs is May - Oct)\nspatial (land-based vs open ocean)\n\n\n\n\nThis study wanted to answer four questions to further understand whether these locations should be considered shark nurseries and therefore as essential fish habitat for specific elasmobranch species.\n\nHow does the composition of elasmobranch communities compare across sites?\nHow does the catch-per-unit-effort (CPUE) per species and life history compare across sites?\nWhat do the sex ratios look like?\nWhat environmental predictors can we use to predict presence of elasmobranchs?\n\nIn this module we will interact with the data set generated for this study to learn how to wrangle data sets using R in the tidyverse and then we will apply those skills to answer these questions."
  },
  {
    "objectID": "08_data-transformation-ii.html#adding-new-variables",
    "href": "08_data-transformation-ii.html#adding-new-variables",
    "title": "8  Data Transformation:",
    "section": "8.1 Adding new variables",
    "text": "8.1 Adding new variables\nSo,turns out selecting columns and filtering based on content in rows is pretty straightforward.\nBut frequently when we are processing our raw data sets we end up wanting to compute additional metrics or use the existing raw data to create new categories.\nThe function mutate() can be used to create new columns. Frequently, this is done based on columns already existing in the data frame. This is a very powerful function with endless possibilities, but we are going to stick to some of the basics for now3.3 Rest assured if your answer is “Oh, could I …” the answer is “Yes”.\nLet’s say you wanted create a column that contained the difference between the fork length and the stretch total length4:4 By default mutate() appends (adds) the new column as the last column. So we can see our results better we’ll used select() to move it to be the first column in the dataframe)\n\ncatch %&gt;%\n  mutate(difference = STL - FL) %&gt;%\n  select(difference, everything())\n\n# A tibble: 2,325 × 13\n   difference Site      Species Sex   Observed_Stage   PCL    FL   STL Hook_Size\n        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1         66 Aransas_… Bagre_… U     &lt;NA&gt;              NA   287   353        10\n 2         70 Aransas_… Bagre_… U     &lt;NA&gt;              NA   425   495        10\n 3         86 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   502        15\n 4         91 Aransas_… Bagre_… U     &lt;NA&gt;              NA   416   507        10\n 5         92 Aransas_… Bagre_… U     &lt;NA&gt;              NA   418   510        15\n 6         81 Aransas_… Bagre_… U     &lt;NA&gt;              NA   434   515        10\n 7         93 Aransas_… Bagre_… U     &lt;NA&gt;              NA   427   520        15\n 8         86 Aransas_… Bagre_… U     &lt;NA&gt;              NA   446   532        10\n 9         73 Aransas_… Bagre_… U     &lt;NA&gt;              NA   465   538        10\n10         89 Aransas_… Bagre_… U     &lt;NA&gt;              NA   450   539        10\n# ℹ 2,315 more rows\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nYou should now have a column called difference at the end of the data frame5.5 Instead of - to substract, you can other mathematical operators such as + to add , * to multiple, and / to divide values when creating a new column.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called ratio, that is the ratio of the fork to stretch total length?\n\n\nYou can also create a column that contains a logical value (TRUE/FALSE). For example we might need a column that indicates if the Sex is unknown.\n\ncatch %&gt;%\n  mutate(unknown_sex = Sex == \"U\") %&gt;%\n  select(unknown_sex, everything())\n\n# A tibble: 2,325 × 13\n   unknown_sex Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size\n   &lt;lgl&gt;       &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10\n 2 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10\n 3 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15\n 4 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10\n 5 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15\n 6 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10\n 7 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15\n 8 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10\n 9 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10\n10 TRUE        Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10\n# ℹ 2,315 more rows\n# ℹ 4 more variables: Set &lt;dbl&gt;, Day &lt;dbl&gt;, Month &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nYou should know have a column called unknown_sex where if the animal that was caught was not sexed contains the value TRUE, if it was identified as male or female it would say FALSE.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called post_2017 that is TRUE if fish were caught after 2017?\n\n\n\n\nFor that last problem, a “conditional mutate” using an ifelse statement (if this then do that, else do that) could have come in handy. Another option is case_when() which allows you to create multiple sets of conditions as opposed to ifelse which sets up a TRUE/FALSE dichotomy (file this information away for “maybe useful later”)."
  },
  {
    "objectID": "08_data-transformation-ii.html#group_by-and-mutate",
    "href": "08_data-transformation-ii.html#group_by-and-mutate",
    "title": "8  Data Transformation:",
    "section": "8.2 group_by() and mutate()",
    "text": "8.2 group_by() and mutate()\nMany problems in data science require you to split your data set into subsets according to some grouping variable, apply a function, and then combine the results. dplyr is designed to make this straightforward; you have already sen an example of this while you were learning about filter().\nSimilarly, you can combine mutate() with group_by().\n\n\nThe function mean() will calculate the mean value of a vector of numbers, the argument na.rm=TRUE tells the function to ignore any NA-values in the data set.\nFor example, let’s say you wanted to create a column that is the difference between the fork length of an individual and the mean fork length of that species.\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  mutate(diff_mean = FL-mean(FL, na.rm = TRUE))\n\n# A tibble: 2,325 × 13\n# Groups:   Species [14]\n   Site     Species Sex   Observed_Stage   PCL    FL   STL Hook_Size   Set   Day\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aransas… Bagre_… U     &lt;NA&gt;              NA   287   353        10     1    28\n 2 Aransas… Bagre_… U     &lt;NA&gt;              NA   425   495        10     1    28\n 3 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   502        15     1    28\n 4 Aransas… Bagre_… U     &lt;NA&gt;              NA   416   507        10     1    28\n 5 Aransas… Bagre_… U     &lt;NA&gt;              NA   418   510        15     1    28\n 6 Aransas… Bagre_… U     &lt;NA&gt;              NA   434   515        10     1    28\n 7 Aransas… Bagre_… U     &lt;NA&gt;              NA   427   520        15     1    28\n 8 Aransas… Bagre_… U     &lt;NA&gt;              NA   446   532        10     1    28\n 9 Aransas… Bagre_… U     &lt;NA&gt;              NA   465   538        10     1    28\n10 Aransas… Bagre_… U     &lt;NA&gt;              NA   450   539        10     1    28\n# ℹ 2,315 more rows\n# ℹ 3 more variables: Month &lt;dbl&gt;, Year &lt;dbl&gt;, diff_mean &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow would you create a new column called that contains the difference between the fork length of an individual and the mean fork length of that species for each month?"
  },
  {
    "objectID": "08_data-transformation-ii.html#create-new-data.frame-based-on-another",
    "href": "08_data-transformation-ii.html#create-new-data.frame-based-on-another",
    "title": "8  Data Transformation:",
    "section": "8.3 Create new data.frame based on another",
    "text": "8.3 Create new data.frame based on another\nNot infrequently we are more interested in summary (descriptive) stats of a data set rather than all the raw data - Tidyverse got you covered with the function summarize().\nFor example, we might want to calculate the mean and standard deviation of the measured fork length.\n\ncatch %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n            sd_FL = sd(FL, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_FL sd_FL\n    &lt;dbl&gt; &lt;dbl&gt;\n1    406.  103.\n\n\n\n\nRemember, that earlier we’ve have used the function max() to obtain the largest value in a vector.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow could you use summarize to get the maximum forklength?\n\n\nThat’s cool but really we could have also just used\n\nmean(catch$FL, na.rm = TRUE)\n\n[1] 405.9179\n\nmax(catch$FL, na.rm = TRUE)\n\n[1] 1140\n\n\nto get that information, since we are only interested in one column (vector).\nsummarize() becomes especially powerful once we leverage group_by() to start calculating summary stats for entries grouped by a grouping variable.\nFor example we can calculate summary stats by species and generate a table to include in a report.\n\ncatch %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n           median_FL = median(FL, na.rm = TRUE),\n           max_FL = max(FL, na.rm = TRUE),\n           min_FL = min(FL, na.rm = TRUE),\n           sd_FL = sd(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# A tibble: 14 × 6\n   Species                    mean_FL median_FL max_FL min_FL sd_FL\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bagre_marinus                 433.      445     575     45  65.4\n 2 Carcharhinus_brevipinna       644.      648     900    489  69.2\n 3 Carcharhinus_leucas           769       702    1140    624 167. \n 4 Carcharhinus_limbatus         613.      579     757    538 101. \n 5 Carcharhinus_porosus          415       415     415    415  NA  \n 6 Hypanus_americanus            NaN        NA    -Inf    Inf  NA  \n 7 Hypanus_sabina                NaN        NA    -Inf    Inf  NA  \n 8 Rhinoptera_bonasus            NaN        NA    -Inf    Inf  NA  \n 9 Rhizoprionodon_terraenovae    412       396     637    306  73.6\n10 Sciades_felis                 299.      297     480    102  41.9\n11 Sciaenops_ocellatus           793       793     841    745  67.9\n12 Sphyrna_lewini                471.      548.    578    210 174. \n13 Sphyrna_tiburo                622.      605     861    370 114. \n14 Synodus_foetens               173       173     173    173  NA  \n\n\n\n\n\n\n\n\n Consider this\n\n\n\nIf you look closely you should see that you are getting a few NA, NaN, -Inf, and Inf values - any guesses why? You might want to pull up the catch data frame in the view panel to see what is going on with those species.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHow could you use summarize() to calculate a range of summary stats for the stretch total length for individuals grouped by sex?\n\n\nSo far, we have been manipulating our data frame using code and printing it directly to the console (and our quarto document). This can be useful for example to generate tables for reports. However, in many cases we want to create a new object that has been manipulated according to our code and then we will further process, visualize, or analyze that dataframe down the line.\n\nsummary &lt;- catch %&gt;%\n  group_by(Species) %&gt;%\n  summarize(mean_FL = mean(FL, na.rm = TRUE),\n           median_FL = median(FL, na.rm = TRUE),\n           max_FL = max(FL, na.rm = TRUE),\n           min_FL = min(FL, na.rm = TRUE),\n           sd_FL = sd(FL, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nWhen you execute this code, you’ll notice that the code (and probably a warning) is printed to the console but there is no output. Instead, if you look at the environment in the bottom left panel you should now see a new object called summary. Per usual, you can pull that up in the Editor/View pane (top left) using either View(summary) in the console or by clicking on the object in the environment.\nYou will be presenting results in reports over the course of the semester, when you knit an quarto file you will get tables formatted in a standard way according to defaults in the resulting html file. If you want finer control over the output, you can use the kable() function. This will allow you to further format the table, for example, you may specify the number of digits printed using the argument digits =.\nBy adding a chunk options for a label as #| label: tbl-sum-stats and table caption as #| tbl-cap: \"Summary statistics for the forklength of each species, you can further modify the output that adheres to typical reporting standards for reports and research articles.\n\nkable(\n  summary,\n  digits = 1\n)\n\n\n\n\n\n\nSpecies\nmean_FL\nmedian_FL\nmax_FL\nmin_FL\nsd_FL\n\n\n\n\nBagre_marinus\n433.4\n445.0\n575\n45\n65.4\n\n\nCarcharhinus_brevipinna\n643.7\n648.0\n900\n489\n69.2\n\n\nCarcharhinus_leucas\n769.0\n702.0\n1140\n624\n167.3\n\n\nCarcharhinus_limbatus\n613.2\n579.0\n757\n538\n101.0\n\n\nCarcharhinus_porosus\n415.0\n415.0\n415\n415\nNA\n\n\nHypanus_americanus\nNaN\nNA\n-Inf\nInf\nNA\n\n\nHypanus_sabina\nNaN\nNA\n-Inf\nInf\nNA\n\n\nRhinoptera_bonasus\nNaN\nNA\n-Inf\nInf\nNA\n\n\nRhizoprionodon_terraenovae\n412.0\n396.0\n637\n306\n73.6\n\n\nSciades_felis\n298.9\n297.0\n480\n102\n41.9\n\n\nSciaenops_ocellatus\n793.0\n793.0\n841\n745\n67.9\n\n\nSphyrna_lewini\n470.8\n547.5\n578\n210\n174.4\n\n\nSphyrna_tiburo\n621.5\n605.0\n861\n370\n114.4\n\n\nSynodus_foetens\n173.0\n173.0\n173\n173\nNA\n\n\n\nTable 8.1: Summary statistics for the forklength of each species in the catch data"
  },
  {
    "objectID": "08_data-transformation-ii.html#combining-verbs",
    "href": "08_data-transformation-ii.html#combining-verbs",
    "title": "8  Data Transformation:",
    "section": "8.4 Combining verbs",
    "text": "8.4 Combining verbs\nWe’ve already combined most of our dplyr verbs with group_by().\nWhen you are wrangling data you will find that making use of the pipe (%&gt;%) to combine select(), filter(), mutate(), and summarize() as a series of commands will be necessary to get your data set in the correct format and further process it.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nExecutre the following the code chunk. Then describe what each line is doing to manipulate the data frame.\n\ncatch %&gt;%\n  select(-PCL, -Hook_Size) %&gt;%\n  separate(Species, into = c(\"genus\", \"species\"), remove = TRUE) %&gt;%\n  unite(Date, Day, Month, Year) %&gt;%\n  filter(genus == \"Carcharhinus\" & Sex %in% c(\"F\", \"M\")) %&gt;%\n  group_by(Site, genus, species, Sex) %&gt;%\n  filter(FL == max(FL)) %&gt;%\n  arrange(species)\n\n# A tibble: 11 × 9\n# Groups:   Site, genus, species, Sex [11]\n   Site               genus species Sex   Observed_Stage    FL   STL   Set Date \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Aransas_Bay        Carc… brevip… M     &lt;NA&gt;             640   792     3 22_9…\n 2 Redfish_Bay        Carc… brevip… F     &lt;NA&gt;             900  1090     1 16_6…\n 3 Redfish_Bay        Carc… brevip… M     &lt;NA&gt;             882  1092     2 16_6…\n 4 Corpus_Christi_Bay Carc… brevip… M     &lt;NA&gt;             699   860     1 25_1…\n 5 Corpus_Christi_Bay Carc… brevip… F     &lt;NA&gt;             704   880     4 2_10…\n 6 Aransas_Bay        Carc… leucas  F     &lt;NA&gt;            1140  1410     1 25_5…\n 7 Corpus_Christi_Bay Carc… leucas  F     YOY              694   854     2 10_6…\n 8 Aransas_Bay        Carc… leucas  M     &lt;NA&gt;             812   912     4 22_9…\n 9 Redfish_Bay        Carc… leucas  M     &lt;NA&gt;             840  1010     3 29_9…\n10 Aransas_Bay        Carc… limbat… F     &lt;NA&gt;             757   940     2 22_6…\n11 Corpus_Christi_Bay Carc… limbat… M     &lt;NA&gt;             610   770     2 30_8…\n\n\n\n\nGenerate the code that will manipulate the data frame as follows6:6 some bullet points may require more than one line of code; you do not have to perform the steps in the sequence presented, play around a little bit to see how to code this more efficiently\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 1:\n\norder columns so Day, Month, Year, Set are at the beginning.\nretain all male individuals in the genus Carcharhinus.\nget rid of columns containing information on observed stage, precaudal length, and hook size\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 2:\n\ncreate a new variable called Set_ID consisting of Day, Month, Year, and Set number.\ndetermine the number of individuals per species per set7.\n\n7 There is a function called n() that allows us to count rows fulfilling a specific condition\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChallenge 3:\n\nremove all gafftops\ncalculate mean forklength for each species by sex and month of the year."
  },
  {
    "objectID": "09_tidy-data.html#producing-tidy-data-sets",
    "href": "09_tidy-data.html#producing-tidy-data-sets",
    "title": "9  Tidy data",
    "section": "9.1 Producing tidy data sets",
    "text": "9.1 Producing tidy data sets\nThe last set of functions that we need to get comfortable with allow us to create tidy data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nList the three characteristics of a tidy data set. Explain why a tidy data set is sometimes also describe as a long data set.\n\n\nLet’s read out data set back into our R session.\n\n# read catch data\ncatch &lt;- read_delim(\"data/longline_catchdata.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at our data set and argue whether or not it is a tidy data set. The easiest way to do this is to determine if it fullfills all the characteristics.\n\n\nLet’s quickly reformat our catch data as follows\n\ncatch_length &lt;- catch %&gt;%\n  unite(SetID, Year, Month, Day, Set, sep = \"_\") %&gt;%\n  select(SetID, Site, Species, Sex, PCL, FL, STL)\n\nhead(catch_length)\n\n# A tibble: 6 × 7\n  SetID       Site        Species       Sex     PCL    FL   STL\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   287   353\n2 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   425   495\n3 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   416   502\n4 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   416   507\n5 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   418   510\n6 2015_7_28_1 Aransas_Bay Bagre_marinus U        NA   434   515\n\n\nWe can turn this into a tidy data set using pivot_longer(). To do this we have to identify columns that will be used as the key (cols =) and then name the column that will hold those values (names_to()) and the column that will hold the value (values_to()).\nIn this case, we have made three observations about length for each specimen, in order to have rows with unique observations we want a column that identifies what type of observation was made, for example called Measurement. This is called the “key” because it allows us to “unlock” what type of measurement the individual observation is, i.e. this column will let us know whether an observation (row) is pre-caudal length, fork length, or stretch total length.\nWe will designate another column Length to hold the values for each measurement.\nWe can identify the columns that need to be gathered either by name or since we have re-arranged our dataframe so they are the last columns by column number.\n\ntidy_length &lt;- catch_length %&gt;%\n  pivot_longer(names_to = \"Measurement\", values_to = \"Length\", cols = 5:7)\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly outline advantages to using tidy data sets.\n\n\nWith this data set it would be straightforward for us to e.g. calculate mean values for each length measurement by species using group_by() and summarize().\n\ntidy_length %&gt;%\n  group_by(Species, Measurement) %&gt;%\n  summarize(mean = mean(Length, na.rm = TRUE))\n\n# A tibble: 42 × 3\n# Groups:   Species [14]\n   Species                 Measurement  mean\n   &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;\n 1 Bagre_marinus           FL           433.\n 2 Bagre_marinus           PCL          NaN \n 3 Bagre_marinus           STL          517.\n 4 Carcharhinus_brevipinna FL           644.\n 5 Carcharhinus_brevipinna PCL          583.\n 6 Carcharhinus_brevipinna STL          804.\n 7 Carcharhinus_leucas     FL           769 \n 8 Carcharhinus_leucas     PCL          691.\n 9 Carcharhinus_leucas     STL          936.\n10 Carcharhinus_limbatus   FL           613.\n# ℹ 32 more rows"
  },
  {
    "objectID": "09_tidy-data.html#convert-a-tidy-data-set-to-wide-format",
    "href": "09_tidy-data.html#convert-a-tidy-data-set-to-wide-format",
    "title": "9  Tidy data",
    "section": "9.2 Convert a tidy data set to wide format",
    "text": "9.2 Convert a tidy data set to wide format\nDespite all the advantages of tidy data sets you can see from the table above that frequently when we are presenting results in a table it may be advantageous in terms of layout to have a non-tidy format.\nThis can be done using pivot_wider() which works like pivot_longer() but in reverse. You designate which column is the key (names_from =), i.e. these will become the column names in the new table. Then you need to identify which column in your current data frame contains the values that should be filled out/spread into the columns that will be generated from your key (values_from =).\nSince we don’t have values for precaudal length, we probably want to use filter() to remove those rows first.\n\n\nMore notes on naming things … recall, that we said that filenames should not contain spaces or special characters? We set similar rules for naming objects. Well, column names is a similar conundrum. Including spaces or species characters as a column name creates problems when we are using functions like select() to subset by column name or mutate() to create new columns based on exisiting columns. Similarly, if the column name is a number you will have problems. If you do have unconvential column names you can rename them using rename() or you can use backticks and either side of the name to indicate that it is a column name.\n\ntidy_length %&gt;%\n  filter(!Measurement == \"PCL\") %&gt;%\n  group_by(Species, Measurement) %&gt;%\n  summarize(mean = mean(Length, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = \"Measurement\", values_from = \"mean\")\n\n# A tibble: 14 × 3\n# Groups:   Species [14]\n   Species                       FL   STL\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1 Bagre_marinus               433.  517.\n 2 Carcharhinus_brevipinna     644.  804.\n 3 Carcharhinus_leucas         769   936.\n 4 Carcharhinus_limbatus       613.  776.\n 5 Carcharhinus_porosus        415   475 \n 6 Hypanus_americanus          NaN   954.\n 7 Hypanus_sabina              NaN   349.\n 8 Rhinoptera_bonasus          NaN   819 \n 9 Rhizoprionodon_terraenovae  412   510.\n10 Sciades_felis               299.  343.\n11 Sciaenops_ocellatus         793   932.\n12 Sphyrna_lewini              471.  628 \n13 Sphyrna_tiburo              622.  792.\n14 Synodus_foetens             173   185 \n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nCalculate the number of individuals per species caught per month in 2018 and present that data in a wide formate to make it easy to compare the number of species (species) per month (columns). As a bonus create an additional column with total catch of that species for 2018."
  },
  {
    "objectID": "10_relational-data.html#composition-of-elasmobranch-communities-compare-across-sites",
    "href": "10_relational-data.html#composition-of-elasmobranch-communities-compare-across-sites",
    "title": "10  Relational data",
    "section": "10.1 Composition of elasmobranch communities compare across sites",
    "text": "10.1 Composition of elasmobranch communities compare across sites\nLet’s start by reading in the data set we will use for this analysis3.3 This is a data set that has been cleaned up to contain only the elasmobranchs caught during the survey since that is the taxonomic group we are interested in\n\nelasmos &lt;- read_delim(\"data/longline_elasmobranchs.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nProduce a table that contains the number of times a species was caught at each site and overall during the long-lining survey and give a brief description of the pattern(s) you see. Briefly, compare the list of species that were caught to the species identified in the longterm TWPD gill net monitoring program.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nYour table should look something like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\nTotal\n\n\n\n\nCarcharhinus_brevipinna\n12\n46\n12\n70\n\n\nCarcharhinus_leucas\n3\n4\n1\n8\n\n\nCarcharhinus_limbatus\n1\n1\n0\n2\n\n\nCarcharhinus_porosus\n0\n1\n0\n1\n\n\nHypanus_americanus\n3\n1\n7\n11\n\n\nHypanus_sabina\n9\n2\n0\n11\n\n\nRhinoptera_bonasus\n0\n0\n1\n1\n\n\nRhizoprionodon_terraenovae\n1\n5\n8\n14\n\n\nSphyrna_lewini\n0\n4\n0\n4\n\n\nSphyrna_tiburo\n1\n18\n16\n35\n\n\n\nTable 10.1: Number of individuals per caught per site and overall across all sites and years.\n\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can use replace(is.na(.), 0) to replace NA values in all columns with a 0.\n\n\nWe are not only interested in which species are observed at each site, we also want to know what at what life stages different species are using the estuaries. Typically, we can classify sharks as young-of-the-year (YOY), juveniles (JUV), or mature (MAT). There are ways to observe this in the field, for example YOY can be identified using their umbilical scar and in male sharks whether or not the claspers are calcified is an indication of maturity.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nDetermine how many individuals have information on their life history stage.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nAnother way to determine the life history stage is to used previously information on length-at-maturity and how quickly YOY grow during their first year of life. This information is species-specific and has been determined for various species using life-history studies that rely on data sets that contain information on size, level of maturity and age4.4 Sharks can be aged using their vertebrae similar to how we can use growth rings on trees to age them.\nFor example, (Carlson and Baremore 2005) determined the following length/history stage relationships for spinner sharks (C. brevipinna)\n\nCarlson, John K., and Ivy Baremore. 2005. “Growth Dynamics of the Spinner Shark (Carcharhinus Brevipinna) Off the United States Southeast and Gulf of Mexico Coasts: A Comparison of Methods.” Fishery Bulletin 103 (2). https://aquadocs.org/handle/1834/26223.\n\nYOY\n\nfemales &lt; 844mm\nmales &lt; 812mm\n\nJuveniles\n\nfemales 844 - 1360mm\nmales 812 - 1380mm\n\nmature (adults)\n\nfemales &gt; 1360 mm\nmales &gt; 1380 mm\n\n\nWhile (Neer, Thompson, and Carlson 2005) published these details for bull sharks (C. leucas)\n\nNeer, J. A., B. A. Thompson, and John K. Carlson. 2005. “Age and Growth of Carcharhinus Leucas in the Northern Gulf of Mexico: Incorporating Variability in Size at Birth - Neer - 2005 - Journal of Fish Biology - Wiley Online Library.” Journal of Fish Biology 67 (2): 370–83. https://onlinelibrary.wiley.com/doi/full/10.1111/j.0022-1112.2005.00743.x.\n\nYOY\n\nfemales &lt; 700mm\nmales &lt; 700mm\n\nJuveniles\n\nfemales 700 - 2250mm\nmales 700 - 2100mm\n\nmature (adults)\n\nfemales &gt; 2250mm\nmales &gt; 2100mm\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nFirst, conceptually describe how you could add this information to your data sheet in excel as a new column called Estimated_Stage.\n\n\nNow, let’s consider how we could use our data wrangling skills to add a new column Estimated_Stage that contains life history stage based on length estimates. Let’s first work this out for the two species above to keep it simple.\nWhen confronted with a more complex problems like this it can be helpful to first walk through the individual steps necessary5.5 Many people find it helpful to write things out in ‘pseudo-code’ first and then work out what the code needs to look like for the specific language they are working in\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly outline what you think our approach should look like - even if you don’t know the functions you need to achieve this.\n\n\nThere are two approaches we can take.\n\n\n\n\n\n\nSolution 1\n\n\n\n\n\nThe first solution involves sub-setting your data.frame using filter() to contain only individuals that fulfill the conditions of specific length ranges that fit the ranges above for each life history stage and the add a new column with the correctly assigned life history stage6.\n\n# C. brevipinna, Carlson & Baremore 2005\n\nC.brevipinna_YOY &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&lt;=812 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&lt;=844 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&lt;=844) %&gt;%\n  mutate(Estimated_Stage=\"YOY\")\n\nC.brevipinna_JUV &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;812 & FL&lt;=1380 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;844 & FL&lt;=1360 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;844 & FL&lt;=1360) %&gt;%\n  mutate(Estimated_Stage=\"JUV\")\n\nC.brevipinna_MAT &lt;- filter(elasmos, Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;1380 | Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;1360 | Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;1360) %&gt;%\n  mutate(Estimated_Stage=\"MAT\")\n\n\n# C. leucas, Neer et al. 2005\n\nC.leucas_YOY &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & FL&lt;=700) %&gt;%\n  mutate(Estimated_Stage=\"YOY\")\n\nC.leucas_JUV &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;700 & FL&lt;=2100 | Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;700 & FL&lt;=2250 | Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;700 & FL&lt;=2250) %&gt;%\n  mutate(Estimated_Stage=\"JUV\")\n\nC.leucas_MAT &lt;- filter(elasmos, Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;2100 | Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;2250 | Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;2250) %&gt;%\n  mutate(Estimated_Stage=\"MAT\")\n\nNow you have a bunch of individual data.frames that we need to put back together into a single data.frame. We can do this using bind_rows() which will combine data.frames that have the same set of columns.\n\nelasmos_stage &lt;- bind_rows(C.brevipinna_YOY, C.brevipinna_JUV, C.brevipinna_MAT,\n                           C.leucas_YOY, C.leucas_JUV, C.leucas_MAT)\n\n\n\n\n6 Remember, you can use & and | to combine two conditionsThis solution fits into our general scheme of “split-apply-combine” - except that we actually created multiple objects during our “split” stage. Is there a way to do this without creating individual objects?\n\n\n\n\n\n\nSolution 2\n\n\n\n\n\nIndeed, our second option circumvents having to first create subsets of the initial data.frame using something called a “conditional mutate”.\n\nelasmos_stage &lt;- elasmos %&gt;%\n  filter(Species %in% c(\"Carcharhinus_leucas\", \"Carcharhinus_brevipinna\")) %&gt;%\n  mutate(Estimated_Stage = case_when(Species == \"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&lt;=812 |\n                                       Species == \"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&lt;=844 |\n                                       Species == \"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&lt;=844 ~ \"YOY\",\n         Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;812 & FL&lt;=1380 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;844 & FL&lt;=1360 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;844 & FL&lt;=1360 ~ \"JUV\",\n         Species==\"Carcharhinus_brevipinna\" & Sex==\"M\" & FL&gt;1380 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"F\" & FL&gt;1360 |\n           Species==\"Carcharhinus_brevipinna\" & Sex==\"U\" & FL&gt;1360 ~ \"MAT\",\n         Species==\"Carcharhinus_leucas\" & FL&lt;=700 ~ \"YOY\",\n         Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;700 & FL&lt;=2100 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;700 & FL&lt;=2250 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;700 & FL&lt;=2250 ~ \"JUV\",\n         Species==\"Carcharhinus_leucas\" & Sex==\"M\" & FL&gt;2100 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"F\" & FL&gt;2250 | \n           Species==\"Carcharhinus_leucas\" & Sex==\"U\" & FL&gt;2250 ~ \"MAT\"))\n\nThis is of course a fairly complicated conditional mutate as we are generally combining multiple conditions per category. In this case we could also leave out the | and instead add a ~ STAGE to each line depending on our coding preferences.\n\n\n\nNormally, we would have to extend our code to estimate life history stage for all of our sampled individuals but I have done this for you and you can load that file from your data folder.\n\nelasmos &lt;- read_delim(\"data/elasmos_complete.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse this data set to create a table with the number of individuals per life history stage caught at each site.\n\n\nThis is what you table should look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nSite\nTotal\nYOY\nJUV\nMAT\nUND\n\n\n\n\nCarcharhinus_brevipinna\nAransas_Bay\n12\n11\n0\n0\n1\n\n\nCarcharhinus_brevipinna\nCorpus_Christi_Bay\n46\n45\n0\n0\n1\n\n\nCarcharhinus_brevipinna\nRedfish_Bay\n12\n8\n3\n0\n1\n\n\nCarcharhinus_leucas\nAransas_Bay\n3\n0\n3\n0\n0\n\n\nCarcharhinus_leucas\nCorpus_Christi_Bay\n4\n4\n0\n0\n0\n\n\nCarcharhinus_leucas\nRedfish_Bay\n1\n0\n1\n0\n0\n\n\nCarcharhinus_limbatus\nAransas_Bay\n1\n0\n1\n0\n0\n\n\nCarcharhinus_limbatus\nCorpus_Christi_Bay\n1\n1\n0\n0\n0\n\n\nCarcharhinus_porosus\nCorpus_Christi_Bay\n1\n0\n1\n0\n0\n\n\nHypanus_americanus\nAransas_Bay\n3\n0\n0\n3\n0\n\n\nHypanus_americanus\nCorpus_Christi_Bay\n1\n0\n0\n1\n0\n\n\nHypanus_americanus\nRedfish_Bay\n7\n0\n3\n4\n0\n\n\nHypanus_sabina\nAransas_Bay\n9\n0\n0\n9\n0\n\n\nHypanus_sabina\nCorpus_Christi_Bay\n2\n0\n0\n2\n0\n\n\nRhinoptera_bonasus\nRedfish_Bay\n1\n0\n0\n1\n0\n\n\nRhizoprionodon_terraenovae\nAransas_Bay\n1\n1\n0\n0\n0\n\n\nRhizoprionodon_terraenovae\nCorpus_Christi_Bay\n5\n5\n0\n0\n0\n\n\nRhizoprionodon_terraenovae\nRedfish_Bay\n8\n8\n0\n0\n0\n\n\nSphyrna_lewini\nCorpus_Christi_Bay\n4\n1\n3\n0\n0\n\n\nSphyrna_tiburo\nAransas_Bay\n1\n0\n1\n0\n0\n\n\nSphyrna_tiburo\nCorpus_Christi_Bay\n18\n0\n14\n3\n1\n\n\nSphyrna_tiburo\nRedfish_Bay\n16\n1\n9\n6\n0\n\n\n\nTable 10.2: Number of individuals per species caught at each site by life history stage.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe your results to compare total catch across sites accounting for differences in life history stage.\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nSubset your data to contain only YOY and generate a table to investigate whether they were caught across all years sampling occured. Summarize your results in 2-3 sentences.\n\n\nThis is what your table should look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSite\nSpecies\n2015\n2016\n2017\n2018\n\n\n\n\nAransas_Bay\nCarcharhinus_brevipinna\n7\n0\n3\n1\n\n\nAransas_Bay\nRhizoprionodon_terraenovae\n0\n0\n1\n0\n\n\nCorpus_Christi_Bay\nCarcharhinus_brevipinna\n0\n6\n16\n23\n\n\nCorpus_Christi_Bay\nCarcharhinus_leucas\n1\n3\n0\n0\n\n\nCorpus_Christi_Bay\nCarcharhinus_limbatus\n0\n1\n0\n0\n\n\nCorpus_Christi_Bay\nRhizoprionodon_terraenovae\n0\n3\n1\n1\n\n\nCorpus_Christi_Bay\nSphyrna_lewini\n0\n0\n1\n0\n\n\nRedfish_Bay\nCarcharhinus_brevipinna\n1\n0\n3\n4\n\n\nRedfish_Bay\nRhizoprionodon_terraenovae\n4\n4\n0\n0\n\n\nRedfish_Bay\nSphyrna_tiburo\n1\n0\n0\n0\n\n\n\nTable 10.3: Number of YOY caught at each site in each sampling year."
  },
  {
    "objectID": "10_relational-data.html#comparison-of-cpue-per-species-across-sites",
    "href": "10_relational-data.html#comparison-of-cpue-per-species-across-sites",
    "title": "10  Relational data",
    "section": "10.2 Comparison of CPUE per species across sites",
    "text": "10.2 Comparison of CPUE per species across sites\n\n\n\n\n\n\n Consider this\n\n\n\nConsider disadvantages of using absolute counts of occurrence to compare composition across sites. What measure could you use instead of total catch to fix this issue?\n\n\nCatch-per-unit-effort (CPUE) is an indirect measure of abundance. Essentially, it is a way to measure relative abundance and be able to account for differences in sampling effort - the key is defining how you will measure “effort”.\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly discuss what measures we could use to determine effort.\n\n\nWe are going to calculate effort as “hook hours”.\nTo do this we need to know how many hooks were on the line per set7 and how long the entire line was in the water per set (this is called soak time), then we can easily calculate hook hours of each set as the number of hooks multiplied by the soak time. And then we can divide the number of e.g. sharks caught on a set (“catch) by hook hours (”effort”) to calculate CPUE.7 A set means that baited hooks on leaders (individual lines) where attached to the main line and that main line was then “set” in the water for a given period of time before pulling it back in and determining which fish were caught on hooks.\nYour data folder contains as tab-delimited file with set meta-data. This includes information that describes the set itself including date of the set, site, soak time, and location and also parameters describing the conditions of the set such as temperature, salinity, depth, and dissolved oxygen.\nLet’s read in the data set.\n\nset_meta &lt;- read_delim(\"data/set_data.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a quick look at the data set to determine what columns are included and what information we can learn about each individual set. How can you amend the data set to include hook hours?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCorrect, a simple mutate() will do the trick.\n\nset_meta &lt;- set_meta %&gt;%\n  mutate(Hook_Hours = Hooks * Soak_Time)\n\n\n\n\nNext we need to count the number of sharks caught per set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIf we look at our elasmo data.frame you will notice that we have a column called Set but that number indicates the nth set of a give sample day. How can you add a column called Set_ID that consists of the date and the set number?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nelasmos &lt;- elasmos %&gt;%\n  unite(Set_ID, Year, Month, Day, Set, sep = \"_\", remove = FALSE) %&gt;%\n  arrange(Set_ID)\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow create a new object called elasmos_set that contains the number of sharks caught per set.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nelasmos_set &lt;- elasmos %&gt;%\n  count(Set_ID)\n\n\n\n\nNow we have two data.frames one contains the information on how many sharks were caught per set and a second one that contains information about the set, including hook hours. This means that our next step will need to be to combine these two data sets.\n\n\n\n\n\n\n Consider this\n\n\n\nEarlier we learned about bind_rows() which allows us to combine two data.frames that contain identical columns, i.e. row-wise. There is an equivalent function called bind_columns() which allows us to combine data.frames column-wise.\nConsider what the problem would be in using bind_columns() to combine these two data sets.\n\n\nHaving multiple tables containing data pertaining to the same question is referred to as relational data - we are interested in how the contents of a pair of table related to each other, not just the individual data sets. Combining two tables is called a join. In this case the type of join we want to execute is called a mutating join which means we can add new variables from one data.frame (set_meta) to matching observations in another (elasmos_set).\nIn order to do that we need to have one column (the key) that way the function can match observations in one data.frame by that key and then copy the matching observations in the columns from the second data.frame across.\nWhen performing a join, new columns are added to the right. We will use the function full_join() which means that all the rows from the left and right data.frame will be retained - when we used count() that excluded sets where no sharks were caught, by using a full_join() we can add those back in.\nWe currently do not have a matching column between the two data sets, so our first step will be to add a new column called Set_ID to our set_meta data.frame, then we can use full_join() to join the two tables. The argument by can be used to specify the column to use as the key. For our example here we have a column with the same name - in general, the function is “smart” enough to identify shared columns and so you do not necessarily have to specify it.\n\n\nYou can pull up the help page using ?full_join to learn how to join tables that have multiple columns in common or that might have a column in common though it is named differently between the two tables.\nNote the notation elasmo_set &lt;- full_join(elasmos_set, set_meta, by = \"Set_ID\") will produce the same result as the syntax we are using here.\n\n# add set id column\nset_meta &lt;- set_meta %&gt;%\n  unite(Set_ID, Year, Month, Day, Set, sep = \"_\", remove = FALSE)\n\n# join data sets\nelasmos_set &lt;- elasmos_set %&gt;%\n  full_join(set_meta) %&gt;%\n  replace_na(list(n = 0))\n\nNow we can calculate CPUE for sharks per site.\n\nelasmos_set &lt;- elasmos_set %&gt;%\n  mutate(CPUE = n/Hook_Hours)\n\nAnd from that we can easily calculate mean and standard deviation CPUE of catching sharks by site.\n\n\n\n\n\n\n\nSite\nmean_CPUE\nstd_CPUE\n\n\n\n\nAransas_Bay\n0.0048954\n0.0089735\n\n\nCorpus_Christi_Bay\n0.0135243\n0.0185803\n\n\nRedfish_Bay\n0.0069282\n0.0114742\n\n\n\nTable 10.4: mean +/- sd CPUE\n\n\nWe are going to perform a Kruskal-Wallis rank sum test to determine if there is significant heterogeneity among sites8.8 You are probably more familiar with the framework of using an ANOVA to test for significant heterogeneity and pairwise t-tests to test for equality of means of a set of values. KW is similar but is a non-parametric approach and does not make assumptions about the distribution of values.\n\n# KW to test for significant heterogeneity\nkruskal.test(CPUE ~ Site, data = elasmos_set)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  CPUE by Site\nKruskal-Wallis chi-squared = 12.325, df = 2, p-value = 0.002106\n\n\nAnd we will follow that using a Dunn’s test for pairwise comparisons.\n\n# post-hoc Dunn test\ndunnTest(CPUE ~ Site, data = elasmos_set, method = \"bh\")\n\n                        Comparison          Z      P.unadj       P.adj\n1 Aransas_Bay - Corpus_Christi_Bay -3.3553910 0.0007925288 0.002377586\n2        Aransas_Bay - Redfish_Bay -0.7980727 0.4248282807 0.424828281\n3 Corpus_Christi_Bay - Redfish_Bay  2.5669868 0.0102586520 0.015387978\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe your results and discuss what this result could mean for our overarching question of identifying shark nurseries.\n\n\nOf course, we are interested how CPUE compares across species and sites.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChoose one species and calculate the CPUE per set. For convenience convert CPUE to effort per 1000 hook hours and then calculate the mean CPUE per 1000 hooks per site for that species.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what that could look like for a single species.\n\nspecies &lt;- \"Carcharhinus_brevipinna\"\n\nspecies_CPUE &lt;- elasmos %&gt;%\n  filter(Species == species) %&gt;%\n  count(Set_ID) %&gt;%\n  full_join(set_meta) %&gt;%\n  replace_na(list(n = 0)) %&gt;%\n  mutate(CPUE = n/Hook_Hours,\n         CPUE_1000 = CPUE * 1000) %&gt;%\n  group_by(Site) %&gt;%\n  summarize(mean_CPUE = mean(CPUE_1000))\n\n\n\n\nFor better presentation, we probably want to convert that do a wider data set; your results should look like this.\n\n\n\n\n\n\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\n\n\n\n\n1.91\n7.88\n1.76\n\n\n\nCatch per unit effort (1000 hook hours) for each site.\n\nWe actually want to have this information for all species, rather than create individual data.frames for each species and then combine those using bind_rows(), I will show you a more efficient way of coding this using a for loop.\n\n# create empty list\nspecies_CPUE &lt;- list()\n\n# calculate hook hours for each species per site\nfor(species in unique(elasmos$Species)){\n  \n  species_CPUE[[species]] &lt;- elasmos %&gt;%\n    filter(Species == species) %&gt;%\n    count(Set_ID) %&gt;%\n    full_join(set_meta) %&gt;%\n    replace_na(list(n = 0)) %&gt;%\n    mutate(CPUE = n/Hook_Hours,\n           CPUE_1000 = CPUE * 1000)\n\n}\n\n# combine data frames in list into single \nCPUE &lt;- bind_rows(species_CPUE, .id = \"Species\")\n\nNext, we would want to run KW tests to determine if there are significant differences among sites for each species.\n\n# create empty dataframe for results\nresults &lt;- setNames(data.frame(matrix(ncol = 2, nrow = 0)), \n                    c(\"Species\", \"pvalue\")) %&gt;%\n  mutate(Species = as.character(Species),\n         pvalue = as.numeric(pvalue))\n\nfor(species in unique(CPUE$Species)){\n  \n  # filter CPUE per species\n  tmp &lt;- CPUE %&gt;%\n    filter(Species == species)\n  \n  # KW to test for significant heterogeneity\n  KW &lt;- kruskal.test(CPUE ~ Site, data = tmp)\n  \n  # extract p-value\n  df &lt;- data.frame(\"Species\" = species,\n                   \"pvalue\" = as.numeric(KW$p.value))\n  \n  results &lt;- bind_rows(results, df)\n\n}\n\nLet’s calculate mean CPUE per species and site, turn that into a wide table for easier comparison and add the p-values.\n\nCPUE_sign &lt;- CPUE %&gt;%\n    group_by(Species, Site) %&gt;%\n    summarize(mean_CPUE = mean(CPUE_1000)) %&gt;%\n    pivot_wider(names_from = Site, values_from = mean_CPUE) %&gt;%\n    left_join(results) %&gt;%\n    arrange(Species)\n\nOnce we’ve run that code to wrangle and transform our data we can compare CPUE for each species and site.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\npvalue\n\n\n\n\nCarcharhinus_brevipinna\n1.91\n7.88\n1.76\n0.00\n\n\nCarcharhinus_leucas\n0.49\n0.52\n0.14\n0.54\n\n\nCarcharhinus_limbatus\n0.14\n0.18\n0.00\n0.60\n\n\nCarcharhinus_porosus\n0.00\n0.15\n0.00\n0.37\n\n\nHypanus_americanus\n0.49\n0.14\n1.08\n0.25\n\n\nHypanus_sabina\n1.50\n0.29\n0.00\n0.00\n\n\nRhinoptera_bonasus\n0.00\n0.00\n0.16\n0.37\n\n\nRhizoprionodon_terraenovae\n0.14\n0.90\n1.29\n0.22\n\n\nSphyrna_lewini\n0.00\n0.50\n0.00\n0.02\n\n\nSphyrna_tiburo\n0.22\n2.96\n2.49\n0.00\n\n\n\nTable 10.5: Catch per unit effort (per 1000 hook hours) for each species by site, p-value indicates whether there are significant differences among sites for a given species.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nUse the table with CPUE per species in your lab manual to briefly describe the results comparing CPUE per species and site and relate that to our overarching question of identifying shark nurseries9.\n\n\n9 Normally, we would want to run additional pairwise tests for each species where there are significant differences among sites, but we’ll skip that step for now and stick to the big picture."
  },
  {
    "objectID": "10_relational-data.html#comparison-of-cpue-for-different-life-history-stages",
    "href": "10_relational-data.html#comparison-of-cpue-for-different-life-history-stages",
    "title": "10  Relational data",
    "section": "10.3 Comparison of CPUE for different life history stages",
    "text": "10.3 Comparison of CPUE for different life history stages\nOf course, we are not only interested in which species were caught at each site, we also want to know what life history stages those individuals were at when they were caught.\nWe will use a similar strategy as above to create a data frame with CPUE per site, species, and life history stage and produce a table comparing the means.\n\n# create empty list\nspecies_CPUE &lt;- list()\n\n# calculate hook hours for each species per site\nfor(species in unique(elasmos$Species)){\n  \n  for(stage in unique(elasmos$Estimated_Stage)){\n    \n      species_CPUE[[paste(species, stage, sep = \":\")]] &lt;- elasmos %&gt;%\n        filter(Species == species & Estimated_Stage == stage) %&gt;%\n        count(Set_ID) %&gt;%\n        full_join(set_meta) %&gt;%\n        replace_na(list(n = 0)) %&gt;%\n        mutate(Estimate_Stage = stage, \n               CPUE = n/Hook_Hours,\n               CPUE_1000 = CPUE * 1000)\n\n  }\n  \n}\n\n# combine data frames in list into single \nCPUE &lt;- bind_rows(species_CPUE, .id = \"Species_Stage\") %&gt;%\n  select(Species_Stage, Set_ID, Site, Hooks, Soak_Time, Hook_Hours, CPUE, CPUE_1000) %&gt;%\n  separate(Species_Stage, into = c(\"Species\", \"Stage\"), sep = \":\", remove = FALSE) %&gt;%\n    group_by(Species_Stage, Site) %&gt;%\n    summarize(mean_CPUE = mean(CPUE_1000)) %&gt;%\n    pivot_wider(names_from = Site, values_from = mean_CPUE) %&gt;%\n    filter(if_any(c(Aransas_Bay, Corpus_Christi_Bay, Redfish_Bay), ~ . &gt; 0)) %&gt;%\n    separate(Species_Stage, into = c(\"Species\", \"Stage\"), sep = \":\") %&gt;%\n    filter(!Stage == \"UND\")\n\nThis will produce the following table summarizing the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nStage\nAransas_Bay\nCorpus_Christi_Bay\nRedfish_Bay\n\n\n\n\nCarcharhinus_brevipinna\nJUV\n0.0000\n0.0000\n0.4586\n\n\nCarcharhinus_brevipinna\nYOY\n1.7477\n7.7283\n1.1510\n\n\nCarcharhinus_leucas\nJUV\n0.4940\n0.0000\n0.1357\n\n\nCarcharhinus_leucas\nYOY\n0.0000\n0.5209\n0.0000\n\n\nCarcharhinus_limbatus\nJUV\n0.1443\n0.0000\n0.0000\n\n\nCarcharhinus_limbatus\nYOY\n0.0000\n0.1794\n0.0000\n\n\nCarcharhinus_porosus\nJUV\n0.0000\n0.1480\n0.0000\n\n\nHypanus_americanus\nJUV\n0.0000\n0.0000\n0.4745\n\n\nHypanus_americanus\nMAT\n0.4872\n0.1444\n0.6095\n\n\nHypanus_sabina\nMAT\n1.4967\n0.2940\n0.0000\n\n\nRhinoptera_bonasus\nMAT\n0.0000\n0.0000\n0.1614\n\n\nRhizoprionodon_terraenovae\nYOY\n0.1443\n0.8970\n1.2884\n\n\nSphyrna_lewini\nJUV\n0.0000\n0.3768\n0.0000\n\n\nSphyrna_lewini\nYOY\n0.0000\n0.1210\n0.0000\n\n\nSphyrna_tiburo\nJUV\n0.2217\n2.3098\n1.4763\n\n\nSphyrna_tiburo\nMAT\n0.0000\n0.4455\n0.8716\n\n\nSphyrna_tiburo\nYOY\n0.0000\n0.0000\n0.1468\n\n\n\nTable 10.6: Comparison of CPUE by life history stage for all observed life history stages in Aransas, Corpus Christi, and Redfish Bay.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nBriefly describe the results comparing CPUE per life history stage and site; these results are all statistically significant.\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nRelate your all of our results back to the overarching question of identifying shark nurseries in Texas Bays and Estuaries to write a short conclusion in terms of what this study has (or has not demonstrated)."
  },
  {
    "objectID": "12_community-diversity.html#compile-data-sets",
    "href": "12_community-diversity.html#compile-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.1 Compile data sets",
    "text": "12.1 Compile data sets\nWe are going to import the data you will need for this chapter exploring how to characterize biological communities. We’ll start by loading to objects into your environment. One is the ASV table and the second is the corresponding taxonomy table from the fungi data set we were working on in the previous chapter.\nWe are going to read in a data set that contains information about the soil plots from which the fungi eDNA was isolated.\n\n# load sample data\nsoil &lt;- read_delim(\"data/soil.csv\", delim = \";\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to take a look at the soil data set and the describe what information it contains (typical things you want to check are row, column numbers, column names, determining if it is a tidy data set, figuring out what information/variables are in the data set).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nThe package phyloseq that we installed has various utility functions to deal with metabarcoding data set. It has a specific object class that allows us to store the asv table, taxonomy table, and sample meta data in different slots. Various functions can then be used to pull information from those slots. We can load the object ps directly into your environment using the following code.\n\n# create phyloseq object\nload(file = \"data/ps.rdata\")"
  },
  {
    "objectID": "12_community-diversity.html#data-filtering-and-transformation",
    "href": "12_community-diversity.html#data-filtering-and-transformation",
    "title": "12  Characterizing community diversity",
    "section": "12.2 Data filtering and transformation",
    "text": "12.2 Data filtering and transformation\nBefore we can start exploring our data set we have a few steps to complete to transform it into containing the data we want in the format we want tit.\nLet’s start by taking a looking at how many taxa are currently present in the data set using phyloseq::ntaxa()\n\nntaxa(ps)\n\n[1] 546\n\n\nWe only want ASVs that were assigned as fungi in our reference database. We can use phyloseq::subset_taxa to filter by kingdom.\n\nps_fungi &lt;- subset_taxa(ps, Kingdom == \"k__Fungi\")\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many non-fungi groups were filtered.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nNext, we want to remove any singletons or doubletons. These are ASVs that are only represented by one or two reads - we can assume these are artifacts.\n\nps_fungi_nosd &lt;- filter_taxa(ps_fungi, function(x) sum(x) &gt; 2, TRUE)\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nFigure out how many taxonomic groups are still in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nIn many situations, metabarcoding eDNA samples is semi-quantitative2. The number of DNA templates in the environment are correlated to the number of individuals/biomass of a given taxonomic group. While you cannot necessarily back calculate the absolute abundance or number of specimen present in an environment you can use the proportion of reads assigned to a taxonomic group as a metric of relative abundance.2 We can always confidentally use metbarcoding data as qualitative data, i.e. as presence/absence data. Though even here we should be careful about whether “not detected” should be interpreted as absent.\nOne way to convert species abundance from absolute to relative is using a Hellinger transformation which standardizes the abundances to the sample totals and then square roots them. The function phyloseq::transform_sample_counts() allows us to apply a function to transform sample counts.\n\nps_fungi_nosd_hel &lt;- transform_sample_counts(ps_fungi_nosd, function(x) sqrt(x/sum(x)))\n\n\n\n\n\n\n\n Consider this\n\n\n\nOur data set could still contain multiple ASVs that have been assigned to the same species. Explain why this is an expected out come when using ASVs but would be rare/non-existant if you are using OTUs as the output of your bioinformatics pipeline.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nTo complete our data filtering and transformation we will use phyloseq::tax_glom() to collapse ASVs asigned to the same taxonomic group at the species level.\n\nps_transf = tax_glom(ps_fungi_nosd_hel, \"Species\", NArm = FALSE)\n\nLet’s explore our final transformed data set\n\n\n\n\n\n\n Give it a whirl\n\n\n\nApply the functions ntaxa(), nsamples(), rank_names(), and sample_variable() to our final transformed phyloseq object. Look up what each function does, make sure to comment/annotate your code and then briefly describe what you’ve learned about our data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nRecall that our phyloseq object contains a slot that holds our ASVs table and our taxonomic table that we can access at as such.\n\notu_table(ps_transf)[1:2, 1:2]\n\nOTU Table:          [2 taxa and 2 samples]\n                     taxa are columns\n    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                                            0.00000000\nS11                                                                                                                                                                                                                                                                                                                                                            0.08178608\n    ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA\nS10                                                                                                                                                                                                                                                                                                                                               0.0000000\nS11                                                                                                                                                                                                                                                                                                                                               0.2085144\n\ntax_table(ps_transf)[1:2, 1:2]\n\nTaxonomy Table:     [2 taxa by 2 taxonomic ranks]:\n                                                                                                                                                                                                                                                                                                                                                                      Kingdom   \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"k__Fungi\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"k__Fungi\"\n                                                                                                                                                                                                                                                                                                                                                                      Phylum            \nATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTTGAGTGTCATTAAATTATCAACCTTGCTCGCTTTTATTAGCTTGAGTTAGGCTTGGATGTGAGGGTTTTGCTGGCTTCCTTCAGTGGATGGTCTGCTCCCTTTGAATGCATTAGCGGGATCTCTTGTGGACCGTCACTTGGTGTGATAATTATCTATGCCTTGAGACTTTGAAACAAACTTATGAGAATCTGCTTATAACCGTCCTCACGGACAACTTTTGACAATTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAA \"p__Basidiomycota\"\nATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAA               \"p__Mucoromycota\" \n\n\n\n\n\n\n\n\n Consider this\n\n\n\nExplain how the notation [1:2, 1:2] modifies the output\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "12_community-diversity.html#combine-data-sets",
    "href": "12_community-diversity.html#combine-data-sets",
    "title": "12  Characterizing community diversity",
    "section": "12.3 Combine data sets",
    "text": "12.3 Combine data sets\nWe have three explanatory variables that could be driving differences in fungal communities among samples.\n\nSoil samples were taken in differnt forest plots that were classified as dominated by Acer saccharum (AS) or Fagus grandifolia (FG) or mixed with other small trees and shrubs present (mixed).\nSoil samples where taken from different soil horizons (depths): L, F, H, Ae, or B\nSoil chemistry (carbon, nitrogen, pH)\n\nThis information is stored in our soil dataframe.\nNow that we’ve filtered and transformed our data set, let’s pull it back out to create a dataframe as the object you are more familiar with in terms of being able to manipulate it.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s pull out our taxonomic table and transform it into a dataframe. Comment the following code line by line to describe what each function/arguments is doing.\n\nasv_tax &lt;- tax_table(ps_transf) %&gt;%  #\n  as.data.frame() %&gt;%            #\n  rownames_to_column(\"asv\")      #\n\n# write out interim file\nwrite_delim(asv_tax, \"results/asv_tax.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s pull out our information on how many times each ASV is observed in a each sample. Comment the following code line by line to describe what each function/arguments is doing. You may need to look up some of the functions.\n\nasv_counts &lt;- otu_table(ps_transf) %&gt;%  #\n  t() %&gt;%                               #\n  as.data.frame() %&gt;%                   #\n  rownames_to_column(\"asv\")             #\n\n# write out interim file\nwrite_delim(asv_counts, \"results/asv_counts.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nTake a look at how your asv_tax and asv_count objects are now formatted and briefly describe it (remember, key things are number or rows, columns, what those columns are).\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nReally, what we want is for our asv_counts table to contain the taxonomic information contained in the asv_tax table. Combine those two data sets into an object called tax_count. Remove the ASV sequence from the dataframe and arrange the remaining columns to first have all the taxonomic information, then the number of occurrences in each sample. Print the first few lines to the console when you are done.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like\n\n\n   Kingdom           Phylum                 Class             Order\n1 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n2 k__Fungi  p__Mucoromycota c__Umbelopsidomycetes o__Umbelopsidales\n3 k__Fungi    p__Ascomycota    c__Dothideomycetes  o__Mytilinidales\n4 k__Fungi p__Basidiomycota     c__Agaricomycetes     o__Agaricales\n5 k__Fungi    p__Ascomycota    c__Sordariomycetes     o__Xylariales\n6 k__Fungi    p__Ascomycota                  &lt;NA&gt;              &lt;NA&gt;\n                Family           Genus       Species       S10        S11\n1  f__Tricholomataceae       g__Mycena          &lt;NA&gt; 0.0000000 0.08178608\n2   f__Umbelopsidaceae   g__Umbelopsis   s__dimorpha 0.0000000 0.20851441\n3        f__Gloniaceae   g__Cenococcum  s__geophilum 0.0000000 0.32414749\n4    f__Hygrophoraceae    g__Hygrocybe s__flavescens 0.2929066 0.40028795\n5 f__Amphisphaeriaceae g__Polyscytalum s__algarvense 0.0000000 0.00000000\n6                 &lt;NA&gt;            &lt;NA&gt;          &lt;NA&gt; 0.5073291 0.22398041\n        S12        S13       S14        S15       S16        S17       S18\n1 0.0000000 0.09578263 0.2379155 0.04867924 0.7712319 1.25129257 0.4761444\n2 0.2721655 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n3 0.5233063 0.13545709 0.0000000 0.23676137 0.2667853 0.00000000 0.0000000\n4 0.0000000 0.00000000 0.0000000 0.06884284 0.0000000 0.00000000 0.0000000\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.3165055\n6 0.1200137 0.19156526 0.0000000 0.13299415 0.2226355 0.07392213 0.5152174\n          S1       S20        S21        S22        S23       S24       S25 S26\n1 0.07372098 0.1543370 0.15971703 0.04598005 0.17325923 0.1726902 0.2505837   0\n2 0.24818179 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n3 0.36422877 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n4 0.24077171 0.0000000 0.00000000 0.00000000 0.00000000 0.0000000 0.0000000   0\n5 0.00000000 0.6046415 0.06052275 0.34039602 0.00000000 0.4626177 0.3245927   0\n6 0.41216069 0.5922461 0.25094334 0.47371431 0.04331481 0.3527087 0.5436833   0\n        S27        S28        S29        S2       S30       S31        S33\n1 0.9263177 0.04145133 0.27471034 0.1142577 0.4917893 0.4379003 0.07124705\n2 0.0000000 0.00000000 0.00000000 0.1842351 0.0000000 0.0000000 0.00000000\n3 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.19991094\n4 0.0000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000 0.00000000\n5 0.0000000 0.13747852 0.04729838 0.0000000 0.3446360 0.3636152 0.00000000\n6 0.0000000 0.33819215 0.17651995 0.7626946 0.1953884 0.8801878 0.12162632\n        S34        S35       S36        S37       S39         S3       S41\n1 0.0000000 0.04393748 0.0559017 0.09449112 0.1474420 0.06516352 0.1036952\n2 0.3815359 0.00000000 0.2091650 0.18898224 0.6158141 0.36572936 0.0000000\n3 0.0000000 0.77616588 0.1118034 0.33838581 0.2197935 0.00000000 0.0000000\n4 0.5209007 0.00000000 0.0000000 0.00000000 0.0695048 0.14497221 0.2375955\n5 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000 0.0000000\n6 0.0000000 0.00000000 0.0000000 0.29827034 0.4928534 0.00000000 1.1342749\n         S42        S43       S44       S45        S46       S47       S48\n1 0.07808688 0.05123155 0.1952834 0.1954906 0.08441499 0.0000000 0.1773317\n2 0.19908326 0.00000000 0.0000000 0.0000000 0.09747404 0.0000000 0.0000000\n3 0.17460757 0.61471931 0.1301889 0.0000000 0.21037942 0.5733137 0.6634328\n4 0.68553927 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n5 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000\n6 0.33809195 0.33116596 0.0000000 0.1382327 0.10897929 0.3473466 0.0000000\n         S49         S4       S50       S51        S52       S53        S54\n1 0.00000000 0.08962214 0.1162476 0.1076244 0.09853293 0.1624354 0.08980265\n2 0.00000000 0.00000000 0.0000000 0.0000000 0.00000000 0.0000000 0.15554275\n3 0.78598070 0.50232050 0.4739717 0.0000000 0.30722902 0.0000000 0.77815937\n4 0.00000000 0.00000000 0.0000000 0.6499631 1.03997108 0.3456639 0.00000000\n5 0.00000000 0.00000000 0.0000000 0.2107167 0.00000000 0.0000000 0.00000000\n6 0.09724333 0.00000000 0.0000000 0.2818244 0.19706586 0.7353688 0.27679036\n         S55        S56       S58        S59         S5        S60        S61\n1 0.07722833 0.00000000 0.0000000 0.00000000 0.08436491 0.05852057 0.09072184\n2 0.00000000 0.35948681 0.2656845 0.00000000 0.00000000 0.00000000 0.06415003\n3 0.14788099 0.09607689 0.3510009 0.07800765 0.00000000 0.00000000 0.00000000\n4 0.16683226 0.13587324 0.0000000 0.00000000 0.00000000 1.19488153 0.57239218\n5 0.00000000 0.00000000 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.29652140 0.24485651 0.2425356 0.00000000 0.17896500 0.34818263 0.46770187\n        S62        S63        S64        S65        S66        S67        S68\n1 0.1528942 0.00000000 0.04252433 0.09911197 0.04756515 0.00000000 0.08119979\n2 0.0000000 0.31596888 0.17533229 0.36009167 0.30082842 0.06356417 0.09376145\n3 0.1441500 0.04045567 0.00000000 0.00000000 0.11651035 0.00000000 0.38923215\n4 0.0000000 0.12793206 0.22443986 0.00000000 0.33580828 0.07784989 0.25246042\n5 0.0000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n6 0.2573043 0.45294784 0.33911838 0.48952099 0.28824313 0.39950351 0.06629935\n         S69        S6        S72        S73       S74      S75       S76\n1 0.12149135 0.0412393 0.06765101 0.08753762 0.1849937 0.000000 0.1237179\n2 0.06074567 0.0000000 0.31001587 0.27681827 0.0000000 0.000000 0.0000000\n3 0.00000000 0.1797580 0.22941573 0.00000000 0.1807754 1.006459 0.6441024\n4 0.22146362 0.8184629 0.00000000 0.00000000 0.2061156 0.000000 0.0000000\n5 0.00000000 0.0000000 0.00000000 0.27681827 0.0000000 0.000000 0.0000000\n6 0.33253266 0.0000000 0.23310641 0.53134923 0.2188566 0.134840 0.0000000\n         S77        S78        S79         S7        S80       S81        S8\n1 0.31370944 0.11909827 0.24942152 0.04719292 0.16464639 0.0433555 0.0000000\n2 0.00000000 0.00000000 0.00000000 0.00000000 0.07761505 0.3386173 0.0000000\n3 0.19649437 0.56472318 0.08971226 0.23596459 0.52008893 0.0000000 0.7211103\n4 0.06213698 0.65223431 0.08971226 0.00000000 0.05488213 0.0000000 0.0000000\n5 0.19649437 0.00000000 0.00000000 0.00000000 0.05488213 0.0000000 0.0000000\n6 0.43896637 0.08421519 0.16353430 0.13348173 0.26499437 0.2410773 0.0000000\n          S9\n1 0.08596024\n2 0.00000000\n3 0.26193862\n4 0.00000000\n5 0.00000000\n6 0.24814583\n\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at your resulting data set and determine if it is a tidy data set or not. Then describe how you would transform it into a tidy data set and explain why those changes make it fulfill all the conditions for it to be tidy.\nSpoiler alert: It’s not tidy … go ahead and create an object called tidy_counts that’s a tidy data frame.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (it’s probably clear why tidy data sets are also referred to as long data sets at this point …)\n\n\n# A tibble: 6 × 9\n  Kingdom  Phylum           Class        Order Family Genus Species ID     count\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0     \n2 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818\n3 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0     \n4 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958\n5 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238 \n6 k__Fungi p__Basidiomycota c__Agaricom… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nHm, if only our tidy_counts object also contained the sample meta-data currently in our dataframe soil. Go ahead and add that information to our tidy_counts data frame. Print the first few rows of your data frame to the console to make sure this worked.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nThis is what your result should look like (are we having fun yet?)… if you are having problems combining these data frames recall that you need one column in common. It’s easiest if those columns also share the same column name, but you can look up the function to see if there is a way around it if they don’t match up.\n\n\n# A tibble: 6 × 17\n  Kingdom  Phylum   Class Order Family Genus Species ID     count sampleID plot \n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n1 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S10   0      M_FS_1/… M_FS…\n2 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S11   0.0818 M_FS_1/… M_FS…\n3 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S12   0      M_FG_FS… M_FG…\n4 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S13   0.0958 M_FS_1/… M_FS…\n5 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S14   0.238  M_AS_FS… M_AS…\n6 k__Fungi p__Basi… c__A… o__A… f__Tr… g__M… &lt;NA&gt;    S15   0.0487 M_FS_1/… M_FS…\n# ℹ 6 more variables: block &lt;dbl&gt;, forest &lt;chr&gt;, horizon &lt;chr&gt;, Carbon &lt;dbl&gt;,\n#   Nitrogen &lt;dbl&gt;, pH &lt;dbl&gt;"
  },
  {
    "objectID": "12_community-diversity.html#characterize-community-diversity",
    "href": "12_community-diversity.html#characterize-community-diversity",
    "title": "12  Characterizing community diversity",
    "section": "12.4 Characterize community diversity",
    "text": "12.4 Characterize community diversity\nOkay… now we’re ready to have some fun.\nFirst, let’s find out what taxonomic groups are present in the entire data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPrint a series of tables to the console and in your html document3 that comprise a single column each that show all the phyla, classes, orders, famiies, genera, and species in the data set in alphabetical order, respectively. Note that the values in those columns have a prefix indicate the taxonomic level. Get rid of that in your output.\n\n\n3 use the function kable() to print the entire data frame in a pretty table\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\n\n\n\n\nAscomycota\n\n\nBasidiomycota\n\n\nGlomeromycota\n\n\nMortierellomycota\n\n\nMucoromycota\n\n\nOlpidiomycota\n\n\nRozellomycota\n\n\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nNow, let’s compare patterns across different forest plot types. Create separate tables and print them to the console/have them print neatly in your rendered html files for easy comparison of the mean relative abundance for phylum, order, and family for each forest plot type. Print the first four digits only.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s what the table for phylum should look like\n\n\n\n\n\nPhylum\nAS\nFG\nMixed\n\n\n\n\np__Ascomycota\n0.0248\n0.0236\n0.0254\n\n\np__Basidiomycota\n0.0143\n0.0139\n0.0132\n\n\np__Glomeromycota\n0.0121\n0.0000\n0.0000\n\n\np__Mortierellomycota\n0.0178\n0.0159\n0.0101\n\n\np__Mucoromycota\n0.0250\n0.0332\n0.0419\n\n\np__Olpidiomycota\n0.0000\n0.0000\n0.0044\n\n\np__Rozellomycota\n0.0041\n0.0113\n0.0042\n\n\nNA\n0.0381\n0.0257\n0.0276\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPick one forest plot type that you are interested in exploring further. Create tables that make it easy to compare the mean relative abundance for phylum, order, and family across the different soil horizons, and print those three table to the console and to your rendered html report.\nDescribe your results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nMeasures of diversity enable us to quantify the complexity of biological communities in a way that allows us to objectively compare communities across space and time. Measures of alpha diversity describe the diversity of a single sample and are based on the number of observed taxonomic groups or their relative abundance, beta diversity describes the variation between samples, gamma diversity describes the global diversity observed across a large number of communities.\nCommon alpha diversity statistics include:\n\nobserved richness the number of observed taxa.\nShannon Index (Shannon-Weaver or Shannon-Wiener Index) quantifies how difficult it is to predict the identity of a randomly sampled individual. Ranges from 0 (total certainty) to log(S) (total uncertainty).\nSimpson Index quantifies the probability that two randomly chosen individuals are the same taxonomic group (ranges from 0 to 1).\nInverse Simpson Index is defined as the number of species needed to have the same Simpson index value as the observed community assuming a theoretical community where all species are equally abundant.\n\nThe Shannon and Simpson Diversity are entropy-based indices that measure the disorder (diversity) of a system. In information theory entropy is used to describe the fact that we can quantify the degree of uncertainty associated with with predicted pieces of information. Applied to ecology this means that when describing diversity using these indices we are determining whether or not individuals randomly drawn from a community are the same or different species (or other taxonomic group).\nThe relationship between species richness and Shannon diversity is non-linear, i.e. at higher levels of species richness, communities appear more similar in terms of the magnitude of the index compared to lower levels of species richness - which is counter intuitive to the way species richness works. The solution to this is to linearize the indices. As a result, more recently, diversity indices have been proposed where diversity values are converted into equivalent (or effective numbers) of species (jost_entropy_2006?). The effective number of species is the number of species in a theoretical community where all species (taxonomic groups) are equally abundant that would produce the same observed value of diversity (a similar principle is applies in genetics for the concept of effective population sizes). While the definition of effective numbers is not as intuitive as the entropy-based ones the values that we yield are. Not only are the “units” species (instead of being a unitless index), but they have properties that we intuitively understand. For example, effective numbers obey the doubling principle: If you have two communities with equally abundant but totally distinct species and combine them, that new community would have a diversity that is 2x that of the original ones.\nThe package vegan has several functions implemented that allow us to calculate these diversity stats. Look up any functions you are not familiar with in the following code and comment it to describe what each line of code is doing.\n\ndiversity &lt;- tidy_counts %&gt;%\n  group_by(ID, forest, horizon) %&gt;%\n  summarize(richness = specnumber(count),\n            shannon = diversity(count, index = \"shannon\"),\n            simpson = diversity(count, index = \"simpson\"),\n            invsimpson = diversity(count, index = \"invsimpson\")) %&gt;%\n  pivot_longer(cols = c(richness, shannon, simpson, invsimpson),\n               names_to = \"metric\")\n\nWe can create a series of plots that compare the different diversity stats for each forest plot type and soil horizon location4.4 Well, I can but you will be able to soon, too\n\nggplot(diversity, aes(x = forest, y = value, color = forest)) +\n  geom_boxplot(aes(color = forest), outlier.shape = NA, fill = \"transparent\", size = 1) +\n  geom_point(aes(fill = forest),\n             position = position_jitterdodge(jitter.width = .3),\n             shape = 21, color = \"black\", size = 3) +\n  facet_grid(metric ~ horizon, scales = \"free\") +\n  labs(x = \" \", y = \" \")\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nDescribe the results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html",
    "href": "11_bioinformatics.html",
    "title": "11  Bioinformatics",
    "section": "",
    "text": "12 Infer amplicon sequence variants (ASVs) and create sequence table\nRunning this step on all samples at the same time is computationally more intensive than running the pipeline one sample at a time but increases the functions ability to resolve low-abundance ASV, including singletons."
  },
  {
    "objectID": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "href": "11_bioinformatics.html#next-generation-sequence-data-enable-metabarcoding-to-describe-community-composition",
    "title": "11  Bioinformatics",
    "section": "11.1 Next-generation sequence data enable metabarcoding to describe community composition",
    "text": "11.1 Next-generation sequence data enable metabarcoding to describe community composition\nHigh-throughput sequencing data, often referred to as next-generation sequencing (NGS) data, is a type of biological data generated through advanced sequencing technologies that allow for the rapid and simultaneous sequencing of a large number of DNA or RNA molecules. These technologies have revolutionized genomics, transcriptomics, and other related fields by enabling the efficient and cost-effective analysis of genetic material on a massive scale. There are various high-throughput sequencing platforms available, including Illumina, Ion Torrent, Pacific Biosciences (PacBio), Oxford Nanopore Technologies (ONT), and others. High-throughput sequencing has become more cost-effective over time, making it accessible to a broader range of researchers and enabling large-scale genomics projects.\nThe key difference between high-throughput sequencing platforms compared to traditional Sanger sequencing3 is that they are capable of simultaneously sequencing thousands to millions of DNA fragments or RNA molecules in a single sequencing run. This parallel processing greatly increases the speed and efficiency of sequencing compared to traditional Sanger sequencing.3 anger sequencing, also known as dideoxy sequencing or chain-termination sequencing, is a traditional DNA sequencing method that was developed by Frederick Sanger and his colleagues in the late 1970s. It was the primary method for DNA sequencing for several decades before the advent of high-throughput next-generation sequencing (NGS) technologies. Sanger sequencing is still used today for specific applications, such as sequencing individual genes or validating NGS results.\nNGS platforms produce a vast amount of data. A single sequencing run can generate gigabytes to terabytes of raw sequencing data, depending on the instrument’s capacity and the type of experiment being conducted4. Typically, high-throughput sequencing generates short sequences, referred to as “reads.” The length of these reads can vary depending on the sequencing platform but is typically in the range of 50 to 300 base pairs. Usually you get one forward and one reverse read per sequenced template DNA.4  Don’t worry, we are going to use a data set that consists of samples have been modified to reduce size but still preserve realistic sequence variation and errors to create small example data set that your laptops can handle.\nIn the context of metabarcoding of environmental DNA this means that we can use universal primers to amplify template DNA present in the environmental sample that potentially are from different organisms5 because we can sequence all the amplified template DNA strands at once.5 If you used Sanger sequencing you would just get a bunch of noise, because the sequencer would not be able to make a distinct base call for any position"
  },
  {
    "objectID": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "href": "11_bioinformatics.html#bioinformatics-pipelines-are-important-tools-to-processing-ngs-data",
    "title": "11  Bioinformatics",
    "section": "11.2 Bioinformatics pipelines are important tools to processing NGS data",
    "text": "11.2 Bioinformatics pipelines are important tools to processing NGS data\nAnalyzing high-throughput sequencing data requires sophisticated bioinformatics tools and pipelines. Bioinformatics pipelines are widely used in genomics and related fields to streamline and standardize data analysis workflows. They comprise a series of structured and automated series of computational and data analysis steps designed to process and analyze biological data. Frequently, data takes on of several paths through the pipeline where the output from one step becomes the input for the next step.\nGenerally, a bioinformatics pipeline takes a specific type(s) or raw input data, such as DNA or RNA sequence data, frequently generated from high-throughput techniques. This data set then goes through the first stage of the pipeline where it is pre-processed for analysis steps, including quality control, data cleaning, and format conversion to ensure the data set is robust and meets a standardized format for downstream analysis. Pipelines frequently consists of a series of analysis steps or modules tailored to specific research questions/tasks, for example sequence alignment, variant calling, gene expression quantification, protein structure prediction.\nBecause pipelines are designed to allow for a high degree of automation to reduce error and maximize efficiency. Frequently, users use scripts or specific workflow management tools to execute each analysis step in a predefined errors. Because of this automation, it can be tempting to “blackbox” the analysis where an inexperienced user can run complex data analysis without understanding what is happening at each stage. However, for an analysis appropriate to a specific data set it is critical that scientists understand how to configure parameters and options for each analysis step to customize the pipeline for the specific data set and research objectives.\nBioinformatics tools are often run from the command line. Linux is frequently the operating system of choice6 as it provides a powerful and standardized command-line interface that makes it easy to automate tasks, write scripts, and integrate various tools into analysis pipelines. It also offers a high degree of customization and flexibility due to its open source nature7. In addition, it is known for its efficiency and stability which is critical when running resource-intensive analysis.6 You’re laptop is likely MacOS or Windows and Linux would be the third common OS, however, it is not designed to be as user-friendly as Mac/Windows and has a steep learning curve.7 This also means its free!\nHigh performance clusters allow for efficient handling of large data set by allowing multiple tasks to be executed in parallel8.8 Your laptop probably has two to four cores, which means you could run tasks in parallel on up to two threads. Many linux servers have upward of 20 threads to run things in parallel on, for high performance clusters (HPCs you may be looking at 100s)\nDon’t worry, you’re not going to have to learn how to use a Linux Terminal. We will be using a pipeline that runs in R.\nDada2 (Callahan et al. 2016) is a bioinformatics pipeline which implements functions that can be used for the quality control, denoising, and analysis of amplicon sequencing data, particularly data generated from high-throughput sequencing technologies like Illumina’s MiSeq or HiSeq platforms. It is widely used in microbiome research and is becoming increasingly popular for the analysis of environmental DNA (eDNA) metabarcoding data sets.\n\nCallahan, Benjamin J., Paul J. McMurdie, Michael J. Rosen, Andrew W. Han, Amy Jo A. Johnson, and Susan P. Holmes. 2016. “DADA2: High-resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7, 7): 581–83. https://doi.org/10.1038/nmeth.3869.\nThe key modules (steps) include\n\nQuality Filtering: DADA2 begins by assessing the quality of each sequence read in the dataset. Low-quality reads, which may contain sequencing errors or noise, are filtered out. This step helps improve the accuracy of downstream analysis.\nSequence Dereplication: Identical sequences are collapsed into unique sequence variants (also known as amplicon sequence variants or ASVs). This step reduces redundancy in the dataset and accounts for potential PCR or sequencing errors.\nDenoising: DADA2 employs a statistical model to distinguish between true biological sequence variants and sequencing errors. By modeling the error profile of the sequencing data, it can identify and correct errors, resulting in more accurate sequence variants.\nChimera Removal: DADA2 identifies and removes chimera sequences, which are artificial sequences generated during PCR that can lead to incorrect biological interpretations.\nPhylogenetic Assignment: After denoising, DADA2 assigns taxonomy to the sequence variants by comparing them to reference databases, allowing researchers to identify the taxa present in the sample.\n\nIn contrast to other pipelines used to process metabarcoding data sets, DADA2 generates ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nExplain what the difference between an OTU and and ASV (include what each abbreviation stands for). You will want to revisit this question after having completed this tutorial and having a better understanding of the methods implemented in DADA2.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nEnough chit chat! Let’s go!"
  },
  {
    "objectID": "11_bioinformatics.html#demultiplexing",
    "href": "11_bioinformatics.html#demultiplexing",
    "title": "11  Bioinformatics",
    "section": "11.3 Demultiplexing",
    "text": "11.3 Demultiplexing\nGenerally, multiple samples are pooled and run on the same NGS sequencing lane. The amplified sequences (amplicons) of each sample are tagged with the same unique molecular barcode or index. This means that the first step is to take the entire output of a sequencing run and demultiplex it, i.e. each sequence read is assigned back to its respective sample based on the barcode/index combination.\n\n\nFASTQ files are a common file format used to store biological sequence data, particularly data generated by next-generation sequencing (NGS) technologies like Illumina sequencing platforms. In contrast to FASTA files which contain only sequence data, FASTQ files contain information about the sequences of DNA or RNA fragments, associated quality scores, and optional sequence identifiers.\nThis data set has already been demultiplexed and the individual fastq files are in your data folder."
  },
  {
    "objectID": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "href": "11_bioinformatics.html#quality-check-and-filter-sequences",
    "title": "11  Bioinformatics",
    "section": "11.4 Quality check and filter sequences",
    "text": "11.4 Quality check and filter sequences\nIf you look in the data folder your project directory, you will see that you have a folder called seq that contains a folder for your raw sequences and also one that we will use to hold your sequences once we have done some quality filtering and trimming.\nBefore we get started we are going to create a series of vectors that contain our file paths and sample names that we can use throughout the analysis9. We are also going to extract the sample names and save them in a variable. We are able to do this because all of our filenames have a similar pattern.9 There are some key advantages to setting up vectors in this way. First, you only need to type in the information once, and then you can just call the vector which is a lot shorter and easier to type in. Second, you make the code more easily reusable for another analysis because you only need to change the file paths in one location and don’t have to find all the spots in your code where the file path needs to be changed. Finally, as you will see you are working with a large number of individual files, so when we create sample names based on the file names we don’t have to type them in all by hand which saves time and also minimizes the chances of typos.\n\n# designate file paths ----\n\n# create variable with initially trimmed reads\nraw &lt;- \"data/seq/raw\"\n\n# path for filtered reads\nfilt &lt;- \"data/seq/filt\"\n\n\n# create lists of matched forward & reverse fastq files ----\n\n# forward reads\nfnFs &lt;- sort(list.files(raw, pattern = \"_R1.fastq\", full.names = TRUE))\n\n# reverse reads\nfnRs &lt;- sort(list.files(raw, pattern = \"_R2.fastq\", full.names = TRUE))\n\n# create vector with sample names\nsample.names &lt;- substr(basename(fnFs), 1, (nchar(fnFs)-25))\n\n\n# create file names & paths for filtered reads ----\n\n# named vectors of forward reads \nfiltFs &lt;- file.path(filt, glue(\"{sample.names}_R1.fastq.gz\"))\nnames(filtFs) &lt;- sample.names\n\n# named vectors of reverse reads \nfiltRs &lt;- file.path(filt, glue(\"{sample.names}_R2.fastq.gz\"))\nnames(filtRs) &lt;- sample.names\n\nThere are 74 samples in the data set.\nNow that we have all of our file paths set up our next step is to take a look at the quality profile of the sequences. We could look at all of them, however, usually it is sufficient to spot check a few sequences to get an idea of what the quality looks like to give us an idea of what threshold values we should be using for quality filtering.\nDADA2 has a built in function plotQualityProfile() that pulls the information from the fastq files which contains the quality information for each nucleotide call. Quality is measured as PHRED scores, a score of 40 should be interpreted as an expected 1 in 10,000 error rate, 20 would be a 1 in 100 chance of a base call being incorrect.\n\n\nWe are wrapping the plotting function in ggplotly() which is built on the plotly package to generate interactive figures. It will take a second for your figure to pup up in the Viewer pane on the bottom left of Rstudio. If you hover over different points of the plot with your mouse you will see a little pop that lists the exact values.\nLet’s go ahead an plot the quality sequence for the first sample We can do this by passing the first file path in the fnFs vector using [1] to indicate the first element of the vector.\n\nggplotly(plotQualityProfile(fnFs[1]))\n\n\n\n\n\n\nFigure 11.1: Quality scores for the forward reads of the first sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is em quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nAs you can see there is a lot going on in this figure. The x-axis indicates the cycle number which is equivalent to the base position in the sequence (one base call is added per cycle) while the y-axis indicates the quality score. In the bottom left you can see the number of reads (sequences) in the file indicated. You can see grey shading which is a heatmap indicating the frequency of each score at each position. Darker colors indicate that a certain quality score is more common at that position across all the reads in the sample. The green line is the mean quality score, orange lines are the quartiles. The read lines indicates the proportion of reads that extend to that position.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nGo ahead and plot the reverse reads for the 12th sample in the data set. Then describe the quality patterns you are seeing for your forward and reverse reads. You can use the table below that summarizes how to interpret the quality scores.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplotly(plotQualityProfile(fnRs[12]))\n\n\n\n\n\n\nFigure 11.2: Quality scores for the reverse reads in 12th sample. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The red line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nReverse reads typically have lower quality and larger drop-off compared to forward reads. Trimming too conservatively can result in downstream issues when forward and reverse reads cannot be merged due to a lack of overlap.\n\n\n\n\n\n\n\n\n\n\nQuality score of base call\nConfidence of base call being correct\n\n\n\n\n10\n90\n\n\n20\n99\n\n\n30\n99.9\n\n\n40\n99.99"
  },
  {
    "objectID": "11_bioinformatics.html#trim-sequences",
    "href": "11_bioinformatics.html#trim-sequences",
    "title": "11  Bioinformatics",
    "section": "11.5 Trim sequences",
    "text": "11.5 Trim sequences\nWe will use the information from the quality scores to make decision on appropriate threshold values to use to trim our sequences.\nWe are going to use a series of filters to remove low quality reads from the samples. Specifically, we will remove any basecalls that were too ambiguous to call,10 and any remaining PhiX reads still in the data set11. Next, the first and last xx bases are trimmed for each read as these are usually low quality. Additionally, reads are truncated at first instance of a quality score &lt; 6. Finally, reads &lt; 35 bp after trimming are removed.10 N are unknown nucleotides. If the signal for a base is too ambiguous to make a call, the Illumina platform will call it N. DADA2 assumes there are no NNN in the data set so we have to remove them.11 PhiX is a bacteriophage. It’s DNA is spiked into libraries being sequenced to improve the quality sequencing run by increasing the sequence diversity\nWe are assuming matching order of forward and reverse reads for each sample.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the filterAndTrim() function and annotate each line of the code to indicate what each argument does, include both what the argument controls in general and then specifically what this means for this example.\n\n\n\n# filter reads\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, #\n                     truncLen = c(280, 280),     #\n                     trimLeft = c(18, 2),        #\n                     truncQ = 6,                 #\n                     maxEE = c(2,2),             #\n                     minLen = 35,                #\n                     rm.phix = TRUE,             #\n                     matchIDs = FALSE,           #\n                     compress = TRUE)            #\n\n\n\n\n\n\n\nPointers\n\n\n\n\n\nHere are descriptions of the key arguments that set thresholds for quality filters\n\ntruncQ: sets a minimum Q score. At the first instance of a quality score less than or equal to truncQ, the sequence is truncated.\ntruncLen: sets the length at which the sequences will be truncated. Sequences shorter than the length are eliminated.\ntrimLeft: sets the length that will be removed on the 5’ side of the reads. This allows you to remove the primers if it has not been done beforehand12.\nmaxEE: sets the maximum number of “expected errors” allowed in a read. This filter is based on the Q index. The more the number is increased, the less strict we are.\n\n12 Primers used are ITS3_KYO2: GATGAAGAACGYAGYRAA = 18bp ITS4: TCCTCCGCTTATTGATATGC = 20b\n\n\nLet’s take a look at what that function has done. We assigned the output to a variable called out.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse the functions class() and head() to get an idea of what type of object we have created and then briefly describe what information is contained in that object.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWait - that function has created a matrix array (a type of object you could describe as a multidimensional vector or a poor man’s data frame) that tells us how many reads per sample went into a the filters and then who many came out.\nBut where did our filtered reads go? Remember, we gave the function the file paths for both the input files (data/seq/raw/*.fastq.gz) and where to write the filtered files and what to name them (data/seq/fil/*.fastq.gz). If you use the file navigation pane you should see that it now contains those files.\nThe object we created that is now in our environment holds the record of what occurred during filtering. It is very common that bioinformatics pipelines generate output files at various steps that let the user track what is happening that can be used to ensure everything is going as expected and also allow them to pick up on unusual patterns that might be indicative of the fact that your threshold values might not be appropriate, that there is something odd going on with the data, or that perhaps commands where not properly formulated and therefore didn’t take place the way the researcher expected them to.\n\n\n\nLet’s take a look to see how many reads where removed from the data set.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nThis code chunk converts the matrix into a dataframe. Use your coding skills to create a new column called perc_lost that contains the percent of reads that were lost in this filtering step. Then calculate the mean and standard deviation of the percent reads lost and the mean and standard deviation of the number of reads that remain per sample.\nThen use kable() to display your function in the console/your rendered html report.\nNote the inline code below that is pulling the key pieces of information directly from your data frame. This can be really helpful because e.g. if you are rerunning code on an updated data set you would not have to dig through your output to make sure that you updated all the results, instead R can do that for you. You can indicate that it is inline code using backticks and then indicating the engine using r. You will see that notation through out this document.\nThe function pull() allows you to extract (pull) a single value from a column.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n\n# summarize proportion of reads lost during trimming\ncheck &lt;- as.data.frame(out) %&gt;%\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nmean_loss\nstd_loss\nmean_reads\nstd_reads\n\n\n\n\n49.05541\n8.624355\n509.4459\n86.24355\n\n\n\nTable 11.1: Mean and standard deviation of the proportion of reads lost due to quality filtering and the mean and standard deviation of the number of remaining reads per sample post filtering.\n\n\n\n\n\nAfter quality trimming samples contain approx. 509% (+/- 86) reads. Quality trimming removed 49.1% (+/- 8.6%) reads from each sample.\nWe will track how many reads we “lose” at each step to understand how different steps affect the number of reads that remain in the sample.\nNext to knowing how many low quality reads where removed we will want to take a look at the quality of the remaining reads after we have trimmed.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nPlot the quality scores for the forward and reverse of one random sample and compare your results to the raw data. Make sure to add a figure caption using the code chunk options!\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what the plot for your forward reads should look like.\n\n\n\n\n\nFigure 11.3: Quality scores of forward reads for 16 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position.\n\n\n\n\nRemember to plot the reverse reads for the same sample.\n\n\n\n\n\nFigure 11.4: Quality scores of reverse reads for 12 random samples in the data set after trimming. Grey-scale underlying heatmap shows frequency of each score at each base position (darker color is higher frequency), green line is median quality score for base position, orange lines indicate the quartiles (solid is median, dashed = 25th and 75th quartile). The read line indicates the percentage of reads that extend to at least that position."
  },
  {
    "objectID": "11_bioinformatics.html#generate-error-model",
    "href": "11_bioinformatics.html#generate-error-model",
    "title": "11  Bioinformatics",
    "section": "11.6 Generate error model",
    "text": "11.6 Generate error model\nNext, we need to be able to distinguish between error due to for example PCR or sequencing error and actual biological differences among sequences. We can train DADA2 to be able to do this using a subset of our data as a training set using a machine learning approach to establish a parametric error model13 to estimate error rates.13 Note that the error rate is specific to a sequencing run\nError rates are generally expected to drop with increased quality. By default 100 Million bases are used to generate the error model, this number can be increased for a better estimate.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRun the code below to generate the error models. This will take a few seconds. While you wait, pull up the description of what this function does in the help pane and use comments to indicate what the function as a whole does and what the different arguments control.\n\n# forward reads\nerrF &lt;- learnErrors(filtFs,               # \n                    nbases = 1e8,         # \n                    multithread = FALSE)  # \n\n9877138 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n# reverse reads\nerrR &lt;- learnErrors(filtRs, \n                    nbases = 1e8,\n                    multithread = FALSE)\n\n10480322 total bases in 37699 reads from 74 samples will be used for learning the error rates.\n\n\n\n\nMachine learning estimates and observed values should show close overlap to indicate the quality (fit) of the model. The observed error rates should be consistent with the expected learned error rates for the 16 possible base transitions. If they do not, the number of bases used can be increased to allow the ML algorithm to train on a larger subset of the data.\nWe can use the function dada2::plotErrors() to compare the observed and estimated error plots as an indication of how good our error models are. We will plot both the forward and reverse error model and assign those to objects so we can use the patchwork package to plot them next to each other in a single plot14.14 the package patchwork allows us to combine multiple plots in one file. You can read up on the basic layout options using +, \\ and | here and more a sneak peak of more complex layouts here\n\n# Visualize estimated error\np1 &lt;- plotErrors(errF, nominalQ = TRUE)\n\np2 &lt;- plotErrors(errR, nominalQ = TRUE)\n\np1 / p2\n\n\n\n\nFigure 11.5: Error rates for each possible transition for forward (top) and reverse reads (bottom). Observed (grey points) and estimated (black line) error rates for each consensus quality score. Expected error rates for nominal definition of the quality score are in red.\n\n\n\n\n\n\nThe plot will pop up in the Plot pane. You can change the size of the pane to resize and make it larger, it can also be helpful to use the zoom button to create a popout window for an even better look.\nNote also that you can use the code chunk option fig-height: to control the size in your rendered html output file.\nAs you can see this creates a series of plots - one for each possible transition e.g. the error frequency for an A being mistakenly called a C (A2C) etc. The black points are the observed error rates for each consensus quality scores. The black line is the estimated error rate after the model has converged. The red line indicates the expected error rates under the nominal definition of the Q-value for Illumina technology.\n\n\n\n\n\n\n Consider this\n\n\n\nUse the figures to briefly describe how well our error models capture the observed data and how this compares to the expected error rates.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "href": "11_bioinformatics.html#dereplicate-infer-and-merge-asvs",
    "title": "11  Bioinformatics",
    "section": "12.1 Dereplicate, infer, and merge ASVs",
    "text": "12.1 Dereplicate, infer, and merge ASVs\nNow we are ready to identify the ASVs present in our data set.\nThe first step to do this is dereplication, which means that we are collapsing all identical reads into a set of unique sequences. Dereplicating or denoising data is an important step in amplicon processing workflows, instead of keeping all identical sequences only one is kept for processing and the number of sequences represented is stored a long with it. A new consensus quality score profile is calculated for each unique sequence based on the average quality score of each base for all sequences that are replicates of that given amplicon. These quality profiles Dereplication makes downstream processing a lot more efficient and less memory intense by eliminating redundant comparisons.\n\n\nThe function lapply() is similar to a for loop, where it applies the same function to every file in the vector containing the filtered samples.\n\nderepFs &lt;- lapply(filtFs,      \n                  derepFastq,      \n                  verbose = FALSE)  \n\nderepRs &lt;- lapply(filtRs, \n                  derepFastq, \n                  verbose = FALSE)\n\nIn the next step, sequence variants are inferred using the dereplicated data and the inferred error rates. Using the consensus quality profiles significantly increases DADA2’s accuracy.\n\n\nAs we’ve mentioned previously, many metabarcoding pipelines use OTUs instead of ASVs. For OTU’s reads that are at least 97% similar are clustered. There are two arguments for doing this, the first is many species have more than one haplotype for the same locus and so if you set the threshold for clustering at 100% you are oversplitting i.e. multiple unique sequences can “belong” to the same species. The second is that sequences from the same species might end up with different sequences not because of true biological differences but because of errors in the process of PCR amplifying sequences or during sequencing. However, ASVs are not the same thing as OTUs with a 100% threshold for clustering, instead, DADA2 uses the information from the quality profiles and the error models to distinguish true biological differences that separate unique amplicons vs amplicons that are only different due to errors (i.e. base differences are artifacts).\n\ndadaFs &lt;- dada(derepFs,            \n               err = errF,         \n               selfConsist = FALSE,\n               multithread = TRUE) \n\ndadaRs &lt;- dada(derepRs,\n               err = errR,\n               selfConsist = FALSE,\n               multithread = TRUE)\n\n\n\nDifferent sequence platforms and sequencing kits are limited by how long the reads can be. For example, this data set was sequenced on a 2x300bp Miseq platform. This means that each amplicon was sequenced 300 bp in the 5’ to 3’ direction and then 300bp in the 3’ to 5’ direction. This means that we can sequence inserts longer than 300 bp and merging the two strands (forward and reverse reads) increase the confidence in the reliability of the sequence in the overlapped region.\nOur last step is that we need to merge our forward and reverse reads to reconstruct the full target amplicon. Forward and reverse-complement of corresponding reverse sequences are aligned and merging requires an overlap of 12 sequences and there are no mismatches permitted in the overlapping region. Paired reads that do not exactly overlap are removed to reduce spurious output.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLook up the mergePairs() function in the help pane and comment the code below to indicate what the function as a whole and each argument does.\n\nmergeASVs &lt;- mergePairs(dadaFs, filtFs, \n                        dadaRs, filtRs,\n                        minOverlap = 12,  \n                        maxMismatch = 0)\n\n\n\nWe can take a look at the results. The function returns a list16. Let’s take a look at the first element.16 Remember, we can think of a list as an object that is like a shelf where each shelf holds one element in this case each sample is an element\n\nhead(mergeASVs[1])\n\n$S1\n                                                                                                                                                                                                                                                                                                                                                                                                            sequence\n1             ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCCCCTTGGTATTCCGAGGGGCACACCCGTTTGAGTGTCGTGAATACTCTCAACCTTCTTGGTTTCTTTGACCACGAAGGCTTGGACTTTGGAGGTTTTTCTTGCTGGCCTCTTTAGAAGCCAGCTCCTCCTAAATGAATGGGTGGGGTCCGCTTTGCTGATCCTCGACGTGATAAGCATCTCTTCTACGTCTCAGTGTCAGCTCGGAACCCCGCTTTCCAACCGTCTTTGGACAAAGACAATGTTCGAGTTGCGACTCGACCTTACAAACCTTGACCTCAAATCGGGTGAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n2                                                                                             ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATCCCGAGGGGCATGCCTGTTCGAGCGTCATTTCACCACTCAAGCCTGGCTTGGTGTTGGGCGACGTCCCCTTTTGGGGACGCGTCTCGAAACGCTCGGCGGCGTGGCACCGGCTTTAAGCGTAGCAGAATCTTTCGCTTTGAAAGTCGGGGCCCCGTCTGCCGGAAGACCTACTCGCAAGGTTGACCTCGGATCAGGCAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n3                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTATAACCACTCAAGCCCCGGCTTGGTCTTGGGGTTCGCGGTCCGCGGCCCTTAAACTCAGTGGCGGTGCCGTCTGGCTCTAAGCGCAGTAATTCTCTCGCTATAGTGTCTAGGTGGTTGCTTGCCATAATCCCCCAATTTTTTACGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n4                                          ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACTCCTTGGTATTCCGAGGAGTATGCCTGTTTCAGTATCATGAGCACTCTCACACCTAACCTTTGGGTTTATGGCGTGGAATTGGAATGCGCCGACTGTCATGGTTGGCCCTTCTAAAATGTAGTTCTTGGCTGTCACCTAATACAGCAGTTTGGCCTAATAGTTTTGGCATTCATTGTCAAATCTTTGGCTAACATTTGCTCCAGGAGTCAGTCTTGATAATACAGAAAACTCATTCAAATTTTGATCTGAAATCAGGTAGGGCTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n5          ATGCGATACGTAATGTGAATTGCAGAATTCCGTGAATCATTGAATCTTTGAACGCATCTTGCGCCTCTTGGTATTCCGAGGGGCATGCCTGTTTGAGTGTCATTAGAACTATCAAAAAAATAGATGATTTCAATCGTTAATTTTTTTGGAATTGGAGGTGGTGCTGGTCTTTTTCCATTAATGGCCCAAGCTCCTCCGAAATGCATTAGCGAATGCAGTGCACTTTTTCTCCTTGCTTTTTCTGGGCATTGATAGTTTACTCTCATGCCCTAAGCTGGTAGGGAGGAAGTCACAGAATGCTTCCCGCTCCTGAATGTAATACAAAACTTGACGATCAAACCCCTCAAATCAGGCAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n7                                                                                 ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATGATCAACCATCAAGCCTGGCTTGTCGTTGGACCCTGTTGTCTCTGGGCGACAGGTCCGAAAGATAATGACGGTGTCATGGCAACCCCGAATGCAACGAGCTTTTTTATAGGCACGCATTTAGTGGTTGGCAAGGCCCCCTCGTGCGTTATTATTTTCTTACGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n8                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTGCAACCCTCAAGCATTGCTTGGTATTGGGCTCCGCTGCTCACCCAGCGGGCCTTAAAATCAGTGGCGGTGCCGTCGAGGCCCTGAGCGTAGTAAATATCCTCGCTATAGGGACTCGGTGGACGCTGGCCATTAACCCCCAACTTTCTAAGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n9                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCCTCTGGTATTCCAGGGGGTATGCCTGTTCGAGCGTCATTACAACCCTCAAGCACTGCTTGGTATTGGATGTCAACCATTGGTGGTGCATCTCAAAAGTATTGGCAGTAGCATTTAGCTTCTAGTGTAGTAAATTTCTCGCTTTGGAGTCAAGTGTCTAATTGCTAGATAGAACCCCTAATTTATCAAAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n10                                                                                                   ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCGCTCGCGGCCGGCCCTAAAGACAGTGGCGGCAGCGTCTGGCTCCAAGCGTAGTACAATCCTCGCTCTGGTGCTAGGCGGTGGCCTGCCAGAACCCCCCTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n11                                                                                               ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCACCTTCTGGTATTCTGGGAGGTATGCCTGTTCGAGCGTCATTGCAACCATCAAGCCTAGGCTTGGTATTGGATGCCACCGCTTGGTGCATTTCAAAATTAGTGGCGGTGCCATTCAGCTTCAAGCGTAGTAAATTTCTCGCTCCTGGAGTTTGTATGTTGTCTGCTAGAACCCCCTAATTTATCAAGGTTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n12                                                                                                                       GCGCGATAGGTATTGTGAATTGCAGAATTGTGAATCATCGAATTTTTGAACGCACATTGCACCCATTGGTATTCCGATGGGTATACTTGTTTGAGCGTCATTTCATTCTCCTTTTGGGTTTTGGCATGAATATTTCTTGCTGAATTATAATGGTGTGGCTACCAGACTACAACGTGATAGATATTTCGTTGGATGTGACTGGGATTGCTCACCTTAAAAACATTGTATAGACCTCAAATCAAGCAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n13 ATGCGATACGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCGCTCTTTGGTATTCCGAAGAGCATGCTTGTTTGAGTATCAGTAAACACCTCAACCTCCTCTTGTTTTTTCAAAAGGAGGGTGGACTTGAGCTATCCCAACAACCTTCACCGGTAGGCGGGCGGCTTGAAATGCAGGTGCAGCTGGACTTTTATCTGAGCTAAAAGCATATCTATTTAGTCCTCGTCAAACAGGATTATTACTATTGCTGCAGCTAACATAAAGGATAATTGTCCTCATTGCTGACTGATGCAGGATTTTACGACACTTTATGTGTTGTTCAACTCGATCTCAAATCAAGTAAGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n14                                                                             ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCAACCATCAAGCCTGGCTTGTCGTTGGACCTCTTTGCCAATGAAATATGTGGCAGGTCCGAAAGATAATGACGGCGTCGTGTTTGACCCTAGATGCAACGAGCTTTTTATAGCACGCATTGATGTGGTCGGGCGACCCAGTCTTTAACCATTATTTTCTAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n15                                                                                 ATGCGATAAGTAGTGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCTCCTGGTATTCCGGGAGGCATGCCTGTTCGAGCGTCATTAAAGACCACTCAAGCGATTTTGCTTGGTATTGGAAGAAGAGTGCCTCTGGCCCTCCCTTCCGAAATCCAATGGCGGAAAGTCTCACGTGCCCCGGCGTAGTAAGTTTATCTTTCGCTTGGACCCTGAGGCGTTCTCGCCCTCAAATCCCCAATACTATAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n16                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACCCTCTGGTCCATTCCAGAGGGTATGCCTGTTTGAGTGTCATTAATGTATCAAACCACCAAGCTTGCTTGGTTGGTCTTGGATGTTGAGGGTTGCTGGGGTTATAATGATCAGCTCCCTTTAAATGCATTAGCTTGGAATGTATAAGCCATTTTAGCTTAGGCTGATATGAATACAGCGTATTAAATGCTTTTGCTAAAGTGTAGCTTGTCTGGGCTTATAACTGTCTCTAGCTGAGACTGTCTTTTGACATTGTTAAATCATGATCATGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n17                                TTGCGATATGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCACCCTTTGGTATTCCGAAGGGTATGCCTGTTTGAGTGTCATTAAATTCTCAACTTCAAATTGAATTTTGAAGCTTGGACTTTGGAGGTTTGCTGGTGTCACTATCGGCTCCTCTTAAATTCATTAGCGGAACTGTAAGGACCGGCTTTGGTTTGATAGCTAACATTATCTATGCCGTTGCTGTGACCTTTGTGTTTGGCTTCTAATGGTCATTTTGTTGACTGTCTCTGCTTTGAGGCATACACTTTTAAGCTTGACCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n18                                                                                                    ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCGGTGGTATTCCGCCGGGCATGCCTATTCGAGCGTCATTACAACCCTCACGCCCCGCGTGGTCTTGGGCCGAGCCCCCCGGGCTGGCCTCAAAAGCAGTGGCGGTGCCTCTGGGTCCTGAGCGTAGTAACACTTCCGCTACAGGGCTCCCGAGCGTGCTGGCCGAACCCCAACCCTTCAGGTTGACCTCGGATTAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n19                                                                                  ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCATATTGCGCCCTTTGGTATTCCGAAGGGCATGCCTGTTCGAGCGTCATTATCACAGTATCAAGCTTGGCTTGTCGTTGGGCCCTTTGTCACCTGGTGACAGGTCCCAAAGAGAATGACTGGTGTCGTAAAGACTCTAAATGCAACGAGCTTATAACAGCACGCATCTAGTAGTAATATGGCCCGGTTCTCACCTCTTTATTTCTCAAGGTTGACCTCGGATCAGGTAGGAATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n21                                                                                            ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGAGGGGCATGCCTGTTCGAGCGTCATTACAACCCTCAAGCAATGCTTGGTGTTGGGCCGCGCCGCTAACCCGGCGGGCCCTAAAACCAGTGGCGGTGCCGTCGGGCTCTGAGCGTAGTAATTCTTCTCGCTATAGAGCCCCGGCGGATGCTAGCCAGCAACCCCCAATTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n22     ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCTCTTTGGTATTCCGAAGAGCATGCCTGTTTGAGTGTCATGAAAATATCAACCTTGACTTGGGTTTAGTGCTCTTGTCTTGGCTTGGATTTGGCTGTTTGCCGCTCGAAAGAGTCGGCTCAGCTTAAAAGTATTAGCTGGATCTGTCTTTGAGACTTGGTTTGACTTGGCGTAATAAGTTATTTCGCTGAGGACAATCTTCGGATTGGCCGAGTTTCTGGGACGTTTGTCCGCTTTCTAATACAAGTTCTAGCTTGCTAGACATGACTTTTTTATTATCTGGCCTCAAATCAGGTAGGACTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n23                                                                                                    ATGCGATAAGTAATGCGAATTGCAGAATTCAGTGAGTCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCCGCCCCCGTGGCCGGCCCCAAAGTCAGTGGCGGTGCCGTCCGGCTCTAAGCGTAGTACATCTCTCGCTCTAGGGTCCCGCGGTGGCCTGCCAGAACCCCAACTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n24                                                                                                 ATGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCTGTGGTATTCCGCAGGGCATGCCTGTTCGAGCGTCATTTAACCACTCAAGCCTAGCTTGGTATTGGGGCACGCGGTCTCGCGGCCCTTAAAATCAGTGGCGGCGCCGGTGGGCTCTAAGCGTAGTACATACTCCCGCTATAGAGTTCCCTCGGTGGCTCGCCAGAACCCCTAATTTTTACAGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n25                                           ATGCGATACGTAATGTGAATTGCAGATTCAGTGAATCATCGAATCTTTGAACGCATATTGCACTCTTTGGTATTCCGAAGAGTATGCCTGTTTCAGTATCATGAAAAACCTCACAAATTCAATTTTGGCTTTGTGGACTTGAGCATTTTGCGGCTTTGTTGCTGCTGGCTTAAAATATATTTCTTGGATAGCATATTATGGCTTTCGAAACTCGGCTTAATAGTTTTGGCTTTTGGTCAAATCTTTAGCTCTTTTCAAAGTCTTCAAGTTATTCAAAAGTTTTATACGAACACTTTCTCAATTTTGATCTGAAATCAGGTAGGATTACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n26                                                                                               CTGCGATAAGTAATGTGAATTGCAGAATTCAGTGAATCATCGAATCTTTGAACGCACATTGCGCCCCTTGGTATTCCGGGGGGCATGCCTGTTCGAGCGTCACTTCAACCCTCAAGCTCTGCTTGGTGTTGGGCCCTGCCGGCGACGGCAGGCCTTAAAACCAGTGGCGGCGCCGCTGGGCCCTGAGCGTAGTAATACTCCTCGCTACTGGGCCCCAGCGGATGCCTGCCAGCAAACCCAACTTTCTATGGTTGACCTCGGATCAGGTAGGGATACCCGCTGAACTTAAGCATATCAATAAGCGGAG\n   abundance forward reverse nmatch nmismatch nindel prefer accept\n1        172       1       1    150         0      0      2   TRUE\n2         47       2       4    230         0      0      1   TRUE\n3         43       3       3    234         0      0      2   TRUE\n4         34       5       5    179         0      0      2   TRUE\n5         32       6       2    147         0      0      2   TRUE\n7         21       4       6    218         0      0      1   TRUE\n8         17       8       9    229         0      0      2   TRUE\n9         17       9       7    229         0      0      2   TRUE\n10        13       7      10    237         0      0      1   TRUE\n11        12      20      21    233         0      0      1   TRUE\n12        12      10       8    257         0      0      2   TRUE\n13        10      11      12    139         0      0      1   TRUE\n14         9      19      20    215         0      0      1   TRUE\n15         9      15      13    219         0      0      2   TRUE\n16         6      17      22    154         0      0      1   TRUE\n17         6      16      23    170         0      0      1   TRUE\n18         6      13      16    238         0      0      1   TRUE\n19         5      23      27    220         0      0      1   TRUE\n21         4      26      25    230         0      0      2   TRUE\n22         4      14      11    143         0      0      1   TRUE\n23         4      22      26    238         0      0      1   TRUE\n24         3      21      24    235         0      0      1   TRUE\n25         3      18      15    181         0      0      2   TRUE\n26         2      25      30    233         0      0      1   TRUE\n\n\nThis contains a dataframe where for each sequence (ASV) in that sample we get information on what the sequence looks like, the number of reads corresponding to this forward/reverse combination (abundance) etc.\nFor example we can query the largest and smallest overlap like this.\n\n# Largest overlap \nmax(mergeASVs[[1]]$nmatch) \n\n[1] 257\n\n# Smallest overlap\nmin(mergeASVs[[1]]$nmatch) \n\n[1] 139"
  },
  {
    "objectID": "11_bioinformatics.html#create-a-sequence-table",
    "href": "11_bioinformatics.html#create-a-sequence-table",
    "title": "11  Bioinformatics",
    "section": "12.2 Create a sequence table",
    "text": "12.2 Create a sequence table\nNow we have a fully denoised set of sequences that can be used to generate a sequence table for further analysis that contains the merged sequence, abundance and indices of forward and reverse sequence variants that were merged.\n\nseqtab &lt;- makeSequenceTable(mergeASVs)\n\nLet’s take a quick look at what this data set looks like.\n\ndim(seqtab)\n\n[1]  74 701\n\nseqtab[,1]\n\n S1 S10 S11 S12 S13 S14 S15 S16 S17 S18  S2 S20 S21 S22 S23 S24 S25 S26 S27 S28 \n  0 218   0   0 124   0  48   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS29  S3 S30 S31 S33 S34 S35 S36 S37 S39  S4 S41 S42 S43 S44 S45 S46 S47 S48 S49 \n  0   0   0   0   0   0  19   0 148   0   0   0   0  70   0   0   0   0 113  32 \n S5 S50 S51 S52 S53 S54 S55 S56 S58 S59  S6 S60 S61 S62 S63 S64 S65 S66 S67 S68 \n  0   0   0  46   0 117   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \nS69  S7 S72 S73 S74 S75 S76 S77 S78 S79  S8 S80 S81  S9 \n  0   0   0   0   0 234   0   0 109   0  43   0   0  40 \n\n\nWe can see that it has 74 rows (each sample is a row) and 701 columns (each ASV is a row). If we print just the first column (ASV) you can see that the way a sequence table is organized is that for each sample the number of times an ASV is observed is recorded."
  },
  {
    "objectID": "11_bioinformatics.html#remove-chimeras",
    "href": "11_bioinformatics.html#remove-chimeras",
    "title": "11  Bioinformatics",
    "section": "12.3 Remove chimeras",
    "text": "12.3 Remove chimeras\nThe DADA2 algorithm accounts for indel errors and substitutions when inferring ASVs but before taxonomic assignment we also need to check for chimeras17. DADA2 identifies sequences that are likely chimeras by aligning each sequence with sets of sequences that were recovered in higher abundance and then determining if any lower-abundant sequences can be made by mixing left and right sequences from two of the more abundant ones.17 Chimeras are non-biological sequences were the left- and right segment of the merged sequence are from two or more parent sequences.\nWe can use the function removeBimeraDenovo() to identify and remove Chimeras and then creating and updated sequence table.\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab,\n                                    method = \"consensus\", \n                                    multithread = FALSE, \n                                    verbose = TRUE)\n\nAfter removing chimeras, the data set consists of 700 unique sequences across 74 samples.\nEven though chimeric sequences can frequently make up a large part of sequence variants and therefore initially make the data set seem more variable than it is, overall once you account for abundance, they should only be a very small component of the merged sequence reads. Here 0.1 % of merged sequence variants are chimeras, though once you account for abundance of these variants, overall 99.9% of merged sequences are not chimeric.\nLet’s take a look at the distribution of our non-chimeric ASV lengths.\n\n\n\n\n\nFigure 12.1: Distribution of sequence length for merged ASVs. The red dotted line indicates sequences that are 100-105bp long. We are targeting a 106 bp amplicon and are using 2x150 bp sequencing. Primer sites are approx 25bp each."
  },
  {
    "objectID": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "href": "11_bioinformatics.html#summary-of-read-filtering-processing-for-qc",
    "title": "11  Bioinformatics",
    "section": "12.4 Summary of read filtering & processing for QC",
    "text": "12.4 Summary of read filtering & processing for QC\nAt this point, we will want to take a look at how many reads where lost at each step to determine if those patterns look as expected. We can pull that information from various output files by counting the reads and putting them all in a dataframe.\n\n# custom function to get read numbers\ngetN &lt;- function(x) sum(getUniques(x))\n\n# create data table with number of reads per sample at each step\ntrack &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergeASVs, getN), rowSums(seqtab.nochim)) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"sample\") %&gt;%\n  rename(trimmed = reads.out,\n         denoisedF = V3,\n         denoisedR = V4,\n         merged = V5,\n         rm_chimera = V6) %&gt;%\n  pivot_longer(names_to = \"filter\", values_to = \"reads\", cols = 2:7) %&gt;%\n  mutate(filter = ordered(filter, levels = c(\"reads.in\", \"trimmed\", \"denoisedF\", \"denoisedR\", \"merged\", \"rm_chimera\")),\n         k_reads = reads/1000)\n\n# plot distribution\nggplot(track, aes(x = filter, y = k_reads)) +\n  geom_boxplot(fill = \"darkorange\") +\n  labs(x = \"filter/processing step\", y = \"thousand reads per sample\") +\n  theme_standard\n\n\n\n\nFigure 12.2: Comparison of change in the number of reads per sample at each filtering & processing stage. Red dotted line indicates targeted 150k reads per sample.\n\n\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nTake a look at the boxplot and describe how the different processing steps have impacted the number of reads remaining in the data set.\nRemember that this is a modified data set where each sample has been subsampled to contain 1000 reads - normally that initial box describing the distribution of samples would be wider as different samples would contain different numbers of reads. Generally, researchers target &gt; 75,000 - 150,000 reads per sample to make sure that all taxa present can be recovered in the data set.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOutside of the initial filtering there should not be any steps at which a substantial amount of reads are lost, the majority of reads should merge. If this is not the case, then trimming is likely to conservative and should be revisited. Similarly, only a small proportion should be chimeric. If primers are not completely removed, ambiguous nucleotides can interfere during chimera ID and that step of quality filtering should be revisited."
  },
  {
    "objectID": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "href": "11_bioinformatics.html#taxonomic-classification-for-asvs-present-in-each-sample.",
    "title": "11  Bioinformatics",
    "section": "12.5 Taxonomic classification for ASVs present in each sample.",
    "text": "12.5 Taxonomic classification for ASVs present in each sample.\nWe are now in the final stretch. Now that we have our ASVs and our sequence table the only thing that we still need to do is figure out which species (or other taxonomic groups) our ASVs correspond to.\nThis brings us to an important limitation of metabarcoding studies which is that our results are only ever as good as our reference database to which we can match our ASVs.\n\n\n\n\n\n\n Consider this\n\n\n\nOur next step is to match the sequences to a list of sequences that have taxonomic information. In our case study this would be a sequenced fungi. Describe your expectations of the results - do you expect every ASV in the data set to find a match in the reference? What other issues could result in results being ambiguous?\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nDADA2 has a built in function of a naive Bayesian classification menthod (assignTaxonomy()) that takes the sequences to be classified as the input along with a fasta file that contains the reference sequences18.18 fasta files consist of a header line that starts with &gt; and can contain any information about the sequence and then the sequence itself is in the next line. A multifasta file can contain multiple sequences, the program using the file determines the start of a new sequence by the fact that it “sees” the &gt; of the next header file.\nDepending on the project, scientists will pull sequences available from public databases such as genbank or if you are working with species that do not have a lot of sequences available you would need to create your own database by sampling individuals representing the different species/taxonomic groups you expect to encounter and sequencing them.\nWe are using the UNITE database of references which is designed to gather available ITS sequences to identify Eukaryotes.\nRunning this function is going to take several minutes - depending on the speed of your computer. Note that this code has the chunk option eval set as false so that it won’t run. Instead you can see that using the function save() I have writen the object out to your results folder. Then we can load it into your R environment in the next code chunk using load().\n\ntaxotab &lt;- assignTaxonomy(seqtab.nochim,\n                          refFasta = \"data/sh_general_release_dynamic_25.07.2023.fasta\",\n                          minBoot = 50, \n                          multithread = FALSE)\n\nsave(taxotab, \n     file = \"results/taxotab.rds\")\n\nWe can take a look at our results… there is a lot of information in this table so let’s just pull the first 5 ASV’s taxonomy without the ASV sequence for a better viewing experience.\n\n# load object into environment\nload(\"results/taxotab.rds\")\n\n# look at first 5 ASVs' taxonomy without the ASV sequence\nwrite.table(taxotab[1:5,], row.names = FALSE)\n\n\"Kingdom\" \"Phylum\" \"Class\" \"Order\" \"Family\" \"Genus\" \"Species\"\n\"k__Plantae\" NA NA NA NA NA NA\n\"k__Fungi\" \"p__Basidiomycota\" \"c__Agaricomycetes\" \"o__Agaricales\" \"f__Tricholomataceae\" \"g__Mycena\" NA\n\"k__Fungi\" \"p__Mucoromycota\" \"c__Umbelopsidomycetes\" \"o__Umbelopsidales\" \"f__Umbelopsidaceae\" \"g__Umbelopsis\" \"s__dimorpha\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Sordariomycetes\" \"o__Xylariales\" \"f__Amphisphaeriaceae\" \"g__Polyscytalum\" \"s__algarvense\"\n\"k__Fungi\" \"p__Ascomycota\" \"c__Dothideomycetes\" \"o__Mytilinidales\" \"f__Gloniaceae\" \"g__Cenococcum\" \"s__geophilum\"\n\n\nWe can also pull the unique species identified.\n\nunique(unname(taxotab[,7])) \n\n  [1] NA                    \"s__dimorpha\"         \"s__algarvense\"      \n  [4] \"s__geophilum\"        \"s__flavescens\"       \"s__terricola\"       \n  [7] \"s__elongatum\"        \"s__punicea\"          \"s__sylvestris\"      \n [10] \"s__humilis\"          \"s__subvinosa\"        \"s__opacum\"          \n [13] \"s__abramsii\"         \"s__ericae\"           \"s__terminalis\"      \n [16] \"s__variata\"          \"s__rexiana\"          \"s__zollingeri\"      \n [19] \"s__deciduus\"         \"s__album\"            \"s__chlamydosporicum\"\n [22] \"s__saponaceum\"       \"s__rufescens\"        \"s__populi\"          \n [25] \"s__podzolica\"        \"s__reidii\"           \"s__asperellum\"      \n [28] \"s__microspora\"       \"s__simile\"           \"s__acicola\"         \n [31] \"s__subsulphurea\"     \"s__camphoratus\"      \"s__pseudozygospora\" \n [34] \"s__heterochroma\"     \"s__brunneoviolacea\"  \"s__verrucosa\"       \n [37] \"s__auratus\"          \"s__mutabilis\"        \"s__chlorophana\"     \n [40] \"s__fuckelii\"         \"s__miniata\"          \"s__lignicola\"       \n [43] \"s__pilicola\"         \"s__phyllophila\"      \"s__australis\"       \n [46] \"s__citrina\"          \"s__fragilis\"         \"s__conica\"          \n [49] \"s__lubrica\"          \"s__pygmaeum\"         \"s__isabellina\"      \n [52] \"s__var._bulbopilosa\" \"s__finlandica\"       \"s__echinulatum\"     \n [55] \"s__lacmus\"           \"s__trabinellum\"      \"s__reginae\"         \n [58] \"s__spadicea\"         \"s__myriocarpa\"       \"s__physaroides\"     \n [61] \"s__calyptrata\"       \"s__nigrella\"         \"s__carneum\"         \n [64] \"s__vagans\"           \"s__metachroides\"     \"s__fumosa\"          \n [67] \"s__cantharellus\"     \"s__laetior\"          \"s__fusiformis\"      \n [70] \"s__spirale\"          \"s__pullulans\"        \"s__crocea\"          \n [73] \"s__sublilacina\"      \"s__acerinum\"         \"s__macrocystis\"     \n [76] \"s__vrijmoediae\"      \"s__changbaiensis\"    \"s__cygneicollum\"    \n [79] \"s__hymenocystis\"     \"s__dioscoreae\"       \"s__alnicola\"        \n [82] \"s__difforme\"         \"s__bicolor\"          \"s__spurius\"         \n [85] \"s__griseoviride\"     \"s__rebaudengoi\"      \"s__rufum\"           \n [88] \"s__globulifera\"      \"s__skinneri\"         \"s__sindonia\"        \n [91] \"s__verhagenii\"       \"s__maius\"            \"s__anomalovelatus\"  \n [94] \"s__diversispora\"     \"s__fellea\"           \"s__splendens\"       \n [97] \"s__coccinea\"         \"s__nitrata\"          \"s__risigallina\"     \n[100] \"s__juniperi\"         \"s__columbetta\"       \"s__rhododendri\"     \n[103] \"s__cinereus\"         \"s__fusispora\"        \"s__scaurus\"         \n[106] \"s__soppittii\"        \"s__grovesii\"         \"s__atropurpureum\"   \n[109] \"s__renispora\"        \"s__pura\"             \"s__foliicola\"       \n[112] \"s__phaeococcinea\"    \"s__rosea\"            \"s__stuposa\"         \n[115] \"s__minima\"           \"s__atrovirens\"       \"s__canadensis\"      \n[118] \"s__silvestris\"       \"s__sepiacea\"         \"s__pyriforme\"       \n[121] \"s__bulbillosa\"       \"s__glutinosum\"       \"s__cylichnium\"      \n[124] \"s__aeria\"            \"s__veluwensis\"       \"s__epicalamia\"      \n[127] \"s__hyalina\"          \"s__cylindrica\"       \"s__miyabei\"         \n[130] \"s__terrestris\"       \"s__rimosissimus\"     \"s__acuta\"           \n[133] \"s__myxotrichoides\"   \"s__physodes\"         \"s__alpina\"          \n[136] \"s__fallax\"           \"s__fumosibrunneus\"   \"s__albicastaneus\"   \n[139] \"s__mors-panacis\"     \"s__glacialis\"        \"s__acerina\"         \n[142] \"s__flavidum\"         \"s__ocularis\"         \"s__exigua\"          \n[145] \"s__piceae\"           \"s__verzuoliana\"      \"s__alliacea\"        \n[148] \"s__entomopaga\"       \"s__hyalocuspica\"     \"s__umbrosum\"        \n[151] \"s__bombacina\"        \"s__boeremae\"         \"s__fortinii\"        \n[154] \"s__miyagiana\"       \n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nUse your coding skills to additionally pull the unique genera, families, and orders contained in the data set and describe our results.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#from-the-beginning",
    "href": "11_bioinformatics.html#from-the-beginning",
    "title": "11  Bioinformatics",
    "section": "12.6 From the beginning …",
    "text": "12.6 From the beginning …\n\n\n\n\n\n\n Consider this\n\n\n\nNow that we have been through all the steps of processing next generation sequencing data set in a metabarcoding processing pipeline go back over the entire process, make sure you have understand what the key steps are and then outline that process below. Be sure to list the key steps in an organized manner and describe what occurs at each step in 2-3 sentences.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "11_bioinformatics.html#acknowledgements",
    "href": "11_bioinformatics.html#acknowledgements",
    "title": "11  Bioinformatics",
    "section": "12.7 Acknowledgements",
    "text": "12.7 Acknowledgements\nThis chapter borrows heavily from the original DADA2 tutorial as well as Alexis Carteron & Simon Morvan’s tutorial based on the subset sequences from (Carteron et al. 2021).\n\n\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and Etienne Lalibert’e. 2021. “Temperate Forests Dominated by Arbuscular or Ectomycorrhizal Fungi Are Characterized by Strong Shifts from Saprotrophic to Mycorrhizal Fungi with Increasing Soil Depth.” Microbial Ecology 82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7."
  },
  {
    "objectID": "z_references.html",
    "href": "z_references.html",
    "title": "References",
    "section": "",
    "text": "Callahan, Benjamin J., Paul J. McMurdie, Michael J. Rosen, Andrew W.\nHan, Amy Jo A. Johnson, and Susan P. Holmes. 2016.\n“DADA2: High-resolution\nSample Inference from Illumina Amplicon Data.”\nNature Methods 13 (7, 7): 581–83. https://doi.org/10.1038/nmeth.3869.\n\n\nCarlson, John K., and Ivy Baremore. 2005. “Growth Dynamics of the\nSpinner Shark (Carcharhinus Brevipinna) Off the\nUnited States Southeast and Gulf of\nMexico Coasts: A Comparison of Methods.” Fishery\nBulletin 103 (2). https://aquadocs.org/handle/1834/26223.\n\n\nCarteron, Alexis, Marie Beigas, Simon Joly, Benjamin L. Turner, and\nEtienne Lalibert’e. 2021. “Temperate Forests\nDominated by Arbuscular or Ectomycorrhizal\nFungi Are Characterized by Strong Shifts from\nSaprotrophic to Mycorrhizal Fungi with\nIncreasing Soil Depth.” Microbial Ecology\n82 (2): 377–90. https://doi.org/10.1007/s00248-020-01540-7.\n\n\nHeupel, Michelle R., John K. Carlson, and Colin A. Simpfendorfer. 2007.\n“Shark Nursery Areas: Concepts, Definition, Characterization and\nAssumptions.” Marine Ecology Progress Series 337 (May):\n287–97. https://doi.org/10.3354/meps337287.\n\n\nHeupel, Michelle R., Shiori Kanno, Ana P. B. Martins, Colin A.\nSimpfendorfer, Michelle R. Heupel, Shiori Kanno, Ana P. B. Martins, and\nColin A. Simpfendorfer. 2018. “Advances in Understanding the Roles\nand Benefits of Nursery Areas for Elasmobranch Populations.”\nMarine and Freshwater Research 70 (7): 897–907. https://doi.org/10.1071/MF18081.\n\n\nNeer, J. A., B. A. Thompson, and John K. Carlson. 2005. “Age and\nGrowth of Carcharhinus Leucas in the Northern\nGulf of Mexico: Incorporating Variability in\nSize at Birth - Neer - 2005 - Journal of\nFish Biology - Wiley Online Library.”\nJournal of Fish Biology 67 (2): 370–83. https://onlinelibrary.wiley.com/doi/full/10.1111/j.0022-1112.2005.00743.x.\n\n\nPlumlee, Jeffrey D., Kaylan M. Dance, Philip Matich, John A. Mohan,\nTravis M. Richards, Thomas C. TinHan, Mark R. Fisher, and R. J. David\nWells. 2018. “Community Structure of Elasmobranchs in Estuaries\nAlong the Northwest Gulf of Mexico.”\nEstuarine, Coastal and Shelf Science 204 (May): 103–13. https://doi.org/10.1016/j.ecss.2018.02.023.\n\n\nSwift, Dominic G., and David S. Portnoy. 2021. “Identification and\nDelineation of Essential Habitat for\nElasmobranchs in Estuaries on the Texas\nCoast.” Estuaries and Coasts 44 (3): 788–800. https://doi.org/10.1007/s12237-020-00797-y."
  },
  {
    "objectID": "14_impacts.html#a-grammar-of-graphics",
    "href": "14_impacts.html#a-grammar-of-graphics",
    "title": "14  Climate change: Impacts",
    "section": "14.1 A grammar of graphics",
    "text": "14.1 A grammar of graphics\nNow that we are confident(ish) in our data wrangling the next step is learning how we can visualize our data for exploration and for communication. We have already used figures using custom functions implemented in DADA2 that are based on the grammar of graphics/wrap functions from ggplot2 and we also familiarized ourselves with the syntax for using ggplot2 in the previous chapter.\nBeing able to visualize your data is an important skill throughout the process of data science. For example, in our next module we will dig into how to complete an exploratory analysis which is an important component of data science as it not only gives you an overview of what information is contained in a data set and can also help you refine the question you are asking. Finally, once you have refined and completed an analysis, visualizations are a key component to communicate your results.\nggplot2 is the core package of the tidyverse used for visualization. Similar to tidyr and dplyr having been built on the concept of a tidy data set, ggplot2 was built on a framework which follows a layered approach to describe and/or build any type of visualization in a structured way termed the “grammar of graphics”.\nThe grammar of graphics breaks down the components of any visualization into seven components.\n\nData is the component of data set to visualize1.\nMapping or Aesthetics include variables to be plotted, plot positions (e.g. x or y axis), and encodings such as size, shape, color2.\nGeometric objects (or layers) are what you actually end up seeing in the visualization, e.g. points, lines, bars etc. to represent data. This can also include statistical transformations3.\nScales map the values in the aesthetic space you specified, this includes your scales for color, shape, and size, you can also explicitly specify scales for the x and y axis.\nCoordinate system, includes all the usual suspects such as cartesian or polar along with some you may have never really heard of.\nFacets, i.e. subplots based on multiple dimensions.\nStatistics, including confidence intervals, means, quantiles, may additionally be added to the plot.\n\n1 You will see that this is either explicitly specified as the data argument (ggplot(data = df)), or you can just specify the data.frame or tibble to plot without typing in data = as ggplot(df).2 These components are specified as the mapping argument using aes(). Again you can explicitly call the argument as mapping =, i.e. ggplot(data = df, mapping = aes()), or you can just specify the mapping aesthetics using aes() because you are using the arguments in the correct sequence (ggplot(df, aes())) in which case R assumes they apply to these various arguments.3 Functions for these geometric objects will all start with geom_* or stat_*Additionally, you can create and use themes that control additional aspects of your your figure is displayed including font size, background color, etc.4.4 The package ggthemes includes various themes but you will also find that you can create your custom themes in a pretty straightforward way and that many R users have creted themes that they have made accessible for other users.\nUntil now you’ve probably thought of each plot type as it’s own distinctive format (a scatter plot, a pie chart, etc) and have not considered that each plot can be broken down into these fundamental components, so at first this way of thinking of plots will likely not seem intuitive.\nBut if you commit to thinking about plots in this abstract way you will quickly learn how these components fit together and realize the flexibility that this framework will give you to quickly generate exploratory plots, optimize your visualizations and be able to generate pretty much any plot you can think up5.5 You will also see that keeping your data tidy will allow you to customize your plots using different aesthetics and mappings to group your data."
  },
  {
    "objectID": "14_impacts.html#building-plots",
    "href": "14_impacts.html#building-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.2 Building plots",
    "text": "14.2 Building plots\nY’all ready for this?\nThen, let’s go back to our global mean temperature anomaly data set to figure out how the grammar of graphics can be applied and plot some data!\n\ntemperature &lt;- read_delim(\"data/GLB.Ts+dSST.csv\", delim = \",\", skip = 1) %&gt;%\n  replace(. == \"***\", NA) %&gt;%\n  mutate_if(is.character, as.numeric)\n\nRecall how we created a line plot:\n\nggplot(temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nWe can generalize this into a basic template that can be used for different kinds of plots, including scatter plots, bar plots, and box plots.\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;\n\nEssentially, the minimum components needed to make a plot are\n\nA data set (&lt;DATA&gt;), this will be a data.frame or a tibble6.\nBased on the data, we need to define what variables we are going to plot (&lt;MAPPINGS&gt;) and how we want them to be represented, i.e. what individual data points should look like and how they are encoded.\nWe need to define the geometries, i.e. what type of plot (&lt;GEOM_FUNCTION).\n\n6 Remember, when we use read_delim() our data is automatically a object both of the format tibble and data.frame; while a tibble has some additional properties but we can use them interchangeablyLet’s break down what is happening to get a better understanding of how these minimum components fit together to create individual plots.\nFirst, we use the ggplot() function to define the specific data.frame to use to build the plot using the data argument (&lt;DATA&gt;)7.7 Remember, that when we are using arguments in the specified sequence that they are defined you do not explicitly need to call them.\nIn our example that would be\n\nggplot(data = temperature)\n\n\n\n\nFigure 14.1: Empty canvas generated using ggplot() function.\n\n\n\n\nNot much happens when you execute that piece of code other than creating a blank canvas in your plot panel8.8 Remember, if you are using and Rmarkdown or quarto document and have Chunk Output Inline enabled it will print beneath your code chunk\nThat is because we still need to define an aesthetic mapping using the aes() function (&lt;MAPPING&gt;).\nHere, we are selecting the variables that we want to plot (columns in our data.frame). At minimum we need to specify what columns we want to plot on the x and y axis, but we can also define variables (columns) we want to use to encode using color, shapes, size etc.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`))\n\n\n\n\nFigure 14.2: Coordinate system plotted after specifying mapping aesthetics, specifically which variables to plot on each axis.\n\n\n\n\nWhen we execute that code you will now get an empty coordinate system. This still seems like slim pickings but patience, young grasshopper, we have not yet defined the geom, i.e. what type of plot (“geometries”, &lt;GEOM_FUNCTION&gt;) we want to use. For example, here we want generate a line plot (geom_line()).\n\n\nFor ggplot we add layers using the + operator, which is a pipe (similar to %&gt;%) which tells R “and now add this”.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line()\n\n\n\n\nFigure 14.3: Line plot (geom_line()) showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNow, we’re playing!\nOne of the advantages of the layered framework of ggplot2 is that we can plot multiple layers in the same plot by adding additional geoms.\nFor example, we can plot a scatterplot and line plot in the same plot as such:\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nFigure 14.4: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point().\n\n\n\n\nNow that you know how to generate a simple plot, let’s think about how we can further modify it to improve your visualization and how well you can communicate your results.\nFor example, we can change the color, fill, size, and shape for each geom layer.\n\n\nIn this example we are using the arguments for individual geom functions to change how data points are represented using colors and shapes for each layer, we can also use aes() to set mapping aesthetics for the entire plots.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3)\n\n\n\n\nFigure 14.5: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified.\n\n\n\n\nShapes for points can be specified using numbers; note how some shapes are “solid”, i.e. specifying color will define the color of the shape. Others, like the one used in our example are filled, i.e. color will determine the color of the outline, and fill the color of the space inside. For “hollow” shapes (or e.g. X’s) color will determine the line color.\n\n\n\nNumeric codes for sympols.\n\n\nYou can specify colors either using the color names defined by R or using hex codes.\n\n\nSome exploring will also lead you to various color pallets, many of them put together by R users around the world. Wes Anderson fan? You can style your plots accordingly… dig the aesthetics of the old school National Park posters and images? R community got you covered..\nPreviously, we also used geom_smooth() to add a layer with a linear regression. If you take a look at the arguments for this function using ?geom_smooth you will see that this has additional arguments apart from the mapping aesthetics like choosing the type of regression, whether or not the confidence interval is shown etc. Let’s say we want to add a red, dashed regression line without the confidence interval.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_line(color = \"darkblue\", size = 1) +\n  geom_point(shape = 21, color = \"darkblue\", fill = \"white\", size = 3) +\n  geom_smooth(stat = \"smooth\", se = FALSE, color = \"red\", linetype = \"dashed\", size = 2)\n\n\n\n\nFigure 14.6: Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified. Trend line fitted using geom_smooth().\n\n\n\n\nAdmittedly, this plot is now starting to look a little bit ridiculous and is probably make it less, not more easy for the viewer to undestand what they can learn from the data. Maybe we should think of alternative options for visualizing this data to explore some additional ggplot options.\nAnother way to visualize this data would be using a bar plot.\n\n\nCan we appreciate for a second how easy it was to change how the same data is plotted? If you were using excel you would have had to insert a new plot, define what data you wanted to plot, relabel the axis, etc. Here all you had to do was change a single line of code.\n\nggplot(data = temperature, aes(x = Year, y = `J-D`)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.7: Bar plot showing global mean temperature anomalies relative to 1951-1980 average.\n\n\n\n\nNotice, how this is still the exact same mapping aesthetics (time/year on the x-axis and the temperature anomaly on the y-axis) but it changes the way we see the data completely - the emphasis now is on the change of temperatures below or above the long-term mean and the clear pattern of a shift of those temperature differences going from negative to being positive.\nLet’s say we want really lean into demonstrating this drastic change in temperatures not only being above the long-term average but also steadily increasing. One way to do this is to use color. We could modify this bar plot to have all of our bars representing years with global mean temperatures below the 1951-1980 average in one color and those above in another.\nTo do this, we need a column that encodes that information… that of course is not an issue for us as pretty much professional data wranglers! We can use a simple conditional mutate using an ifelse()9 statement to add a column (year_type) that indicates whether temperatures are above or below the the long-term average.9 recall, that we previously used case_when() for a conditional mutate, when the condtion is binary (true/false or this/that) using ifelse() makes for a much shoerte syntax.\nWe can manipulate our data.frame using the dplyr functions we are already familiar with. We can even use the %&gt;% pipe to pass the data argument directly to the ggplot() function1010 When you do this, remember that you need to switch back to using the + operator once you are adding your geom layers.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFigure 14.8: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set.\n\n\n\n\nAnything that we add to the ggplot() layer as a mapping is a universal plot setting that will apply to all subsequent layers. You can override these global settings is you specify aesthetics like color, shape or size specifically in an separate layer. For example if we do the following, we will override our color coding even though we specified it in our ggplot() layer.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"blue\")\n\n\n\n\nFigure 14.9: Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set. Settings in geom_bar() override the general plot specifications.\n\n\n\n\nLet’s see what else we can visualize using ggplot. Recall from when you explored our temperature anomaly data set, that this data set also contains information for seasons and individual months.\nLet’s take a look at the distribution of mean global winter temperatures (DJF = December, January, February) using a histogram, (geom_histogram()). Note that we only need to define our x-axis to plot a histogram, this is because ggplot2 will count how many observations fall into each bin for you … this geom is a mixture of a geometry (bar plot) and a statistical transformation (binning data points into ranges and the plotting those as bar plots).].\n\nggplot(temperature, aes(x = DJF)) +\n  geom_histogram()\n\n\n\n\nFigure 14.10: Histogram summarizing the distribution of mean global winter temperatures.\n\n\n\n\nWe can refine our plot by choosing our own bin width and by manipulating fill and color.\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\")\n\n\n\n\nFigure 14.11: Histogram displaying distribution of mean global winter temperatures with customized bin width and coloration.\n\n\n\n\nFor our example we might also want to add a vertical line (geom_vline()) to indicate 0 (i.e. the long-term global temperature mean calculated for 1951 - 1980).\n\nggplot(temperature, aes(x = SON)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", fill = \"darkorange\") +\n  geom_vline(xintercept = 0, color = \"darkred\", linetype = \"dashed\", size = 1)\n\n\n\n\nFigure 14.12: Histogram displaying distribution of mean global winter temperatures with customized binwidth and coloration and added vertical line (red).\n\n\n\n\nIf we want to compare multiple distributions, box plots can be more helpful than histograms. Let’s say we wanted to compare the distribution of mean global temperature for each season.\nHow can we go about this?\nCurrently, our seasons are in individual columns, so our first step would be to create a tidy data set.\n\ntidy_season &lt;- temperature %&gt;%\n  select(Year, DJF, MAM, JJA, SON) %&gt;%\n  pivot_longer(names_to = \"season\", values_to = \"temperature\", 2:5)\n\nNow we can plot our seasons on the x-axis and the distribution of temperatures on the y-axis using geom_boxplot().\n\nggplot(tidy_season, aes(x = season, y = temperature)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.13: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons.\n\n\n\n\nAgain, you can add additional information using the mapping aesthetics to e.g. color code the boxes by season.\n\nggplot(tidy_season, aes(x = season, y = temperature, fill = season)) +\n  geom_boxplot()\n\n\n\n\nFigure 14.14: Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons color-coded by season."
  },
  {
    "objectID": "14_impacts.html#faceting-plots",
    "href": "14_impacts.html#faceting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.3 Faceting plots",
    "text": "14.3 Faceting plots\nOne of the advantages of ggplot2 being based not only on the grammar of graphics but also around the concept of a tidy data set is being able to create faceted plots. A faceted plot involves splitting a single plot into a matrix of panels, where each panel shows a different subset of the data. This is especially helpful during exploratory analysis where you might first plot all your data points in a single graph but then want to look at whether or not individual subsets within the data set behave the same.\nLet’s look at an example to better understand what faceting plots looks like. For example, let’s say we wanted to create individual plots of our bar plots showing our deviations of global temperatures from the 1951 - 1980 mean for each season.\nHow could you create individual plots with the methods you are already familiar with?\nHere is how you can do it using facet_grid().\nNow we can plot our data and using facet_grid() we can specify that we want individual panels by month in separate rows.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season))\n\n\n\n\nFigure 14.15: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nYou could also specify that you want the individual plots to be separated into columns.\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(cols = vars(season))\n\n\n\n\nFigure 14.16: Change in temperature anomaly relative to long-term avarage per season (columns)\n\n\n\n\nYou can also create faceted plot where subset your data by two variables and so you end up with one variable defining the rows and one the columns and you can use a simpler syntax row-variable ~ column-variable.\nYou can also use this syntax if you are only faceting by one variable as we are doing in this example by specifying the variable and leaving the other one “blank” using a ..\nFor example, to plot this faceted data set in rows, you would use the following syntax -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(season ~ .)\n\n\n\n\nFigure 14.17: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nAnd to plot this data set column-wise you would specify it like so -\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(. ~ season)\n\n\n\n\nFigure 14.18: Change in temperature anomaly relative to long-term avarage per season (columns)"
  },
  {
    "objectID": "14_impacts.html#customizing-plots",
    "href": "14_impacts.html#customizing-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.4 Customizing plots",
    "text": "14.4 Customizing plots\nWe have already played around a little bit with the options that we have in terms of customizing plots using color, fill, shapes, and sizes. But we’ve barely scratched the surface.\nYou have probably noticed that there are some default settings like the gray background and white grid lines, font sizes of label axis, what the axis labels are, and even color schemes that are automatically used. Even using the defaults we get pretty clean plots that are visually appealing. This is super helpful during exploratory analysis because even though you playing around with the data you still have nicely formatted and easy to interpret figures.\nOnce you have identified the central plots that you want to use to communicate the results and conclusions of your data analysis you will want to further customize your visualization to optimize communication, this includes how you encode data using color and shape but also making sure that everything is well labeled and clear to the person who is reading your report or listening to your presentation.\nOne of the first things we frequently want to changes is the axis labels. ggplot2 does handily use the column names to automatically label your axes, so you will always have a label which is great during exploratory analysis but generally you will want to customize that for your final figure. The function labs() can be used to specify a title, subtitle, axis labels and additional annotations (caption) below the figure.\nFor example we could customize our faceted figure to look like this:\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\")\n\n\n\n\nFigure 14.19: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nPretty much every component of a ggplot figure can be further customized using theme(), this includes things like font size, background and line colors, grids, legend position … ggplot2 odes have some pre-defined themes that you can call up that will change the layout.\nThe default theme is theme_grey().\n\nggplot(tidy_season, aes(x = Year, y = temperature)) +\n  geom_bar(stat = \"identity\", fill = \"darkorange\", color = \"darkorange\") +\n  facet_grid(rows = vars(season)) +\n  labs(title = \"Change in global seasonal temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_grey()\n\n\n\n\nFigure 14.20: Change in temperature anomaly relative to long-term avarage per season (rows)\n\n\n\n\nHere, we explicitly specified it using theme_grey(); if you do not specify a theme this is the theme that will be used for your plot. There are other themes that are part of the ggplot package that include theme_bw(), theme_minimal(), theme_classic() or theme_light().\n\n\n\n\n\n\n Give it a whirl\n\n\n\nRe-plot the same figure using the four themes specified above.\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]\n\n\nYou will likely find a theme that you like but still want or need to make additional tweaks. In that case the function theme() is your friend.\nLet’s go back to our bar plot of the global temperature anomaly to look at an example of likely the three most common things you will want to adjust which is the legend position, changing font size, color, turning x-axis labels by 90 degrees.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\n\n\n\nFigure 14.21: Change in mean global temperature relative to long-term avaerage with customized font sizes and legend positions.\n\n\n\n\nThat’s starting to look pretty slick.\nAs you start your visualization adventure the R Cookbook is a good resource. It is written in a helpful style that starts with a concrete plotting problem and then walks you through a solution."
  },
  {
    "objectID": "14_impacts.html#arranging-plots",
    "href": "14_impacts.html#arranging-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.5 Arranging plots",
    "text": "14.5 Arranging plots\nWe’ve already seen that we can create multi-panel plots in a very straightforward way when we are plotting the same plot for different subsets of the data (facetted plots).\nBut what if we want to generate individual plots that might fit together thematically but aren’t subsets that we can facet but we still want to be able to present them together? Fear not, this too can be solved; one option is using patchwork, a package designed for exactly this purpose. Let’s generate a few additional plots so we can try this out.\nIn our data folder there is a tab-delimited file with monthly mean CO2 concentrations (parts per million) from Maunua Loa. Let’s read in that data set and create line plot with a regression line.\n\ncarbon &lt;- read_delim(\"data/CO2_monthly.txt\", delim = \"\\t\")\n\nggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\n\n\n\nFigure 14.22: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory from 1958 - 2021.\n\n\n\n\nWe also have a data set from the Global Carbon Project downloaded from the Our World in Data repository that contains atmospheric CO2 emissions. We can plot that as a simple line plot with a regression.\n\nemissions &lt;- read_delim(\"data/emissions.txt\", delim = \"\\t\") %&gt;%\n  filter(iso_code == \"OWID_WRL\")\n\nggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"CO2 Emissions over time.\",\n       subtitle = \"Global emissions atmospheric emissions by year.\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\n\n\n\nFigure 14.23: Global atmospheric CO2 emissions.\n\n\n\n\nTo be able to plot multiple plots in one using patchwork we need to assign our figures as objects.\n\np1 &lt;- ggplot(carbon, aes(x = date, y = average)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2\",\n       subtitle = \"monthly mean CO2 Mauna Loa CO2\",\n       x = \"year\", y = \"CO2 concentration air [ppm]\",\n       caption = \"Data: NOAA/ESRL\") +\n  theme_classic()\n\np2 &lt;- ggplot(emissions, aes(x = year, y = co2)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(color = \"red\") +\n  labs(title = \"Atmospheric CO2 Emissions\",\n       subtitle = \"global emissions\",\n       x = \"year\", y = \"CO2 [Gt/year]\",\n       caption = \"Data: Global Carbon Project/Our World in Data\") +\n  theme_classic()\n\nNow we can combine them side by side using a simple syntax.\n\np1 + p2\n\n\n\n\nFigure 14.24: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (left panel) and Global atmospheric CO2 emissions (right panel).\n\n\n\n\nSimilarly, we can plot them underneath each other like so -\n\np1 / p2\n\n\n\n\nFigure 14.25: Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (top panel) and Global atmospheric CO2 emissions (bottom panel).\n\n\n\n\nOh, it gets better. Let’s say we wanted to plot our global mean temperatures in the top row and our two emissions plots below.\n\np3 &lt;- temperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\np3 / (p1 | p2)\n\n\n\n\nFigure 14.26: Comparison of change in mean global temperature (top panel), atmospheric CO2 concentrations (bottom left), and atmospheric CO2 emissions (bottom right).\n\n\n\n\nYou can create complex compositions in patchwork using syntax combining +, |, and \\. To see how you can control the layout even further you can check out the documentation for patchwork."
  },
  {
    "objectID": "14_impacts.html#exporting-plots",
    "href": "14_impacts.html#exporting-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.6 Exporting Plots",
    "text": "14.6 Exporting Plots\nThe last thing we still have to figure out is how to save plots to file. You have a few options.\nThe quickest and dirtiest option is to simply right click on the plot pane after plotting a figure and then save the figure using Save image as. This works and is easy to do but gives you very little control over the dimensions, resolution, file format etc.\nFor more control over the format of your figure you can use the Export tab in the Plot pane which will allow you to adjust the dimensions and the the file format.\nFinally, the ggplot2 has a function called ggsave() that will allow you to determine the dimensions (width, height), resolution (dpi), and format (device).\nBy default it will save the last plot that was plotted. If you specify the format in the file name (e.g. *.svg, *.jpg, *.png) it will automatically recognize the format11.11 Remember that we have designed our research compendium to keep raw and processed data, and results separate? Figures are considered results so you should always save them to the results folder.\n\ntemperature %&gt;%\n  mutate(year_type = ifelse(`J-D` &lt;=0, \"colder\", \"hotter\")) %&gt;%\n  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Change in global mean temperatures 1880 - 2020\",\n       subtitle = \"Global mean temperatures relative to 1951 - 1980 mean\",\n       x = \"Year\", y = \"Temperature [C]\",\n       caption = \"Date source: NASA Goddard Institute for Space Studies\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size = 12, color = \"black\",\n                                   angle = 90, hjust = 0.5, vjust = 0.5),\n        axis.text.y = element_text(size = 12, color = \"black\",),\n        axis.title = element_text(size = 14))\n\nggsave(\"results/global_temp.png\", dpi = 300, width = 15, height = 10)\n\n\n\n\nFigure 14.27: Change in mean global temperature realtive to long-term average.\n\n\n\n\nCheck your results folder to see if you were successful!\nIf you assign your plot to an object you can use the plot argument of the ggsave() function to export it. This also works for figures that consist of multiple panels combined using patchwork."
  },
  {
    "objectID": "14_impacts.html#more-plots",
    "href": "14_impacts.html#more-plots",
    "title": "14  Climate change: Impacts",
    "section": "14.7 More Plots!",
    "text": "14.7 More Plots!\nReady to take the training wheels off?\nIncreasing global temperatures are effecting change across the different components of the climate system. There are several data sets in your data folder. Read in each data set as a data frame, use View() to take a look at what information is contained in the data set and the use your new found visualization skills to plot the data using ggplot.\nWe have looked at rising mean global temperatures and discovered that rapidly increasing emissions and atmospheric concentrations of CO2 are consistent with increased concentrations of greenhouse gases driving temperature change.\nThe effects of global warming are observed throughout components of the climate system including the atmosphere, the hydrosphere (marine and freshwater systems), cryosphere (land and sea ice), and lithosphere (earth’s surface/crust). Let’s look at a few data sets that illustrate the impact of rising temperatures.\nFor all the plots, comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\n\n\n\n\n\n\n Give it a whirl\n\n\n\nIncreasing atmospheric temperatures result in land ice melting and the ocean heat content rising. The latter leads to thermal expansion which together with the increase freshwater inflow results in rising sea levels.\nThe first data set is from the NOAA Laboratory for Satellite Altimetry and contains records from coastal sea level tide gauges. Start by reading in the data set:\n\nsealevel &lt;- read_delim(\"data/sealevel.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set12\nPlot the change in mean sea level over time as a line plot and color code the line(s) by method (see column names). Chose a theme and position your legend below the figure.\nFigure out how to use geom_hline() to add a line at 0.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n12 Include things like the number of columns, what data is contained in each sample, what time periods are included etc. Practice deducing these types of things from the context of the data set description above and by browsing the content.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere’s an example of what tour final figure could look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOur next data set is from the National Snow and Ice Data Center and contains the Arctic (Northern Hemisphere) July sea ice extent.\n\nice_arctic &lt;- read_delim(\"data/arctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a theme. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nLet’s take a look at the equivalent data set depicting sea ice extent in the Antarctic (Southern Hemisphere.)\n\nice_arctic &lt;- read_delim(\"data/antarctic_ice.txt\", delim = \"\\t\")\n\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific.\nPlot the change in ice extent over time as a line plot. Chose a new theme from what you’ve been using previously. Add a linear regression and confidence interval.\nExtra Challenge: Change the default colors of the regression line and the confidence interval.\nComment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.\nWrite a short description of your figure summarizing the key results displayed.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot\nSummary of results\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nOne more data set. This one is from the NOAA Hurricane Research Division. While interpreting the observed patterns in sea level and ice extent and modeling future projections is pretty straightforward, understanding whether extreme events are getting “worse” is a little bit more complicated13.\nNote: For this coding challenge you will need to combine your data wrangling and visualization skills!\n\nhurricane &lt;- read_delim(\"data/hurricanes.txt\", delim = \"\\t\")\n\nTake a quick look at the data set using View() and briefly summarize what data is contained in this data set. Be specific14.\nPlot 1:\nWhen thinking about hurricanes there are two metrics to consider when determining whether or not “hurricanes have gotten worse”. First, we can look at whether or not hurricanes have become more common by plotting the number of hurricanes for each year.\n\nCreate a faceted plot with individual line plots showing the number of named storms (this would include not only hurricanes put also tropical storms), the number of hurricanes, the number of major hurricanes, and the number of hurricanes in the United States.\nAdd a linear regression.\nChose a custom theme.\n\n\n\nExtra Challenge: We’ve previously used facet_grid() to create faceted plots which requires us to specify how panels should be laid out in rows and/or columns. Figure out how to use facet_wrap() to just specify which variable to use to create individual panels with out them all needing to be laid out next to each other in columns or underneath each other in rows.\n\nPlot 2:\nThe other way to evaluate whether or not “hurricanes have gotten worse” is to not look at the number of storms but to determine whether individual storms or storms as a whole have become more intense and/or destructive. One of the columns you may not have been able to fully figure out what information it contains is the column containing information on the “accumulated cyclone energy”15.\n\nCreate a bar plot showing the ACE for each year.\nChose a theme and customize the color of your bars16 3. Extra Challenge: Use geom_hline() to add a horizontal line indicating the mean ACE for the recorded time period.\nWrite a short description for each figure summarizing the key results displayed and argue whether or not you think “hurricanes have gotten worse”.\n\n16 You can specify color and fill for bar plots. Try setting the color and fill to the same and to different colors to see what happens when you have a bar plot with this many individual bars.\n15 Now that you have this information gp ahead and update your notes to add it to your short data set description.14 Note, several of the columns start with ‘Revised’ this just has to do with the entire data set having revised (most recent) definitions applied for the metrics included. You can ignore that component.13 We will take a closer look at this aspect when we combine our data wrangling and visualization skills to learn how to perform an exploratory analysis\nYou will have to reformat the data set in order to plot it in this fashion. It can be helpful to first think about what your ggplot code should look like to create the plot described above and then format your data set to fit those needs. Recall that ggplot is designed not only around the grammar of graphics but also the principles of tidy data… so, do we currently have a tidy data set? Or what would it need to look like for this plotting challenge?.\n\n\n\n\n\n\n\n Did it!\n\n\n\nData Set:\nPlot 1:\nPlot 2:\nSummary of results\n\n\n\n\n\n\n\n\n Pointers\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\nHere is what a draft of your first plot could look like before you finalize things like titles and themes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Give it a whirl\n\n\n\nChoose five of the plots we have generated in this chapter and combine them into a single plot using patchwork; include at least two columns and two rows in your layout17.\nSave your plot in a way that retains proper proportions to make it easy for somebody else to easily see the details and discern the general patterns. Share your multi-panel plot in the #assignment channel of our slack workspace.\nUse your the figure you have created to support argue that “the observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures”. Draw from the descriptions you made for the figures throughout this challenges to summarize what you observe to support this claim. In your answer you will want to consider which of the data sets refer to which component of the earth-climate system and why the data sets you have looked at are consistent with increasing mean global temperatures.\nExtra challenge: You can use #| fig-height and #| fig-width; code chunk options to specify the dimensions of the output of a figure in the knitted document18. When you initially render your quarto document you will find that the default dimensions will not format your you multi-panel plot in a way that does not make it super legible. Manipulate these parameters and re-knit your html to display your figure in an appropriate way.\n\n\n18 Yes, this does mean that you will need to check the format of your html output and potentially change it before you submit. Checking your html report before you submit is a good habit to have just to make sure that everything is rendering as expected17 This can include your temperature anomaly data set as well.\n\n\n\n\n\n Did it!\n\n\n\nFigure\nThe observed changes in the different components of the climate system are internally and physically consistent with increased global temperatures\n\n\n\n\n\n\n\n\n Consider this\n\n\n\nConsider the following and write a short paragraph (7-10 sentences):\n\nThink back over the analyses we have performed for this and the previous chapter and argue whether the analysis you have performed should be considered descriptive, exploratory, inferential/predictive, or causal/mechanistic.\nDescribe both what you actually did, and then also briefly sketch out what your experimental design/analysis would need to look like if you had performed analysis in the other categories.\nCausal/mechanistic analyses are the most involved frequently also the most difficult to perform. Argue whether or not your results are less meaningful or informative if they are not mechanistic/causal or whether they are still useful to support the initial thesis that climate change is unprecedented, unequivocally anthropogenic, and the impacts are already impacting all components of the earth-climate system.\n\n\n\n\n\n\n\n\n\n Did it!\n\n\n\n[Your answer here]"
  },
  {
    "objectID": "A_hello-world.html",
    "href": "A_hello-world.html",
    "title": "Hello world",
    "section": "",
    "text": "The phrase “hello world” is a popular and simple introductory program that is often used to demonstrate the basic syntax and structure of a programming language. It’s typically one of the first programs that people write when learning a new programming language.\nIt is usually a simple program that prints the text Hellow World to the output, typically a console or terminal window. Writing and running a “hello world” program helps beginners understand the essential steps of writing, compiling, and executing code in a specific programming language.\nTo create a “Hello, World!” program in R, you can use the print() function:\nprint(\"Hello, World!\")\n\nIn this section, we will get an overview of what Data Science is, install R/Rstudio and orient ourselves in how to use Rstudio and learn the basics about using R as an object-oriented language."
  }
]